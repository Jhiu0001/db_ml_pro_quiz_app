[
  {
    "question_id":"skillcertpro_e1_q1",
    "question": "You are monitoring a machine learning model that uses a categorical feature called user_device_type with possible values: 'mobile', 'desktop', and 'tablet'. Over time, you notice that the distribution of these categories has started to shift. You want to determine if this shift is statistically significant and should be addressed. Which of the following is the most appropriate approach to detect drift in this scenario?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Calculate the variance of user_device_type"},
      {"id": 1, "text": "Apply the Mann-Whitney U test to user_device_type"},
      {"id": 2, "text": "Perform a Chi-square test to detect drift in user_device_type"},
      {"id": 3, "text": "Use the Kolmogorov-Smirnov (KS) test to detect drift in user_device_type"}
    ],
    "ans_id":2,
    "explanation":"The Chi-square test is the most appropriate statistical test for detecting drift in categorical features like user_device_type. It compares the observed frequencies of each category in the production data with the expected frequencies based on the training data (or a baseline distribution). A statistically significant Chi square statistic indicates that the difference in the distributions is unlikely to have occurred by random chance, suggesting drift."
  },
  {
    "question_id":"skillcertpro_e1_q2",
    "question": "You are working on a Databricks project where you need to implement a machine learning pipeline to predict customer churn. The dataset contains several features that include missing values, categorical variables, and imbalanced classes. Your task is to implement preprocessing steps in the ML lifecycle to handle these issues, log them properly in MLflow, and ensure the steps are reproducible across multiple environments. Which of the following strategies should you adopt to properly implement preprocessing logic and track it in MLflow for managing the model lifecycle?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Handle all preprocessing steps using automated feature engineering tools available in Databricks, which automatically logs preprocessing steps in MLflow."},
      {"id": 1, "text": "Only log the final model in MLflow, as preprocessing steps are typically inferred during model evaluation and do not need explicit logging."},
      {"id": 2, "text": "Use Databricks' built-in functions for data cleaning, but avoid logging preprocessing steps in MLflow since they dont affect the final model."},
      {"id": 3, "text": "Implement preprocessing steps (such as handling missing values and encoding categorical variables) in a dedicated data processing pipeline, and log the entire pipeline in MLflow using a custom Python script."}
    ],
    "ans_id":3,
    "explanation": "Implement preprocessing steps (such as handling missing values and encoding categorical variables) in a dedicated data processing pipeline, and log the entire pipeline in MLflow using a custom Python script.\n\nThis strategy provides the most control, reproducibility, and transparency for managing preprocessing steps in the ML lifecycle and tracking them with MLflow.\n\nDedicated Pipeline:\nSeparating preprocessing logic into a dedicated pipeline (e.g., using Spark transformations or custom functions) promotes modularity and allows for independent testing and management of these steps.\n\nExplicit Implementation:\nThis gives you precise control over how missing values are handled, categorical variables are encoded, and imbalanced classes are addressed, ensuring the preprocessing is tailored to your specific dataset and model requirements.\n\nMLflow Logging of the Pipeline:\nLogging the entire preprocessing pipeline (including the specific methods and parameters used for each step) in MLflow ensures reproducibility. You can log the code or save the preprocessing steps as artifacts (e.g., using mlflow.save_model with a custom PythonModel that encapsulates the preprocessing logic).\n\nReproducibility Across Environments:\nBy logging the preprocessing pipeline in MLflow, you can deploy and reuse the exact same preprocessing steps in development, staging, and production environments."
  },
  {
    "question_id":"skillcertpro_e1_q3",
    "question": "In a Databricks project, you're designing a custom machine learning model class for deployment. You want to ensure that the preprocessing steps such as normalization, missing value imputation, and feature encoding are encapsulated within the model class. Which of the following is the best practice for including preprocessing logic in your custom model class?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Encapsulate preprocessing within the model class as part of the fit and predict methods, ensuring consistent data transformations during both training and inference"},
      {"id": 1, "text": "Store the preprocessing logic in a separate script and apply it outside the model class to ensure modularity and reusability."},
      {"id": 2, "text": "Implement the preprocessing steps in the model class but apply them only during training to save computational resources during inference."},
      {"id": 3, "text": "Use preprocessing logic only in the fit method of the model class, and assume that the incoming data during inference will always be in the same format as during training."}
    ],
    "ans_id":0,
    "explanation":"Encapsulate preprocessing within the model class as part of the fit and predict methods, ensuring consistent data transformations during both training and inference. This is the best practice for including preprocessing logic in a custom model class for deployment."
  },
  {
  "question_id":"skillcertpro_e1_q4",
  "question": "You are building a time series forecasting model in Databricks, and in addition to the models evaluation metrics (e.g., RMSE), you want to track custom visualizations such as a plot of the forecast against the actual data and log additional metadata (e.g., training dataset details) for future reference. How should you log these artifacts and metadata so they are accessible and organized in the experiment? Which combination of MLflow methods would best allow you to log both custom visualizations and metadata for your forecasting model?",
  "type":"MC",
  "choices": [
    {"id": 0, "text": "Use mlflow.log_figure() for visualizations and mlflow.set_tags() for logging metadata like dataset information."},
    {"id": 1, "text": "Use mlflow.log_model() to store both the visualizations and metadata together, as this method will automatically save any related files or details."},
    {"id": 2, "text": "Use mlflow.log_artifact() for visualizations and mlflow.log_param() for logging metadata like dataset information"},
    {"id": 3, "text": "Use mlflow.log_metric() to save the visualizations and mlflow.log_param() for metadata."}
  ],
  "ans_id":0,
  "explanation":"Use mlflow.log_figure() for visualizations and mlflow.set_tags() for logging metadata like dataset information. \nThis combination of MLflow methods is the most appropriate for logging custom visualizations and additional metadata in an organized and accessible way:" 
  },
  {
  "question_id":"skillcertpro_e1_q5",
  "question": "In a Databricks Structured Streaming application, you are processing real-time financial transactions. The events occasionally arrive out-of-order. Which of the following approaches best handles out-of-order events while ensuring accurate aggregations with minimal data loss or late records?",
  "type":"MC",
  "choices": [
    {"id": 0, "text": "Switch to processing time as the event timestamp might be unreliable."},
    {"id": 1, "text": "Use event time, but apply no watermark to avoid losing any late events."},
    {"id": 2, "text": "Increase the batch interval to handle late data."},
    {"id": 3, "text": "Use event time and apply a watermark with a reasonable delay tolerance."}
  ],
  "ans_id":3,
  "explanation":"Use event time and apply a watermark with a reasonable delay tolerance.\n\n This is the most effective approach for handling out-of-order events in Structured Streaming while ensuring accurate aggregations and minimizing data loss or late records.\n\nEvent Time: Using event time (the timestamp embedded in the data itself) ensures that aggregations are based on when the event actually occurred, regardless of when it was processed by the streaming pipeline. This is crucial for accurate financial transaction analysis.\n\nWatermarking: A watermark is a threshold that tells Spark when it can consider events with timestamps older than the watermark as “late.“ This allows Spark to perform time-based aggregations (e.g., windowing) with a degree of tolerance for out-of-order data.\n\nReasonable Delay Tolerance: Setting an appropriate watermark delay (e.g., based on the expected maximum lateness of events) allows the system to wait for reasonably late events to arrive before processing the aggregation. This minimizes the risk of prematurely dropping data that arrives slightly out of order."
  },
  {
  "question_id":"skillcertpro_e1_q6",
  "question": "You are monitoring the drift of numerical features in a time-series machine learning model. The data has shown some fluctuation over time, and you want to avoid unnecessary model retraining unless a significant drift is confirmed. Which of the following approach combinations would provide the most comprehensive and robust solution for detecting and validating numeric feature drift",
  "type":"MC",
  "choices": [
    {"id": 0, "text": "Summary Statistics and Population Stability Index (PSI)"},
    {"id": 1, "text": "Chi-Squared Test and Mean Comparison"},
    {"id": 2, "text": "Wasserstein Distance and Summary Statistics"},
    {"id": 3, "text": "Kolmogorov-Smirnov Test and Wasserstein Distance"}
  ],
  "ans_id":3,
  "explanation":" Kolmogorov-Smirnov Test and Wasserstein Distance: This combination offers a robust and comprehensive approach for detecting and validating numerical feature drift, especially in time series data where distributions can evolve in complex ways.\n\nKolmogorov-Smirnov (KS) Test: The KS test is a non-parametric test that compares the cumulative distribution functions (CDFs) of two samples. It is sensitive to any differences in the shape, location, or scale of the distributions, making it effective at detecting various types of drift in numerical features without assuming a specific distribution. \n\n Wasserstein Distance (Earth Mover's Distance): The Wasserstein distance measures the minimum cost to transform one probability distribution into another. It provides a more nuanced measure of the distance between distributions compared to simple statistical tests, as it considers the 'effort' required to move the probability mass. This makes it particularly useful for detecting drift where the overall shape of the distribution changes, even if summary statistics like mean and variance remain relatively stable. Combining these two methods provides a strong approach: the KS test can flag statistically significant differences in the distributions, while the Wasserstein distance can quantify the magnitude and nature of that difference. Using both can help confirm if the drift is substantial enough to warrant model retraining." 
  },
  {
  "question_id":"skillcertpro_e1_q7",
  "question": "You are tasked with updating a Feature Store in Databricks. You need to add new features for an existing machine learning model while ensuring that previously stored features are not overwritten. Which method should you use to update the Feature Store table?",
  "type":"MC",
  "choices": [
    {"id": 0, "text": "append"},
    {"id": 1, "text": "merge"},
    {"id": 2, "text": "create"},
    {"id": 3, "text": "overwrite"}
  ],
  "ans_id":1,
  "explanation":"The merge operation in Databricks Feature Store is the appropriate method for updating an existing feature table while preserving previously stored features. merge allows you to update existing rows based on a join condition and insert new rows for data that doesn't already exist. This ensures that you can add new features (which would likely correspond to new columns) and new data for existing or new entities without deleting the older data." 
  },
  {
  "question_id":"skillcertpro_e1_q8",
  "question": "You are working on a machine learning project at a company where predictions are made for customer churn. Your team has decided to implement a batch prediction system using Databricks, where predictions are computed at regular intervals and stored in a database for querying. Which of the following are correct benefits of using precomputed batch predictions in this scenario for live serving? (Select two)",
  "type":"MS",
  "choices": [
      {"id": 0, "text": "Reduces the need for real-time feature engineering"},
      {"id": 1, "text": "Automatically updates the model with new data in real-time"},
      {"id": 2, "text": "Eliminates the need for feature store versioning"},
      {"id": 3, "text": "Avoids inference delay during the request cycle"},
      {"id": 4, "text": "Increases prediction accuracy over time"}      
      ],
  "ans_ids":[0,3],
  "explanation":"Reduces the need for real-time feature engineering - With precomputed batch predictions, the feature engineering process is typically performed as part of the batch pipeline before the model makes predictions. The results are then stored. When serving these predictions live, the system simply retrieves the precomputed scores based on the customer ID or other relevant keys. This avoids the computational overhead and complexity of performing feature engineering on demand for each incoming request in a live serving environment.\n\n Since the predictions are already computed and stored, serving them live involves a simple lookup (e.g., a database query). This significantly reduces the latency associated with making a real-time prediction because the model doesn't need to be executed for each request. The inference delay is essentially eliminated,leading to faster response times for applications consuming the predictions."
  },
  {
    "question_id":"skillcertpro_e1_q9",
    "question": "You are tasked with deploying a machine learning model that generates quarterly sales forecasts for a large retail chain. The forecasts need to be stored for later analysis and occasional retrieval, but they are not accessed frequently. Which type of storage solution would be the most cost-effective and appropriate for this scenario, given the infrequent access?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Relational database storage such as MySQL or PostgreSQL"},
      {"id": 1, "text": "On-premise local storage with SSDs"},
      {"id": 2, "text": "Cold storage such as AWS Glacier or Azure Archive Storage"},
      {"id": 3, "text": "Hot storage such as AWS S3 Standard or Azure Blob Hot"}
    ],
    "ans_id":2,
    "explanation":"Cold storage such as AWS Glacier or Azure Archive Storage \n\nCold storage solutions are specifically designed for data that is accessed infrequently. They offer significantly lower storage costs compared to hot or relational database storage, but come with higher retrieval latencies and potentially higher costs for data retrieval. Given that the quarterly sales forecasts are stored for later analysis and occasional retrieval, the infrequent access pattern makes cold storage the most cost-effective option. The longer retrieval times are acceptable since the data is not needed on demand." 
  },
  {
    "question_id":"skillcertpro_e1_q10",
    "question": "Your organization wants to automate the model retraining process whenever new data is ingested into the pipeline. The goal is to maintain a highly accurate model that can adapt to new trends in real-time data. You are tasked with identifying key automated tests that should be implemented in the CI/CD pipeline to ensure smooth and reliable model updates. Which of the following steps should be automated as part of a CI/CD pipeline to maintain model accuracy and robustness after every retraining? (Select two)",
    "type":"MS",
    "choices": [
        {"id": 0, "text": "Model Evaluation Based on Historical Performance Only"},
        {"id": 1, "text": "Performance Regression Testing"},
        {"id": 2, "text": "Cross-Validation During Training"},
        {"id": 3, "text": "Business Value Testing"},
        {"id": 4, "text": "Manual Model Selection by Domain Experts"}
        ],
    "ans_ids":[1,2],
    "explanation":"Performance Regression Testing - After retraining a model, it's crucial to compare its performance against the previous production model (or a baseline) on a held-out dataset. Performance regression testing automatically evaluates key metrics (e.g., accuracy, F1-score, RMSE) and ensures that the new model's performance has not significantly degraded compared to the established benchmark. If the new model performs worse, the CI/CD pipeline should prevent its deployment, ensuring that model accuracy is maintained.\n\n Cross-Validation During Training - Automating cross-validation as part of the training process within the CI/CD pipeline helps ensure the model's robustness and generalization ability on unseen data. Cross-validation provides a more reliable estimate of the model's performance than a single train-test split and can detect issues like overfitting. If the cross validation performance is consistently poor or below a certain threshold, it indicates a problem with the training process or the new data, and the pipeline should flag this issue."
  }
]