[
  {
    "question_id":"skillcertpro_e2_q51",
    "question": "Your organization has a model inference pipeline deployed in Databricks that processes real-time data and serves predictions. The pipeline runs multiple times per day but doesn t require continuous operation outside of these scheduled jobs. The company's goal is to minimize infrastructure costs while ensuring that the inference jobs have sufficient resources. You need to decide whether to use a Job cluster or an all purpose cluster to meet this requirement. What is the most appropriate choice?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use a Job cluster because it automatically scales the number of workers based on the size of the data being processed, unlike an all-purpose cluster."},
      {"id": 1, "text": "Use a Job cluster because it allows you to share the same resources across multiple concurrent inference jobs, improving cost efficiency."},
      {"id": 2, "text": "Use a Job cluster because it is automatically terminated after completing the inference job, minimizing resource consumption between runs."},
      {"id": 3, "text": "Use an all-purpose cluster because it is more efficient at running short, repeated tasks without needing to start and stop the cluster each time."}
    ],
    "ans_id":2,
    "explanation":"Use a Job cluster because it is automatically terminated after completing the inference job, minimizing resource consumption between runs. \n\n This is correct. Job clusters in Databricks are ephemeral they are created specifically for a job run and automatically shut down after the job finishes. This behavior is ideal for pipelines that run on a schedule but do not require continuous uptime, as it minimizes infrastructure costs by avoiding idle compute time between jobs. It's a best practice when resource efficiency and cost control are key."
  },
  {
    "question_id":"skillcertpro_e2_q52",
    "question": "You are a data scientist at a financial services firm that operates in both Azure and AWS environments. The company has developed a machine learning model that predicts customer churn. The model was trained on Databricks in Azure using a combination of Spark and Python. Now, the business team wants to deploy the model in a multi-cloud environment to ensure high availability and scalability. The deployment should enable real-time predictions across both AWS and Azure. You are asked to design the model deployment pipeline. Which of the following approaches is the best for deploying your machine learning model in a multi-cloud environment for real-time predictions?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Export the model as a Docker image and deploy it on Kubernetes clusters in both Azure Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS)."},
      {"id": 1, "text": "Deploy the model in Databricks on Azure and directly connect it to the AWS environment for real-time predictions via APIs."},
      {"id": 2, "text": "Deploy the model using MLflow's model registry and use it to serve the model directly on both Azure and AWS platforms."},
      {"id": 3, "text": "Deploy the model as a REST API in Azure ML and use an Azure function to serve the model predictions across both AWS and Azure."}
    ],
    "ans_id":0,
    "explanation":"Export the model as a Docker image and deploy it on Kubernetes clusters in both Azure Kubernetes Service (AKS) and Amazon Elastic Kubernetes Service (EKS).\n\n This is correct. Exporting the model as a Docker image and deploying it on Kubernetes clusters in both AKS and EKS offers the most flexible and scalable multi-cloud deployment approach. This method ensures high availability, resilience, and the ability to serve real-time predictions in both Azure and AWS environments. It also aligns well with modern DevOps and MLOps practices, enabling containerized, cloud-agnostic deployments."
  },
  {
    "question_id":"skillcertpro_e2_q53",
    "question": "You have a machine learning model that was originally deployed in a batch pipeline. Now, you want to convert this deployment pipeline into a real-time streaming pipeline using Databricks. The model consumes structured data for real-time predictions. Which of the following is the most effective approach to convert the batch pipeline into a streaming pipeline for real-time predictions?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Modify the pipeline to read from a streaming source, remove the batch intervals, and use checkpointing to ensure fault tolerance."},
      {"id": 1, "text": "Modify the pipeline to use Structured Streaming with micro-batches and apply continuous trigger processing for low-latency predictions."},
      {"id": 2, "text": "Modify the pipeline to read from a streaming source, maintain existing batch intervals, and write the output to a static sink."},
      {"id": 3, "text": "Modify the batch pipeline by adding a sliding window function to simulate streaming behavior and predict in near-real-time."}
    ],
    "ans_id":1,
    "explanation":"Modify the pipeline to use Structured Streaming with micro-batches and apply continuous trigger processing for low-latency predictions.\n\n This is correct. Databricks supports Structured Streaming, which processes data in micro-batches, making it well-suited for real-time inference. Using a continuous trigger reduces latency further, enabling near real time predictions. This is the most effective and production-ready method to transition from batch to streaming in Databricks for structured data."
  },
  {
    "question_id":"skillcertpro_e2_q54",
    "question": "You are working on deploying a machine learning model to predict customer behavior in real-time on a popular e-commerce platform. The platform experiences large fluctuations in traffic, especially during sales events. The model must be able to handle unpredictable spikes in traffic while ensuring that predictions remain fast and reliable. You decide to use a cloud-based solution with RESTful APIs in containers. Which feature of cloud-provided RESTful services in containers makes them ideal for handling unpredictable spikes in traffic for real-time model deployments?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "RESTful services prioritize traffic management by ensuring requests are queued and processed in order, avoiding any service disruptions during high traffic."},
      {"id": 1, "text": "Cloud-provided RESTful services automatically replicate model instances in parallel, ensuring no delays during traffic spikes, with containers isolating each request to prevent interference between predictions."},
      {"id": 2, "text": "Cloud-provided RESTful services and containers decouple the model from the application logic, allowing the model to scale horizontally in response to traffic surges."},
      {"id": 3, "text": "Containers provide an abstraction layer that automatically balances GPU and CPU utilization across all requests, optimizing inference times during high-traffic events."}
    ],
    "ans_id":2,
    "explanation":"Cloud-provided RESTful services and containers decouple the model from the application logic, allowing the model to scale horizontally in response to traffic surges. \n\n This option accurately describes the benefit of using cloud-provided RESTful services in containers for handling unpredictable traffic spikes. Decoupling the model deployment (within containers and exposed via a REST API) from the main application allows the model service to be scaled independently. When traffic increases, the cloud platform can automatically provision more instances (horizontal scaling) of the containerized model service to handle the increased load without impacting the application itself."
  },
  {
    "question_id":"skillcertpro_e2_q55",
    "question": "You are deploying a machine learning model in a batch pipeline using the score_batch operation in Databricks. The dataset consists of millions of rows, and the batch operation is meant to score each row efficiently. What is the primary practical benefit of using the score_batch operation in this scenario?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "It allows the model to be scored only on a single machine, which reduces complexity."},
      {"id": 1, "text": "It allows the model to be trained and deployed in a single step within the same operation."},
      {"id": 2, "text": "It enables distributed scoring across multiple nodes, leveraging Spark s parallelism to process large datasets efficiently."},
      {"id": 3, "text": "It ensures that the entire dataset is loaded into memory before processing, thus preventing any disk I/O during scoring."}
    ],
    "ans_id":2,
    "explanation":"It enables distributed scoring across multiple nodes, leveraging Spark s parallelism to process large datasets efficiently.\n\n The score_batch operation in Databricks is designed to efficiently handle large datasets by leveraging the distributed computing capabilities of Apache Spark. When you use score_batch, the input data is partitioned and processed in parallel across the Spark cluster's nodes. This significantly reduces the overall time required to score millions of rows compared to processing the data on a single machine."
  },
  {
    "question_id":"skillcertpro_e2_q56",
    "question": "Your team is managing a large number of model versions, some of which are outdated and need to be either archived for future reference or deleted to conserve resources. You need to ensure that you follow best practices when archiving or deleting models, keeping in mind regulatory compliance and the importance of tracking historical models. Which of the following statements correctly describe how model version archiving and deletion are handled in MLflow? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Deleting a model version removes all associated metadata and model artifacts permanently."},
      {"id": 1, "text": "Archived models can be permanently deleted from the MLflow Model Registry by an administrator."},
      {"id": 2, "text": "The MLflow Model Registry supports versioning of deleted models, allowing a model to be restored to a prior state after deletion."},
      {"id": 3, "text": "A model must be in the 'Archived' stage before it can be deleted from the Model Registry."},
      {"id": 4, "text": "Once a model is deleted, its history and metadata are preserved for auditing purposes."}
    ],
    "ans_ids":[0,3],
    "explanation":"Deleting a model version removes all associated metadata and model artifacts permanently.\n When a model version is deleted in MLflow, it is a permanent operation. All the associated metadata, such as the creation time, tags, run ID, and the actual model artifacts stored in the artifact repository, are permanently removed and cannot be recovered through the MLflow UI or API. This is a crucial consideration for compliance and tracking.\n\n A model must be in the 'Archived' stage before it can be deleted from the Model Registry. \nMLflow enforces a two-step process for permanently removing model versions. Before a model version can be deleted, it must first be transitioned to the 'Archived' stage. This acts as a safeguard against accidental deletion and provides a clear indication that a model version is no longer intended for active use but might still need to be retained for some period before permanent removal."
  },
  {
    "question_id":"skillcertpro_e2_q57",
    "question": "You are working with a fraud detection model in a financial institution. The model was trained on historical fraud data, but due to changes in fraud schemes, the institution has recently updated the definition of what constitutes fraudulent activity. As a result, you suspect label drift. Which of the following scenarios best reflects label drift?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The ground truth labels (fraud or not fraud) in the evaluation set no longer align with how the labels were defined during training."},
      {"id": 1, "text": "The input features, such as transaction amount and transaction time, have changed in distribution, even though the fraud definition remains the same."},
      {"id": 2, "text": "The performance of the model has decreased because the fraud detection system has been updated to use different features."},
      {"id": 3, "text": "The model is incorrectly identifying more fraudulent cases because customers have started to change their spending patterns."}
    ],
    "ans_id":0,
    "explanation":"The ground truth labels (fraud or not fraud) in the evaluation set no longer align with how the labels were defined during training.\n\nLabel drift occurs when the meaning or definition of the target variable (in this case, whether a transaction is fraudulent or not) changes over time. If the institution has updated its definition of fraud, then the labels in new, unseen data (like the evaluation set) will be based on this new definition. This means that transactions labeled as 'fraud' in the new data might not have been labeled as 'fraud' under the old definition used during training, and vice versa. This misalignment between the training labels and the current reality is the essence of label drift."
  },
  {
    "question_id":"skillcertpro_e2_q58",
    "question": "You are working for a fintech company that uses a machine learning model to predict customer credit scores. The model is trained on a dataset with several numeric features such as income, credit utilization ratio, and loan-to-value ratio. Over time, your team notices that the model's performance is degrading in production. You suspect that some numeric features have drifted, impacting model predictions. You decide to use summary statistic monitoring to detect numeric feature drift. Which of the following approaches best aligns with detecting feature drift using summary statistic monitoring?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Monitoring feature importance scores over time and comparing them with the training dataset."},
      {"id": 1, "text": "Monitoring the statistical mean, standard deviation, and skewness of numeric features across time windows."},
      {"id": 2, "text": "Monitoring the mean and variance of the model's prediction errors for potential feature drift."},
      {"id": 3, "text": "Monitoring the p-values of feature coefficients from the model's training to detect significant changes in feature distribution."}
    ],
    "ans_id":1,
    "explanation":" Monitoring the statistical mean, standard deviation, and skewness of numeric features across time windows.\n Summary statistic monitoring for numeric feature drift involves tracking key statistical properties of the features over different time periods (e.g., daily, weekly, monthly) in the production data and comparing them to the statistics of the training data or previous production data windows. Significant changes in the mean, standard deviation (indicating changes in spread), or skewness (indicating changes in the symmetry of the distribution) of a feature can be strong indicators that the distribution of that feature has drifted."
  },
  {
    "question_id":"skillcertpro_e2_q59",
    "question": "Your team is investigating a sudden drop in the accuracy of your machine learning model after a recent data update. You suspect that the update to the Delta table used for training data introduced some anomalies. Your goal is to find out which update caused the issue by reviewing the changes made to the Delta table over the last week. You also want to reload the version of the table that existed right before the problematic update for further investigation. What should be your approach to identify the problematic update and reload the correct version?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use the VACUUM command to delete recent updates and revert to a version of the table from before the update, then review the changes using the table's history."},
      {"id": 1, "text": "Run a full audit of all records by comparing the current version of the Delta table with an earlier snapshot saved locally, then manually rollback the changes."},
      {"id": 2, "text": "Use SELECT * FROM  to retrieve all records from the current table and filter for records that seem anomalous, then delete those records and reload the previous version."},
      {"id": 3, "text": "Use the DESCRIBE HISTORY command to list all changes made to the Delta table, identify the timestamp of the problematic update, and use Time Travel to query the version of the table before the update"}
    ],
    "ans_id":3,
    "explanation":"Use the DESCRIBE HISTORY command to list all changes made to the Delta table, identify the timestamp of the problematic update, and use Time Travel to query the version of the table before the update \n\n-DESCRIBE HISTORY: Lists all changes to the Delta table with timestamps, user info, and operation types.\n-Identify the update that caused the issue. \n-Time Travel: Query the table as it was before the update using AS OF VERSION or AS OF TIMESTAMP.\n-Efficient for investigating and restoring correct data."
  },
  {
    "question_id":"skillcertpro_e2_q60",
    "question": "You are working on a machine learning experiment using MLflow in Databricks. You have trained multiple models with different hyperparameter settings and want to compare their performance. Which of the following steps is most critical for effectively tracking and comparing these experiment runs in MLflow?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Creating a separate experiment for each hyperparameter setting"},
      {"id": 1, "text": "Logging all hyperparameters, metrics, and model artifacts during each run"},
      {"id": 2, "text": "Assigning a unique experiment ID for each run"},
      {"id": 3, "text": "Logging the experiment results only at the end of the experiment"}
    ],
    "ans_id":1,
    "explanation":"Logging all hyperparameters, metrics, and model artifacts during each run \n\n-Hyperparameters: Track model configurations used in each run.\n-Metrics: Record performance (e.g., accuracy, F1-score) for comparison.\n-Artifacts: Save trained models for reproducibility and deployment.\n-Enables easy comparison, selection of best models, and understanding of what works."
  }
]