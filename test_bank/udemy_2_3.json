[
  {
    "question_id":"udemy_e2_q21",
    "question": "A machine learning engineer must choose a deployment approach for an application where feature data isn't available until the moment of request, and each prediction must be delivered with minimal delay on a per-record basis.",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Deploy the model on-device or at the edge"},
      {"id": 1, "text": "Use a streaming inference pipeline"},
      {"id": 2, "text": "None of the above approaches will satisfy these requirements"},
      {"id": 3, "text": "Run the model in batch mode"},
      {"id": 4, "text": "Serve the model in a real-time inference setup"}
    ],
    "ans_id":4,
    "explanation":"Serve the model in a real-time inference setup \n\nWhen feature data only becomes available at request time and you need per-record predictions with minimal latency, a real-time (online) inference deployment is the appropriate paradigm. Databricks's Mosaic AI Model Serving (MLflow Model Serving) provides low-latency REST endpoints that spin up serverless compute, auto-scale, and deliver predictions in milliseconds on a per-request basis"
  },
  {
    "question_id":"udemy_e2_q22",
    "question": "In a scenario where a machine learning engineer needs to deliver predictions in real-time, with feature values available one week before the query time, what is a benefit of utilizing a batch serving deployment over a real-time serving deployment, where predictions are computed at query time?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Batch serving deployment integrates seamlessly with Databricks Machine Learning"},
      {"id": 1, "text": "No discernible advantage exists for batch serving deployments compared to real-time serving deployments"},
      {"id": 2, "text": "Real-time computation of predictions yields more current results"},
      {"id": 3, "text": "Real-time serving deployments do not allow for testing"},
      {"id": 4, "text": "Retrieving stored predictions can be quicker than computing predictions in real-time"}
    ],
    "ans_id":4,
    "explanation":"Retrieving stored predictions can be quicker than computing predictions in real-time \n\nIn this scenario:\n-Feature values are available 1 week before prediction time\n-Predictions must be returned in real-time\nThis means the model can precompute predictions in advance (e.g., nightly or weekly), store them, and serve them instantly when requested.Why batch serving can be beneficial\nBatch serving allows:\n-Computing predictions ahead of time\n-Storing them in a fast lookup table (Delta, SQL, key-value store)\n-Returning predictions instantly, often faster than running the model at request time\nThus, r etrieving a precomputed prediction is typically faster than performing compute-heavy inference on demand."
  },
  {
    "question_id":"udemy_e2_q23",
    "question": "In the context of a machine learning engineering team experiencing slow querying of predictions stored in a Delta table due to sparse row distribution within data files, which optimization technique can expedite the query process by arranging similar records together while considering values in multiple columns?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Implementing Z-Ordering"},
      {"id": 1, "text": "Utilizing Bin-packing"},
      {"id": 2, "text": "Writing data as a Parquet file"},
      {"id": 3, "text": "Employing Data skipping"},
      {"id": 4, "text": "Adjusting file size tuning"}
    ],
    "ans_id":0,
    "explanation":"Implementing Z-Ordering \n\nZ-Ordering is a Delta Lake optimize operation that physically co-locates rows with similar values across one or more columns into the same set of files. When you run, for example,\n*OPTIMIZE my_delta_table\n*ZORDER BY (feature1, feature2);\nDelta Lake reorganizes the data so that files contain clustered ranges of (feature1, feature2) values. This enhances the effectiveness of data-skipping—only files whose min/ max statistics overlap your query predicates need to be scanned—dramatically speeding up queries when matching rows are sparsely distributed across the dataset"
  },
  {
    "question_id":"udemy_e2_q24",
    "question": "A data scientist has configured their MLflow pipeline to automatically record a chart or plot with every run. They now wish to inspect these saved visualizations within Databricks.Which Databricks UI location will display the logged data visualizations?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The Model page for the entry in the MLflow Model Registry"},
      {"id": 1, "text": "The Artifacts panel on the MLflow Experiment overview page"},
      {"id": 2, "text": "It's not possible to view logged visualizations in Databricks"},
      {"id": 3, "text": "The Artifacts panel on the specific MLflow Run details page"},
      {"id": 4, "text": "The Figures section on the specific MLflow Run details page"}
    ],
    "ans_id":3,
    "explanation":"The Artifacts panel on the specific MLflow Run details page \n\nWhen you call mlflow.log_figure() (or mlflow.log_artifact()) inside a run, MLflow stores the plot files as artifacts of that run. In the Databricks MLflow UI, you navigate to the Run details page and expand the Artifacts section to browse and preview all logged files—including your visualizations"
  },
  {
    "question_id":"udemy_e2_q25",
    "question": "A machine learning engineer is planning to deploy a model for real-time serving using MLflow Model Serving. With one model version in each stage of the MLflow Model Registry, the engineer needs to identify which model versions will be available for querying once Model Serving is activated.",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Model versions from Staging, Production, and Archived stages"},
      {"id": 1, "text": "Only model versions from the Production stage"},
      {"id": 2, "text": "Model versions from None, Staging, Production, and Archived stages"},
      {"id": 3, "text": "Model versions from Staging and Production stages"},
      {"id": 4, "text": "Model versions from None, Staging, and Production stages"}
    ],
    "ans_id":3,
    "explanation":"Model versions from Staging and Production stages \n\n When you enable MLflow Model Serving in Databricks, the serving cluster automatically deploys all active model versions—that is, those in the Staging and Production stages—as REST endpoints. You can then query each endpoint by its stage name (for example, /model/<model-name>/Staging/invocations and /model/<model-name>/Production/invocations) without needing to reference numeric version IDs"
  },
  {
    "question_id":"udemy_e2_q26",
    "question": "A data scientist is updating a machine learning model. After making changes to the code locally, they committed and pushed the changes to the project's Git repository. However, the Git repository in the Databricks workspace is outdated. How can the data scientist load the recent changes into Databricks?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Access the Repo Git dialog and activate automatic syncing."},
      {"id": 1, "text": "Access the Repo Git dialog and initiate the “Sync” process."},
      {"id": 2, "text": "Access the Repo Git dialog and execute the “Merge” operation."},
      {"id": 3, "text": "Access the Repo Git dialog and enable automatic pulling."},
      {"id": 4, "text": "Access the Repo Git dialog and perform the “Pull” action."}
    ],
    "ans_id":4,
    "explanation":"Access the Repo Git dialog and perform the “Pull” action. \n\n In Databricks Repos, the Git integration exposes a Pull button in the Git operations dialog that fetches and merges the latest commits from the remote branch into your workspace files. Clicking Pull updates notebooks and other files in the Repo to match the remote Git repository, ensuring your Databricks workspace reflects your recent local commits"
  },
  {
    "question_id":"udemy_e2_q27",
    "question": "A data scientist has computed updated rows containing new feature values for primary keys already stored in the Feature Store table 'features'. The updated feature values are stored in a DataFrame called 'features_df'. The goal is to update the rows in the 'features' table if the associated primary key exists in 'features_df'. If a row's primary key is not in 'features_df', it should remain unchanged in the 'features' table. Which code block using the Feature Store Client fs can achieve this task?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "fs.write_table(name='features', df=features_df, mode='merge')"},
      {"id": 1, "text": "fs.write_table(name='features', df=features_df, mode='overwrite')"},
      {"id": 2, "text": "fs.write_table(name='features', df=features_df)"},
      {"id": 3, "text": "fs.create_table(name='features', df=features_df, mode='append')"},
      {"id": 4, "text": "fs.refresh_table(name='features', df=features_df, mode='overwrite')"}
    ],
    "ans_id":0,
    "explanation":"fs.write_table(name='features', df=features_df, mode='merge')\n\n Using FeatureStoreClient.write_table() with mode='merge' updates only those rows in the existing feature table whose primary keys match those in features_df. Any rows in the table whose keys are not present in features_df remain unchanged, exactly fulfilling the requirement to selectively refresh feature values"
  },
  {
    "question_id":"udemy_e2_q28",
    "question": "A senior machine learning engineer is configuring a machine learning pipeline. They have automated the process to transition a new version of a registered model to the Production stage in the Model Registry once it successfully passes all tests using the MLflow Client API. Which operation was utilized to transition the model to the Production stage?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "client.update_model_stage"},
      {"id": 1, "text": "client.transition_model_version_stage"},
      {"id": 2, "text": "client.transition_model_version"},
      {"id": 3, "text": "client.update_model_version"}
    ],
    "ans_id":1,
    "explanation":"The MLflow Python client's transition_model_version_stage() method is the documented API for moving a specific model version into a new stage (e.g., Production). You simply provide the registered model's name, the version number, and the target stage. This call transitions the version into Production "
  },
  {
    "question_id":"udemy_e2_q29",
    "question": "A machine learning team aims to integrate the Python library 'newpackage' across all their projects, utilizing a shared cluster for their work. What approach would ensure that the Python library 'newpackage' is accessible to all notebooks executed on the cluster?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Edit the cluster configuration to utilize the Databricks Runtime for Machine Learning."},
      {"id": 1, "text": "Set the Spark session's runtime-version variable to 'ml'."},
      {"id": 2, "text": "Execute %pip install newpackage once on any notebook connected to the cluster."},
      {"id": 3, "text": "Include /databricks/python/bin/pip install newpackage in the cluster's bash init script."},
      {"id": 4, "text": "There is no method to enable the 'newpackage' library on a cluster."}
    ],
    "ans_id":3,
    "explanation":"Include /databricks/python/bin/pip install newpackage in the cluster's bash init script. \n\n A cluster-scoped init script runs on every node (driver and executors) when the cluster starts, installing your Python package before any Spark or Python processes launch. By placing a line such as:\n\n/databricks/python/bin/pip install newpackage\n\n in the cluster's Init Scripts configuration, you ensure that all notebooks—and all Spark jobs—on that shared cluster have access to newpackage without manual per-notebook installs"
  },
  {
    "question_id":"udemy_e2_q30",
    "question": "A data scientist has developed a two-class decision tree classifier using Spark ML and generated predictions stored in a Spark DataFrame named preds_df. The DataFrame has the following schema: prediction DOUBLE actual DOUBLE\n\n Which of the following code blocks can accurately compute the model's accuracy based on the data in preds_df and assign it to the accuracy variable?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "accuracy = RegressionEvaluator(predictionCol='prediction',labelCol='actual',metricName='accuracy').evaluate(preds_df)"},
      {"id": 1, "text": "accuracy = MulticlassClassificationEvaluator(predictionCol='prediction',labelCol='actual',metricName='accuracy').evaluate(preds_df)"},
      {"id": 2, "text": "classification_evaluator = BinaryClassificationEvaluator(predictionCol='prediction',labelCol='actual',metricName='accuracy')\naccuracy = classification_evaluator.evaluate(preds_df)"},
      {"id": 3, "text": "Summarizer(predictionCol='prediction',labelCol='actual',metricName='accuracy').evaluate(preds_df)"}
    ],
    "ans_id":1,
    "explanation":"accuracy = MulticlassClassificationEvaluator(predictionCol='prediction',labelCol='actual',metricName='accuracy').evaluate(preds_df)\n\nExplanation \n*Since this is a two-class (binary) classification model, accuracy can be computed as the proportion of correct predictions among all predictions.\n*In Spark MLlib, the appropriate evaluator for computing classification metrics such as accuracy, precision, recall, or F1-score is the MulticlassClassificationEvaluator — even for binary classification tasks when you're computing accuracy.\n*The BinaryClassificationEvaluator supports only metrics such as 'areaUnderROC' and 'areaUnderPR', not 'accuracy', so using it with metricName='accuracy' would cause an error.\n*The RegressionEvaluator is designed for regression tasks and does not support classification metrics.\n*The Summarizer class is used for statistical summaries, not evaluation of classification performance."
  }
]