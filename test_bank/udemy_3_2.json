[
  {
    "question_id":"udemy_e3_q11",
    "question": "A machine learning engineer has deployed a recommendation model using MLflow Model Serving. They now need to send a request specifically to the model version that is currently assigned to the Staging stage in the MLflow Model Registry.\n\nWhich model URI would allow them to query that staged model version?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "You must know the version number of the model currently promoted to Production to perform this action."},
      {"id": 1, "text": "https://model-serving/recommender/Staging/invocations"},
      {"id": 2, "text": "Accessing a model version in the Staging stage is not supported."},
      {"id": 3, "text": "https://model/recommender/Staging/invocations"}
    ],
    "ans_id":1,
    "explanation":"https://<databricks-instance>/model-serving/recommender/Staging/invocations: Databricks Model Serving exposes models through the following endpoint pattern:\n\nhttps://<databricks-instance>/model-serving/<model-name>/<stage>/invocations\n\nThis allows you to directly invoke the version of a model assigned to a given stage (Staging, Production), without needing to reference the version number.This endpoint is part of Databricks' managed serverless model serving."
  },
  {
    "question_id":"udemy_e3_q12",
    "question": "A machine learning engineer has trained a model and logged it using FeatureStoreClient fs, with the saved model referenced by model_uri. They now need to run batch inference on the original training dataset that was logged with the model. However, the spend feature has been updated since training, and the latest values are available in the customer-level Spark DataFrame spark_df. The customer_id column is the primary key in both spark_df and the training dataset used during model training.\n\nWhich code snippet will generate predictions for the training set while replacing its outdated spend values with the updated ones from spark_df?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "fs.score_batch(model_uri, spark_df)"},
      {"id": 1, "text": "fs.score_model(model_uri, spark_df)"},
      {"id": 2, "text": "df = fs.get_updated_feature(spark_df, model_uri)\nfs.score_batch(model_uri, df)"},
      {"id": 3, "text": "df = fs.get_updated_features(spark_df)\nfs.score_batch(model_uri, df)"}
    ],
    "ans_id":0,
    "explanation":"fs.score_batch(model_uri, spark_df). In Databricks Feature Store, fs.score_batch() performs the following automatically:\n\n1.Looks up the model's training feature table(s).\n2.Resolves the primary key (customer_id) from the input DataFrame (spark_df).\n3.Fetches all required features from the Feature Store, including:\n*Updated features available in spark_df (e.g., the updated spend)\n*Historical features from the original feature tables\n4.Constructs the complete feature vector required by the model.\n5.Runs batch inference.\nBecause spark_df includes the updated spend values and the primary key, the Feature Store automatically merges:\n*Updated features from spark_df\n*Original features from feature tables\n\nThis is precisely the intended behavior for running batch inference with updated feature values.No manual feature merging is required — Feature Store handles it."
  },
  {
    "question_id":"udemy_e3_q13",
    "question": "Which mechanism can be used to automatically trigger a test job whenever a new model version is added to the MLflow Model Registry?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "MLflow Model Registry user interface"},
      {"id": 1, "text": "MLflow Client API"},
      {"id": 2, "text": "MLflow Model Registry webhooks"},
      {"id": 3, "text": "MLflow REST API"}
    ],
    "ans_id":2,
    "explanation":"MLflow Model Registry webhooks\n\nMLflow Model Registry webhooks allow you to automatically trigger external actions whenever specific events occur in the Model Registry — including:\n*A new model version being created\n*A model version transitioning to a new stage\n*A model being archived\n\nBy creating a webhook that listens for the 'MODEL_VERSION_CREATED' event, you can automatically trigger:\n*A CI/CD workflow\n*A Databricks job\n*A custom test job\n*Any external service endpoint"
  },
  {
    "question_id":"udemy_e3_q14",
    "question": "Which of the following statements correctly describes built-in, library-specific MLflow model flavors?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "These flavors are mandatory if you want to use model signatures"},
      {"id": 1, "text": "These flavors enable exporting models in their native library format"},
      {"id": 2, "text": "These flavors make models universally compatible with any ML library"},
      {"id": 3, "text": "These flavors are restricted to logging models only"}
    ],
    "ans_id":1,
    "explanation":"These flavors enable exporting models in their native library format\n\nMLflow provides built-in, library-specific model flavors (such as mlflow.sklearn, mlflow.xgboost, mlflow.pytorch, etc.) so that models can be:\n*saved\n*logged\n*loaded\n*served\nin the native format of the library that created them.\nThis ensures that models retain full compatibility with the original framework while also gaining MLflow features like tracking, serving, and reproducibility."
  },
  {
    "question_id":"udemy_e3_q15",
    "question": "A machine learning engineer has set up a webhook using the following configuration:\n\njob_json = {\n'model_name': model,\n'events': ['MODEL_VERSION_TRANSITIONED_TO_PRODUCTION'],\n'description': 'Job webhook trigger',\n'status': 'Active',\n'job_spec': {\n'job_id': job_id,\n'workspace_url': url,\n'access_token': token\n}\n}\nresponse = http_request(\nhost_creds=host_creds,\nendpoint=endpoint,\nmethod='POST',\njson=job_json\n)\n\nThe webhook is configured to activate whenever a model version is moved into the Production stage.Which code snippet will trigger this webhook and therefore run the associated job?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "client.transition_model_version_stage(name=model,version=model_version,stage='Production')"},
      {"id": 1, "text": "client.transition_model_version_stage(name=new_model,version=model_version,stage='Production')"},
      {"id": 2, "text": "client.transition_model_version_stage(name=model,version=model_version,from='None',to='Production')"},
      {"id": 3, "text": "client.transition_model_stage(name=new_model,version=model_version,stage='Production')"}
    ],
    "ans_id":0,
    "explanation":"client.transition_model_version_stage(name=model,version=model_version,stage='Production')\n\n 'model_name': model,\n'events': ['MODEL_VERSION_TRANSITIONED_TO_PRODUCTION']\nThis means:\n*The webhook listens specifically for this model (the variable model)\n*It triggers exactly when any version of this model transitions into the Production stage\nTherefore, the only code snippet that will activate the webhook is the one that:\n1.Uses the same model name (model)\n2.Transitions the model version to 'Production'"
  },
  {
    "question_id":"udemy_e3_q16",
    "question": "A machine learning engineer needs to promote a specific model version (model_version) of an MLflow Model Registry model (model) from Staging to Production using the MLflow client.\n\nWhich snippet will correctly perform this stage transition?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "client.transition_model_stage(\nname=model,\nversion=model_version,\nfrom='Staging',\nto='Production'\n)"},
      {"id": 1, "text": "client.transition_model_version_stage(\nname=model,\nversion=model_version,\nfrom='Staging',\nto='Production'\n)"},
      {"id": 2, "text": "client.transition_model_stage(\nname=model,\nversion=model_version,\nstage='Production'\n)"},
      {"id": 3, "text": "client.transition_model_version_stage(\nname=model,\nversion=model_version,\nstage='Production'\n)"}
    ],
    "ans_id":3,
    "explanation":"client.transition_model_version_stage(\nname=model,\nversion=model_version,\nstage='Production': MLflow provides a single correct API for stage transitions:\n\nclient.transition_model_version_stage(name, version, stage)\n\nYou only specify the target stage ('Production').MLflow automatically handles the transition from the current stage (e.g., Staging) to the new one.This is the officially supported method in the MLflow Client API.)"
  },
  {
    "question_id":"udemy_e3_q17",
    "question": "A data scientist is using MLflow to track machine learning experiments. After finishing a run identified by run_id within the experiment exp_id, they want to retrieve the metrics logged for that specific run using an existing MLflow client and an active Spark session spark.\n\nWhich code snippet will correctly return the logged metrics for run_id?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "client.get_run(exp_id.run_id).data.metrics"},
      {"id": 1, "text": "mlflow.search_runs(exp_id, order_by=['metrics.rmse DESC'])[0]"},
      {"id": 2, "text": "client.get_run(run_id).data.metrics"},
      {"id": 3, "text": "spark.read.format('mlflow-run').load(run_id)"}
    ],
    "ans_id":2,
    "explanation":"To retrieve metrics for a specific MLflow run, you use:\nclient.get_run(run_id)\nThis returns a Run object, and the metrics are stored under:\n.data.metrics\nSo the full and correct way to fetch logged metrics is:\nclient.get_run(run_id).data.metrics\nThis matches the official MLflow client API."
  },
  {
    "question_id":"udemy_e3_q18",
    "question": "A machine learning engineer has built a pipeline that outputs a scikit-learn model (model) along with evaluation metrics for the test set: RMSE (rmse), MAE (mae), and R-squared (r2). These metric values have been collected into a dictionary named metrics. The engineer wants to record these metrics in an MLflow run and executes:\n\nwith mlflow.start_run(experiment_id=exp_id, run_name=run_name) as run:\n# Log metrics\nmlflow.log_metric(metrics)\n\nThis code results in an error.What modification will allow the metrics to be logged correctly?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Replace mlflow.log_metric with mlflow.sklearn.log_metric"},
      {"id": 1, "text": "Replace metrics with model"},
      {"id": 2, "text": "Replace log_metric with log_metrics"},
      {"id": 3, "text": "Replace metrics with rmse, mae, r2"}
    ],
    "ans_id":2,
    "explanation":"Replace log_metric with log_metrics \n\nmlflow.log_metric() logs one metric at a time, using:\nmlflow.log_metric(key, value)\nBut the engineer is passing a dictionary, which causes an error.To log multiple metrics at once from a dictionary, MLflow provides:\nmlflow.log_metrics(metrics_dict)\nSo the correct statement is:\nmlflow.log_metrics(metrics)\nThis logs all metrics (RMSE, MAE, R²) in a single call."
  },
  {
    "question_id":"udemy_e3_q19",
    "question": "A data scientist has generated updated feature rows for primary keys that already exist in the Feature Store table features. These new feature values are stored in the DataFrame features_df. The goal is to update only the rows in features whose primary keys appear in features_df, while leaving all other rows untouched.\n\nWhich Feature Store Client (fs) operation will correctly apply these updates?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "fs.write_table(name='features',df=features_df,mode='append')"},
      {"id": 1, "text": "fs.write_table(name='features',df=features_df,mode='overwrite')"},
      {"id": 2, "text": "fs.refresh_table(name='features',df=features_df,mode='overwrite')"},
      {"id": 3, "text": "fs.write_table(name='features',df=features_df,mode='merge')"}
    ],
    "ans_id":3,
    "explanation":"fs.write_table(name='features',df=features_df,mode='merge') \n\nThis mode performs:\n*Upsert behavior = update rows whose primary key exists\n*Insert behavior = optionally add new rows if keys do not exist\n*Leave untouched = rows not referenced in features_df remain unchanged"
  },
  {
    "question_id":"udemy_e3_q20",
    "question": "A data scientist needs to load the contents of the Feature Store table table from the dev database as a Spark DataFrame. They have access to the Feature Store Client fs.\n\nWhich line of code will correctly retrieve this table as a Spark DataFrame?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "fs.get_table('table')"},
      {"id": 1, "text": "fs.read_table('dev.table')"},
      {"id": 2, "text": "fs.create_table('dev.table')"},
      {"id": 3, "text": "fs.get_table('dev.table')"}
    ],
    "ans_id":1,
    "explanation":"fs.read_table('dev.table')\n\n To load a Feature Store table as a Spark DataFrame, the Feature Store Client provides the method:\nfs.read_table('<database>.<table_name>')\nSo to read the table table from the dev database:\nfs.read_table('dev.table') \nThis returns a Spark DataFrame containing all rows of the Feature Store table."
  }
]