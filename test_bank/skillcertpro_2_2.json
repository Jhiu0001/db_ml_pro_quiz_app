[
  {
    "question_id":"skillcertpro_e2_q11",
    "question": "You are tasked with deploying a machine learning model that continuously predicts future stock prices based on incoming data streams. The deployment uses Databricks' MLflow integration for streaming inferences. The model needs to predict the stock price every minute and write predictions to a time-based prediction store, while ensuring low-latency and consistency for real-time decision-making. Which two of the following strategies would ensure efficient, consistent, and low-latency continuous predictions in a time based prediction store during streaming deployment? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Set the stream trigger interval to 'Once' to process each new batch of data as soon as it arrives, minimizing latency."},
      {"id": 1, "text": "Use checkpointing for intermediate state storage to ensure fault tolerance and consistency across streaming predictions."},
      {"id": 2, "text": "Deploy the model with MLflow's predict method in real-time streaming mode, writing predictions directly to the time-series database."},
      {"id": 3, "text": "Use a micro-batching system to aggregate incoming data every 5 minutes before generating predictions and writing them to the store."},
      {"id": 4, "text": "Scale the model deployment by adding more workers to ensure each streaming partition is processed independently."}
    ],
    "ans_ids":[1,2],
    "explanation":"Use checkpointing for intermediate state storage to ensure fault tolerance and consistency across streaming predictions.\nCheckpointing is crucial in Structured Streaming for maintaining state across micro-batches. In a continuous prediction scenario, the model might need to maintain some state (e.g., for windowing or aggregations). Checkpointing ensures that if the streaming pipeline fails, it can restart from the last consistent state, preventing data loss and ensuring the consistency of the predictions written to the time based store.\n\nDeploy the model with MLflow's predict method in real-time streaming mode, writing predictions directly to the time-series database. MLflow supports deploying models for real-time inference in streaming applications. By integrating the model's predict method directly into the Structured Streaming pipeline, predictions can be generated and written to the time-series database as each micro-batch of data is processed. This minimizes latency as predictions are generated close to the arrival time of the data."
  },
  {
    "question_id":"skillcertpro_e2_q12",
    "question": "You are maintaining a customer churn prediction model in production for an e-commerce platform. The model was initially trained on customer behavior data collected over the past year. However, after six months, you notice that the model s accuracy has decreased from 85% to 60%. Additionally, after analyzing recent prediction errors, you observe that the customer demographics and spending patterns have changed significantly since the model was deployed. What type of drift is most likely affecting the model's performance, and what approach would be most appropriate to resolve it?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Concept drift The statistical relationship between the input features (e.g., customer spending, browsing history) and the target variable (churn) has changed over time."},
      {"id": 1, "text": "Data shift  The input data has changed completely, and the features used during training are no longer relevant in the current data."},
      {"id": 2, "text": "Covariate drift  The relationship between the input features and the target (churn) has remained the same, but the distribution of the input features (such as customer spending habits) has changed."},
      {"id": 3, "text": "Model degradation  The model s performance has decreased due to factors like overfitting to the training data, without any changes in the input or target distribution."}
    ],
    "ans_id":0,
    "explanation":"Concept drift The statistical relationship between the input features (e.g., customer spending, browsing history) and the target variable (churn) has changed over time.\n-Concept Drift: The scenario describes a situation where the customer demographics and spending patterns have changed significantly, leading to a decrease in the model's accuracy. This indicates that the underlying relationship between the input features and the likelihood of churn has changed. The model, trained on older patterns, is no longer accurately capturing the current drivers of churn. This is the definition of concept drift."
  },
  {
    "question_id":"skillcertpro_e2_q13",
    "question": "You are setting up an HTTP webhook in the MLflow Model Registry to notify an external CI/CD pipeline when a model transitions from Staging to Production. This pipeline automatically tests and deploys models. To correctly configure the webhook, you need to provide a valid URL where the webhook will send notifications. Where should the webhook URL come from?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The webhook URL is an endpoint provided by the Databricks platform that automatically monitors model lifecycle changes."},
      {"id": 1, "text": "The webhook URL should be generated by MLflow when you register a new model version in the Model Registry."},
      {"id": 2, "text": "The webhook URL is an externally provided endpoint from the external service or system that will receive the event notifications."},
      {"id": 3, "text": "You need to create a custom URL using Databricks APIs and host it within your Databricks workspace to receive the webhook events."}
    ],
    "ans_id":2,
    "explanation":"The webhook URL is an externally provided endpoint from the external service or system that will receive the event notifications.\n\n When you configure a webhook in the MLflow Model Registry to notify an external CI/CD pipeline, the webhook needs to send an HTTP POST request to a specific URL. This URL must be an endpoint provided by the external CI/CD system or a service that acts as an intermediary to trigger the pipeline. MLflow sends the notification to this external URL whenever the specified event (model transition to Production) occurs."
  },
  {
    "question_id":"skillcertpro_e2_q14",
    "question": "Your team is tasked with deploying a machine learning model built in PyTorch on Databricks to multiple environments, including local development, an on-premises cluster, and cloud-based platforms. You want to ensure that the model is easily portable and that different teams using different machine learning frameworks can utilize it without needing extensive code changes. Which of the following describes how MLflow flavors can be leveraged in this scenario, and what benefits they provide?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "MLflow flavors enable seamless model deployment by encapsulating not only the model but also key information about how it should be run, such as the necessary libraries and data preprocessing steps."},
      {"id": 1, "text": "MLflow flavors ensure that models built using different machine learning libraries (such as PyTorch, TensorFlow, or Scikit-learn) are stored in a common format, making them portable across environments"},
      {"id": 2, "text": "Flavors in MLflow are only relevant for cloud-based deployments, and they are not necessary for local or on-premise deployments."},
      {"id": 3, "text": "MLflow flavors only apply to the model itself and do not log environment dependencies, making it the user's responsibility to manage package installations."}
    ],
    "ans_id":0,
    "explanation":"A. MLflow flavors enable seamless model deployment by encapsulating not only the model but also key information about how it should be run, such as the necessary libraries and data preprocessing steps.\n\n MLflow flavors are a convention for packaging models from different machine learning libraries (like PyTorch) along with metadata that describes how to load and use them. This metadata includes information about the model's dependencies (e.g., specific versions of PyTorch and other libraries) and can also include instructions or artifacts related to preprocessing. This encapsulation makes the model portable and easier to deploy across different environments because MLflow can handle the loading and execution based on the flavor's specifications."
  },
  {
    "question_id":"skillcertpro_e2_q15",
    "question": "You are managing several machine learning models for a production pipeline using Databricks and MLflow. The models are at different stages of development, testing, and deployment, and you want to track their lifecycle effectively by assigning them to appropriate stages in MLflow. Which of the following statements correctly describe the available model stages in Databricks MLflow and their usage? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "The 'Staging' stage is used to test models in a pre-production environment and ensures that they meet performance criteria before moving to production."},
      {"id": 1, "text": "Once a model is moved to the 'Archived' stage, it cannot be restored or reused in any environment without retraining."},
      {"id": 2, "text": "The 'Staging' stage prevents model inference to ensure no untested models are served in any environment."},
      {"id": 3, "text": "The 'Production' stage is reserved for models that are actively being served in a production environment and used by live applications."},
      {"id": 4, "text": "Models in the 'Archived' stage are still active but can only be accessed by administrators for fine tuning."}
    ],
    "ans_ids":[0,3],
    "explanation":"The 'Staging' stage is used to test models in a pre-production environment and ensures that they meet performance criteria before moving to production.\n The 'Staging' stage in MLflow is intended for models that have passed initial development and are undergoing testing and validation in an environment that mirrors production. This allows teams to assess the model's performance, stability, and integration with other systems before promoting it to the live 'Production' stage.\n\nThe 'Production' stage is reserved for models that are actively being served in a production environment and used by live applications.\n Models in the 'Production' stage are those that have been validated and approved for use in the live system. This stage clearly marks the models that are currently powering production applications, making it easy to identify the active version."
  },
  {
    "question_id":"skillcertpro_e2_q16",
    "question": "Your team is focused on cost optimization for your machine learning workloads. You are considering using job clusters for scheduled model training and inference tasks. Which of the following is the most significant cost advantage of job clusters compared to all-purpose clusters in Databricks?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Job clusters only run when a scheduled job is active and are automatically terminated afterward, reducing idle resource costs."},
      {"id": 1, "text": "Job clusters offer higher computational power at a lower cost per hour compared to all-purpose clusters, resulting in better price-performance ratios."},
      {"id": 2, "text": "Job clusters can be manually paused and resumed, allowing you to control when resources are being used and reduce downtime expenses."},
      {"id": 3, "text": "Job clusters provide lower costs due to their ability to reuse hardware across different tasks and workflows, ensuring maximum utilization."}
    ],
    "ans_id":0,
    "explanation":"Job clusters only run when a scheduled job is active and are automatically terminated afterward, reducing idle resource costs.\nThis is the most significant cost advantage of job clusters. They are ephemeral, meaning they are created at the start of a job and automatically shut down once the job completes. This eliminates the cost of paying for compute resources when they are not actively being used, which is a common issue with always-on all-purpose clusters."
  },
  {
    "question_id":"skillcertpro_e2_q17",
    "question": "You are building a fraud detection system for an e-commerce platform, where each transaction must be evaluated for fraud risk in real-time. The system processes a small but steady stream of transaction data, and predictions need to be made quickly to prevent fraudulent transactions from being completed. In this case, real-time inference is critical to meeting the performance requirements. Why is real-time inference particularly beneficial for low-throughput, high-frequency data streams where fast predictions are necessary?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Real-time inference allows you to accumulate more data before making a decision, improving the accuracy of fraud detection models."},
      {"id": 1, "text": "Real-time inference supports online learning, allowing the model to continuously adapt to changes in the data without the need for retraining."},
      {"id": 2, "text": "Real-time inference reduces the prediction delay, ensuring that actions like flagging fraudulent transactions happen immediately, preventing costly delays."},
      {"id": 3, "text": "Real-time inference optimizes the use of compute resources by predicting only when a threshold number of records has been received, reducing overall computation time."}
    ],
    "ans_id":2,
    "explanation":"Real-time inference reduces the prediction delay, ensuring that actions like flagging fraudulent transactions happen immediately, preventing costly delays.\n In a fraud detection system for e-commerce, the primary goal is to identify and prevent fraudulent transactions before they are completed. Real-time inference processes each transaction as it occurs, generating a fraud risk score almost instantaneously. This low latency is crucial because it allows for immediate actions, such as flagging the transaction for review or blocking it outright, thereby minimizing potential financial losses and preventing delays in fraud intervention."
  },
  {
    "question_id":"skillcertpro_e2_q18",
    "question": "You are maintaining a customer churn prediction model in production. The model was trained using customer interaction data (e.g., click-through rates, purchase history) from last year. Recently, you notice that the input data has begun to differ from what the model was trained on. Which of the following scenarios is most likely an indication of feature drift?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The ground truth labels (churn or not churn) used for evaluation are no longer being recorded as consistently as before."},
      {"id": 1, "text": "The distribution of a key feature, such as average purchase frequency, has shifted significantly from the training data."},
      {"id": 2, "text": "The model's accuracy has dropped significantly, but the input data distribution appears unchanged."},
      {"id": 3, "text": "A new marketing campaign is launched, and you observe changes in customer behavior, but no clear data shifts have occurred."}
    ],
    "ans_id":1,
    "explanation":"The distribution of a key feature, such as average purchase frequency, has shifted significantly from the training data.\n Feature drift (also known as covariate drift or input drift) occurs when the statistical properties of the input features used by the model change over time compared to the data the model was trained on. A significant shift in the distribution of a feature like 'average purchase frequency' indicates that the patterns in customer behavior related to spending habits have evolved, which is a direct example of feature drift."
  },
  {
    "question_id":"skillcertpro_e2_q19",
    "question": "You are asked to deploy a pre-trained machine learning model using spark_udf in a batch pipeline. You want to ensure that each worker node only loads the model once, and that the model can be applied in parallel over the dataset. Which of the following configurations will help you avoid issues related to excessive model loading and inefficient resource usage?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Load the model inside the UDF definition to ensure that it is reloaded each time the UDF is applied to a partition."},
      {"id": 1, "text": "Broadcast the model to all worker nodes using Spark s broadcast variables and apply it using spark_udf."},
      {"id": 2, "text": "Cache the DataFrame before applying the spark_udf to ensure faster access to the data and model."},
      {"id": 3, "text": "Apply the model sequentially on the driver node and use spark_collect() to gather the results."}
    ],
    "ans_id":1,
    "explanation":"Broadcast the model to all worker nodes using Spark s broadcast variables and apply it using spark_udf.\nBroadcasting is a mechanism in Spark for efficiently distributing read-only variables (like a trained machine learning model) to all worker nodes in a cluster. By broadcasting the model, each worker node will have a local copy of the model in its memory, and the spark_udf running on that worker can access the model without needing to reload it for each row in the partition it's processing. This avoids the overhead of repeated model loading and ensures efficient resource utilization for parallel inference."
  },
  {
    "question_id":"skillcertpro_e2_q20",
    "question": "You are working for a retail company that wants to improve its product recommendations by deploying a machine learning model that predicts customer purchase behavior. The model is already trained, and you plan to use it to score customer data in a batch mode using Databricks score_batch operation. The dataset contains information on tens of millions of customers, and your model will be scoring them daily based on the latest available data. You need to ensure the scoring process is efficient and integrates well with the rest of the company's data pipeline, which uses Delta Lake as the storage format. Which of the following is the most practical benefit of using the score_batch operation in this scenario?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Allows for scoring unstructured data such as images and text"},
      {"id": 1, "text": "Efficiently scores large datasets using distributed computing"},
      {"id": 2, "text": "Real-time scoring for individual customer predictions"},
      {"id": 3, "text": "Automatically retrains the model before scoring the data"}
    ],
    "ans_id":1,
    "explanation":"Efficiently scores large datasets using distributed computing \n\n The score_batch operation in Databricks is designed to leverage the distributed computing capabilities of Spark. When you use score_batch on a large dataset like tens of millions of customer records, it automatically distributes the scoring workload across the nodes of your Databricks cluster. This parallel processing significantly speeds up the inference process compared to processing the data on a single machine, making it highly efficient for large-scale batch scoring required for daily updates."
  }
]