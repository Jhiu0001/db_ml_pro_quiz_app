[
  {
    "question_id":"udemy_e2_q31",
    "question": "A Data Scientist at an e-commerce company needs to train multiple regression models for different product categories using Databricks. Each model requires slightly different preprocessing logic, but all share a common pipeline structure for feature transformation and evaluation. The dataset contains tens of millions of records. The team wants to automate this process to train models in parallel for each category without exceeding cluster memory limits. \n\nWhich approach should the scientist use to efficiently implement this training workflow?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Build a single SparkML pipeline and loop over product categories sequentially within the same Spark session."},
      {"id": 1, "text": "Use the Pandas Function API to group data by category, then apply category-specific model training logic within each group in parallel."},
      {"id": 2, "text": "Create a separate Databricks Job for each category and execute them concurrently using job clusters."},
      {"id": 3, "text": "Use Ray to distribute individual models as separate tasks within the same driver process."}
    ],
    "ans_id":2,
    "explanation":"Create a separate Databricks Job for each category and execute them concurrently using job clusters.\n\n This approach is the most scalable and memory-safe for training multiple large regression models in parallel. By isolating each training run into its own job cluster, you avoid competing for executor memory inside a single Spark session, which is critical when working with tens of millions of records per model. It also aligns with Databricks recommended pattern for large-scale parallel ML training workflows."
  },
  {
    "question_id":"udemy_e2_q32",
    "question": "A Machine Learning Engineer is designing a distributed hyperparameter optimization workflow on Databricks for a large XGBoost model. The team wants to leverage Ray for tuning because of its native parallel trial execution and resource-aware scheduling. However, they also need every trial's results logged to MLflow for traceability and later comparison.\n\nWhich configuration ensures minimal overhead while meeting these requirements?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use MLflowCallback with Optuna and configure Ray's RayExecutor to manage distributed trials while logging results to MLflow."},
      {"id": 1, "text": "Implement Ray Tune with MLflowLoggerCallback to automatically log metrics and parameters for each trial in an MLflow experiment."},
      {"id": 2, "text": "Configure Optuna with MLflowStorage to automatically push trial results to the MLflow experiment."},
      {"id": 3, "text": "Use ray.air.Scaler with Databricks SDK to log metrics to Unity Catalog directly instead of MLflow."}
    ],
    "ans_id":1,
    "explanation":"Implement Ray Tune with MLflowLoggerCallback to automatically log metrics and parameters for each Ray trial into an MLflow experiment.Explanation\n\nBecause the workflow is distributed using Ray, the most direct and low-overhead integration for logging trial results is Ray Tune's built-in MLflowLoggerCallback, which: \n*Logs each trial's parameters and metrics as a separate MLflow run\n*Works natively with Ray's distributed executor model\n*Requires no extra coordination layer between Ray and MLflow\n*Preserves MLflow lineage and experiment traceability\n*Avoids Optuna or intermediate storage layers\n\nThis is the cleanest, Ray-native solution for distributed HPO with MLflow logging."
  },
  {
    "question_id":"udemy_e2_q33",
    "question": "A Machine Learning Engineer needs to design an ML pipeline to classify customer feedback into positive, neutral, or negative categories. The data contains 500 million rows of mixed numerical and text features. The model must be retrainable weekly and deployable both in batch inference for trend analysis and streaming mode for real-time dashboard updates.\n\nWhich design approach best satisfies these scalability and use case requirements?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Implement the entire workflow using SparkML, with Tokenizer and HashingTF for text preprocessing, VectorAssembler for feature combination, and LogisticRegression for classification."},
      {"id": 1, "text": "Use scikit-learn's Pipeline API for model training and export the model for deployment in batch and streaming jobs."},
      {"id": 2, "text": "Train the model using PyTorch Lightning and export it as a single-node model for deployment via Model Serving."},
      {"id": 3, "text": "Use Spark UDFs for preprocessing, then feed the data to a pre-trained TensorFlow model for both batch and streaming predictions."}
    ],
    "ans_id":0,
    "explanation":"Implement the entire workflow using SparkML, with Tokenizer and HashingTF for text preprocessing, VectorAssembler for feature combination, and LogisticRegression for classification.\n\n*Scales to 500M rows: Spark ML pipelines run distributed across the cluster for both feature engineering and training.\n*Weekly retraining: Reproducible pipelines (Estimators/Transformers) make scheduled retraining straightforward. \n*Batch + streaming: Use the same fitted pipeline for batch inference and apply the Transformer stage to Structured Streaming data for real-time scoring and dashboards (train in batch; score in streaming)."
  },
  {
    "question_id":"udemy_e2_q34",
    "question": "A financial analytics team is training several machine learning models to predict loan defaults using millions of customer records. The models are trained on different loan segments and require tracking of nested experiment runs: one parent run for the entire pipeline and child runs for each model configuration. The team also wants to store cross-validation metrics, model artifacts, and hyperparameters.\n\nWhich approach should the team use?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use a single flat MLflow run with multiple tags to differentiate experiments."},
      {"id": 1, "text": "Use MLflow nested runs, creating a parent run for the experiment and child runs for each configuration with their respective metrics and artifacts logged."},
      {"id": 2, "text": "Use MLflow Projects to define runs as different jobs with tags identifying configurations."},
      {"id": 3, "text": "Log all models and parameters manually using Databricks CLI inside a single notebook run."}
    ],
    "ans_id":1,
    "explanation":"Use MLflow nested runs, creating a parent run for the experiment and child runs for each configuration, with their respective metrics and artifacts logged. \n\nThis approach is designed exactly for multi-model, multi-configuration tracking where you need:\n*A single parent run to represent the whole training pipeline \n*Child runs for each model/loan-segment configuration\n*Isolation of metrics, hyperparameters, and artifacts per experiment branch\n*Clear lineage and organization in the MLflow UI"
  },
  {
    "question_id":"udemy_e2_q35",
    "question": "A Machine Learning Engineer is designing a distributed pipeline to predict demand forecasts for 500 regional warehouses. The model needs to be trained separately for each region using shared preprocessing logic. The data volume is large and uneven across regions, and the team wants to maximize throughput without bottlenecks.\n\nWhich implementation ensures scalable parallel training and optimal cluster utilization?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Launch multiple Spark sessions, one for each region, within the same Databricks notebook."},
      {"id": 1, "text": "Use the Pandas Function API with groupby().applyInPandas() to distribute training by region and aggregate metrics back to a Spark DataFrame."},
      {"id": 2, "text": "Use foreachBatch() in Spark Structured Streaming to train regional models on incoming data streams."},
      {"id": 3, "text": "Export regional subsets to external storage and train them individually using scikit-learn."}
    ],
    "ans_id":1,
    "explanation":"Use the Pandas Function API with groupby().applyInPandas() to distribute training by region and aggregate metrics back into a Spark DataFrame.\n\nThis approach enables:\n*Parallel model training per region distributed across Spark executors\n*Dynamic workload balancing across uneven data volumes\n*Shared preprocessing logic applied consistently inside the function\n*Return of metrics/artifacts in a structured Spark-native form\n\nIt is the recommended design for large-scale, partition-aware ML workloads on Spark."
  },
  {
    "question_id":"udemy_e2_q36",
    "question": "A Data Scientist is preparing an automated feature engineering pipeline for a fraud detection system. The system must compute features in near real time from streaming transactions and serve them consistently for both training and inference. The team also needs to ensure point-in-time correctness to prevent data leakage when backfilling historical data.\n\nWhich design satisfies all requirements?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Build the pipeline using the FeatureEngineering Client to compute and write features to the Feature Store with online tables enabled for low-latency serving."},
      {"id": 1, "text": "Store computed features in Delta tables and directly join them during model training and inference."},
      {"id": 2, "text": "Use MLflow to log the latest feature values as artifacts for online scoring."},
      {"id": 3, "text": "Run a daily batch job to recompute all features to avoid drift in the online environment."}
    ],
    "ans_id":0,
    "explanation":"Build the pipeline using the Feature Engineering Client to compute and write features to the Databricks Feature Store with online tables enabled for low-latency serving.\n\nThis is the only option that satisfies all three requirements:\n*Near real-time computation from streaming sources\n*Consistent feature serving for both training and inference (offline + online)\n*Point-in-time correctness when backfilling historical data → prevents data leakage\nDatabricks Feature Store (with online tables) is specifically designed to ensure that the same features and lookups used in training are served during inference, with built-in time-travel semantics for correctness."
  },
  {
    "question_id":"udemy_e2_q37",
    "question": "A Machine Learning Engineer wants to compare the efficiency of horizontal versus vertical scaling when training a deep learning model in Databricks. The dataset is large, and the model is GPU-intensive. The engineer must decide which scaling approach to use to minimize training time and cost.\n\nWhich comparison accurately describes the trade-offs?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Vertical scaling adds more worker nodes to increase data parallelism but reduces batch throughput due to serialization."},
      {"id": 1, "text": "Vertical scaling uses multiple nodes to distribute model replicas, improving parallelization but increasing synchronization costs."},
      {"id": 2, "text": "Horizontal scaling increases GPU memory on each node, improving model fit but limiting parallelization."},
      {"id": 3, "text": "Horizontal scaling increases node count and enables distributed training across workers, improving throughput but introducing higher communication overhead."}
    ],
    "ans_id":3,
    "explanation":"Horizontal scaling increases node count and enables distributed training across workers, improving throughput but introducing higher communication overhead.\n\nThis trade-off is accurate because:\n*Horizontal scaling = more nodes, same-size GPU per node → enables data parallelism.\n*Training is faster overall due to parallel execution of batches across workers.\n*But synchronization of gradients between workers adds network / communication overhead."
  },
  {
    "question_id":"udemy_e2_q38",
    "question": "A Machine Learning Engineer is managing multiple Databricks environments — development, staging, and production — for a customer churn prediction pipeline. The team wants to ensure consistent deployment of MLflow models, feature tables, and serving endpoints across all environments. They also require automated promotion of tested models from dev to prod while maintaining full version control and rollback capability.\n\nWhich approach should they adopt to meet these requirements with minimal manual intervention?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Create separate notebooks for each environment and parameterize deployment scripts using environment-specific variables."},
      {"id": 1, "text": "Clone the MLflow experiment workspace for each environment and manually deploy models to Model Serving in production."},
      {"id": 2, "text": "Use Databricks Asset Bundles (DABs) to define all ML assets declaratively and promote the same bundle through different environments using CI/CD."},
      {"id": 3, "text": "Use the Databricks REST API to export and re-import model versions manually between workspaces for each environment."}
    ],
    "ans_id":2,
    "explanation":"Use Databricks Asset Bundles (DABs) to declaratively define all ML assets (models, feature tables, serving endpoints, jobs) and promote the same bundle through dev → staging → prod via CI/CD.\n\nThis approach gives:\n*Reproducible deployments across all environments\n*True environment promotion (not re-creation)\n*Built-in support for rollback\n*Version-controlled infrastructure-as-code model\n*Automated CI/CD-driven deployment without manual UI or REST-based copying"
  },
  {
    "question_id":"udemy_e2_q39",
    "question": "A Data Science team has developed a credit scoring model that will be retrained weekly based on newly arriving data. The MLOps team must ensure that retraining only triggers when either (a) new data arrives or (b) performance metrics degrade beyond a defined threshold. Additionally, retrained models must automatically be evaluated, versioned, and promoted if they outperform the current production model.\n\nWhich solution most effectively implements this automated retraining workflow in Databricks?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Create a scheduled Databricks Job that always retrains the model at fixed intervals, then manually review results for promotion."},
      {"id": 1, "text": "Use Delta Live Tables to trigger a retraining job when new data is detected, combine it with Lakehouse Monitoring to initiate retraining based on model performance drift, and use MLflow for model comparison and versioning."},
      {"id": 2, "text": "Configure Unity Catalog lineage tracking to automatically start retraining whenever a model input dataset changes."},
      {"id": 3, "text": "Use a Databricks Workflow that polls feature tables for updates, retrains the model each time, and overwrites the existing registered model version in MLflow."}
    ],
    "ans_id":1,
    "explanation":"Use Delta Live Tables to trigger retraining when new data is detected, combine it with Lakehouse Monitoring for performance-drift-based retraining triggers, and use MLflow for evaluation, versioning, and automatic promotion.\n\nThis is the only option that supports both:\n*Data-driven retraining (new data arrival → trigger)\n*Quality-driven retraining (metric degradation → trigger)\n*Automated model registry promotion via MLflow comparison\nThis creates a closed-loop production pipeline where retraining occurs only when necessary and automatically promotes a new candidate model only if it outperforms the current one."
  },
  {
    "question_id":"udemy_e2_q40",
    "question": "A financial institution uses Databricks to deploy multiple fraud detection models across business units. Each model is tracked in MLflow and deployed through Model Serving. During a quarterly audit, the compliance team requests that every model's testing strategy — including unit, integration, and end-to-end validations — be documented and automated to prevent human oversight.\n\nWhich design ensures consistent and auditable validation testing across environments?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Execute all tests within a single production workspace to ensure identical environment variables during audit validation."},
      {"id": 1, "text": "Store test notebooks in the Feature Store and execute them via scheduled Databricks Jobs to generate compliance reports."},
      {"id": 2, "text": "Use MLflow to log unit tests as parameters and integration tests as metrics, running them manually before deployment."},
      {"id": 3, "text": "Implement Databricks Repos with notebooks containing embedded unit tests written in pytest, schedule integration tests as Databricks Jobs for end-to-end validation, and log all test results to MLflow as artifacts for traceability."}
    ],
    "ans_id":3,
    "explanation":"Implement Databricks Repos with notebooks containing embedded unit tests written in pytest, schedule integration tests as Databricks Jobs for end-to-end validation, and log all test results to MLflow as artifacts for traceability.\n\nThis approach provides:\n*Version control (Repos → Git-backed test definitions)\n*Automated validation (Jobs → repeatable execution)\n*Environment consistency (CI/CD-style promotion across dev → staging → prod)\n*Auditable lineage (MLflow stores artifacts for each test run)"
  }
]