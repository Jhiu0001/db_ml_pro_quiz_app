[
  {
    "question_id":"udemy_e3_q21",
    "question": "A data scientist wants to log multiple runs of a random forest model while experimenting with different hyperparameters. For each run, they vary the number of trees and the maximum tree depth. Their code looks like this:\n\nwith mlflow.start_run(experiment_id=exp_id, run_name=run_name) as run:\nrf = RandomForestRegressor(**params)\nrf.fit(X, y)\npredictions = rf.predict(X_test)\nmlflow.log_params(params)\nreturn run.info.run_id\n\nTo successfully unpack the keyword arguments and log them with MLflow, what type must params be?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "array"},
      {"id": 1, "text": "PySpark DataFrame"},
      {"id": 2, "text": "dictionary"},
      {"id": 3, "text": "list"}
    ],
    "ans_id":2,
    "explanation":"dictionary (dict) \n\n The code uses two operations that both require keyword argument unpacking:\n\nModel creation\nrf = RandomForestRegressor(**params)\n\nThe ** operator only works when params is a dictionary whose keys are strings representing parameter names and whose values are the parameter values.Logging parameters\nmlflow.log_params(params)\nlog_params() expects a dict of key-value pairs.\nTherefore, params must be a Python dictionary."
  },
  {
    "question_id":"udemy_e3_q22",
    "question": "A machine learning engineer is migrating a Hyperopt-driven hyperparameter search from manual MLflow logging to MLflow Autologging. During this transition, they observe that Autologging does not capture every artifact or detail, especially when using nested runs for each Hyperopt trial. As a result, certain items must still be logged explicitly.\n\nWhich of the following pieces of information must be manually logged when using Hyperopt with MLflow Autologging and nested runs?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Models produced by each Hyperopt trial"},
      {"id": 1, "text": "The status (success/failure) of each trial"},
      {"id": 2, "text": "The best trial's evaluation metric"},
      {"id": 3, "text": "Hyperparameter values explored during trials"}
    ],
    "ans_id":0,
    "explanation":"Models produced by each Hyperopt trial\n\n When using Hyperopt + MLflow autologging, especially with nested runs, MLflow captures:\n*Hyperparameters for each trial\n*Metrics logged during each trial\n*Artifacts produced by supported libraries (e.g., sklearn model if using mlflow.sklearn.autolog())\nHowever, Hyperopt itself does not produce a model object â€” it only returns:\n*The hyperparameters to try\n*The objective function outputs\n*Trial metadata\n\nTherefore, MLflow Autologging does not automatically log the model artifacts created inside a Hyperopt trial unless you explicitly call a flavor-specific logging function, such as:\nmlflow.sklearn.log_model(model, 'model')\ninside the objective function.\nThis is why the engineer must manually log the model generated during each Hyperopt trial if they want that trial's model stored in the MLflow run."
  },
  {
    "question_id":"udemy_e3_q23",
    "question": "A machine learning engineer has configured a webhook using this setup:\n\njob_json = {\n  'model_name': model,\n  'events': ['MODEL_VERSION_TRANSITIONED_TO_STAGING'],\n  'description': 'Job webhook trigger',\n  'status': 'Active',\n  'job_spec': {\n    'job_id': job_id,\n    'workspace_url': url,\n    'access_token': token\n  }\n}\n\nresponse = http_request(\n  host_creds=host_creds,\n  endpoint=endpoint,\n  method='POST',\n  json=job_json\n)\n\nThe webhook is designed to fire whenever a model version enters the Staging stage.Which snippet will trigger this webhook and execute the linked job?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "client.transition_model_version_stage(name=new_model,version=model_version,from='None',to='Staging')"},
      {"id": 1, "text": "client.transition_model_version_stage(name=new_model,version=model_version,stage='Staging')"},
      {"id": 2, "text": "client.transition_model_version_stage(name=model,version=model_version,from='None',to='Staging')"},
      {"id": 3, "text": "client.transition_model_stage(name=new_model,version=model_version,stage='Staging')"},
      {"id": 4, "text": "client.transition_model_version_stage(name=model,version=model_version, stage='Staging')"}
    ],
    "ans_id":4,
    "explanation":"client.transition_model_version_stage(name=model,version=model_version, stage='Staging') \n\nYour webhook is configured to fire only when the model named model transitions into:\n\nMODEL_VERSION_TRANSITIONED_TO_STAGING - This means the only action that will trigger the webhook is:\n*Transitioning a version of that same model (model)\n*Into the Staging stage\n*Using the correct MLflow API method:client.transition_model_version_stage(...)\n*With the correct argument: stage='Staging'"
  },
  {
    "question_id":"udemy_e3_q24",
    "question": "A machine learning engineer is using the following code snippet as part of a batch inference pipeline:\n\ninference_df = (\n  spark.read\n    .schema(schema)\n    .format('delta')\n    .table('inference')\n  )\n\n  predictions_df = inference_df.withColumn(\n     'prediction',\n     predict(*inference_df.columns)\n  )\n\nThey now want this logic to function correctly when the inference table is used as a streaming source rather than a batch source.Which modification is required for the code to work with a streaming Delta table?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Change 'inference' to the full filesystem path of the Delta table"},
      {"id": 1, "text": "Replace .schema(schema) with .option('maxFilesPerTrigger', 1)"},
      {"id": 2, "text": "Replace spark.read with spark.readStream"},
      {"id": 3, "text": "Change .format('delta') to .format('stream')"}
    ],
    "ans_id":2,
    "explanation":"Replace spark.read with spark.readStream \n\nTo use a Delta table as a streaming source, Spark requires the use of the structured streaming API, which begins with:\nspark.readStream\nInstead of:\nspark.read\nEverything else in the code (schema, format, table name, UDF for prediction) can remain the same.Delta Lake fully supports streaming reads as long as the read operation uses readStream."
  }
]