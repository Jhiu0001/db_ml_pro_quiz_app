[
  {
    "question_id":"skillcertpro_e2_q31",
    "question": "A healthcare company has developed a machine learning model to predict patient readmission risks. The model is registered in Databricks' MLflow Model Registry, and they plan to run batch predictions on historical patient data stored in a Delta Lake table. The team needs to ensure that they always use the latest version of the registered model for the batch job. The predictions will be saved back into another Delta Lake table for downstream analysis by the analytics team. Which of the following approaches would ensure the model is loaded from the registry and batch predictions are generated correctly?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use mlflow.pyfunc.load_model() to load the model and then manually convert the patient data from a Spark DataFrame into a Pandas DataFrame to generate predictions."},
      {"id": 1, "text": "Use mlflow.models.load_model() with the specific model URI of the latest version of the registered model for batch inference."},
      {"id": 2, "text": "Use mlflow.spark.load_model() to load the latest version of the model and apply it directly to the Spark DataFrame containing patient data."},
      {"id": 3, "text": "Use mlflow.pyfunc.load_model() to load the latest version of the model from the registry and apply it to the Spark DataFrame containing patient data."}
    ],
    "ans_id":3,
    "explanation":"Use mlflow.pyfunc.load_model() to load the latest version of the model from the registry and apply it to the Spark DataFrame containing patient data.\n\n-mlflow.pyfunc.load_model(): This function loads an MLflow model as a generic Python function (pyfunc). This is the most flexible way to load models, especially when you need to apply them to Spark DataFrames. You can then define a Spark User Defined Function (UDF) that wraps the pyfunc model's predict method, allowing you to apply the model in a distributed manner across the rows of the Spark DataFrame.\n-Loading the Latest Version: To load the latest version, you can use the model URI in the format models://latest. MLflow will automatically resolve this to the most recent version of the registered model.\n-Applying to Spark DataFrame: By wrapping the loaded pyfunc model within a Spark UDF, you can seamlessly apply the model to the Spark DataFrame containing the patient data, leveraging Spark's distributed processing capabilities for efficient batch inference on large datasets. Explanation of why the other options are incorrect:"
  },
  {
    "question_id":"skillcertpro_e2_q32",
    "question": "You are part of a data science team responsible for managing the lifecycle of machine learning models on Databricks. You need to ensure that each model follows the correct sequence from experimentation to production while also maintaining version control for models that are no longer actively used. Which of the following statements accurately compare the available model stages in MLflow, focusing on their respective use cases and restrictions? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "The 'Archived' stage automatically deletes models after a certain retention period, ensuring storage efficiency."},
      {"id": 1, "text": "The 'Staging' stage allows teams to perform thorough testing and A/B testing before promoting models to production."},
      {"id": 2, "text": "Only models in the 'Production' stage can be versioned and tracked within the MLflow platform."},
      {"id": 3, "text": "The 'Archived' stage is for models that are no longer in use but may need to be referenced for historical or audit purposes."},
      {"id": 4, "text": "The 'Production' stage prevents further testing and experimentation with a model, locking it into its final state."}
    ],
    "ans_ids":[1,3],
    "explanation":"The 'Staging' stage allows teams to perform thorough testing and A/B testing before promoting models to production.\n The 'Staging' stage in MLflow is specifically designed for pre-production activities such as rigorous testing, integration checks, and even A/B testing against the current production model. This stage acts as a buffer to ensure that only validated and high-performing models are moved to the 'Production' stage. \n\nThe 'Archived' stage is for models that are no longer in use but may need to be referenced for historical or audit purposes. The 'Archived' stage serves as a repository for inactive models. These models are no longer deployed but are retained within the MLflow Model Registry for record-keeping, compliance, or future reference if insights from them are needed. They are not automatically deleted."
  },
  {
    "question_id":"skillcertpro_e2_q33",
    "question": "You are designing a custom machine learning model object for deployment in Databricks, and you want to integrate preprocessing logic directly into the model. What is the primary benefit of including preprocessing steps inside the custom model class, rather than separating them into a standalone preprocessing pipeline?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "It allows for easier hyperparameter tuning by exposing preprocessing steps as tunable parameters."},
      {"id": 1, "text": "It increases computational efficiency during training by running preprocessing and model training in parallel."},
      {"id": 2, "text": "It reduces code complexity by eliminating the need for preprocessing transformations during inference, relying on the model s learned parameters instead."},
      {"id": 3, "text": "It ensures that preprocessing steps are consistently applied during both training and inference, preventing data leakage and misalignment issues."}
    ],
    "ans_id":3,
    "explanation":"It ensures that preprocessing steps are consistently applied during both training and inference, preventing data leakage and misalignment issues.\n\n Integrating preprocessing logic within the custom model class ensures that the exact same transformations applied during training are also applied during inference. This is crucial for preventing data leakage (where information from the test set inadvertently influences the training process) and for avoiding misalignment between the data format expected by the trained model and the format of the new data at inference time. By encapsulating both the model's learning and the data transformations within a single object, you maintain consistency throughout the model's lifecycle."
  },
  {
    "question_id":"skillcertpro_e2_q34",
    "question": "A logistics company uses a machine learning model to generate delivery route optimizations on a weekly basis. The resulting data is then stored for compliance and auditing purposes, but retrieval is rare. Which data storage solution would be the most appropriate for storing these optimization results, considering the infrequent access and long-term storage needs?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Cold storage solutions such as Google Cloud Archive or AWS Glacier"},
      {"id": 1, "text": "NoSQL database such as MongoDB or DynamoDB"},
      {"id": 2, "text": "Hot storage optimized for low-latency access, such as Azure Blob Hot or Google Cloud Nearline"},
      {"id": 3, "text": "Data lake storage using Delta Lake on Databricks"}
    ],
    "ans_id":0,
    "explanation":"Cold storage solutions such as Google Cloud Archive or AWS Glacier \n\nCold storage solutions are specifically designed for long-term data retention where access is infrequent. They offer the lowest storage costs but typically have higher retrieval latencies and costs. Given that the delivery route optimization results are stored primarily for compliance and auditing with rare retrieval, the cost savings of cold storage outweigh the slow access times, making it the most appropriate choice."
  },
  {
    "question_id":"skillcertpro_e2_q35",
    "question": "You are part of a marketing team that has developed a machine learning model to attribute conversions (e.g., purchases) to different marketing channels such as social media, email campaigns, and search engine ads. The company runs weekly marketing campaigns, and you need to use the attribution model to understand the effectiveness of each campaign and allocate future marketing budgets accordingly. Your team is discussing how to deploy the model for the company's marketing strategy. What deployment strategy would you recommend, and why?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Batch deployment is ideal because marketing attribution can be done on a weekly basis, and updates are not needed in real-time."},
      {"id": 1, "text": "Real-time deployment is necessary since marketing channels operate continuously and attribution must be updated with each new interaction."},
      {"id": 2, "text": "Stream processing is the best approach, as you need to adjust marketing strategies immediately based on live attribution data."},
      {"id": 3, "text": "Batch deployment is not suitable because marketing decisions should be made in real-time based on the latest customer interactions."}
    ],
    "ans_id":0,
    "explanation":"Batch deployment is ideal because marketing attribution can be done on a weekly basis, and updates are not needed in real-time.\n\nFor weekly marketing campaigns and budget allocation based on their overall effectiveness, batch deployment is a suitable and often more efficient strategy. The attribution model can be run once a week (or after each campaign cycle) on the accumulated data from the past week to attribute conversions to the different marketing channels. The results of this batch process can then be used to inform decisions for future weekly campaigns and budget allocations. Real-time or streaming updates are not critical since the decision-making cycle aligns with the weekly campaign cadence."
  },
  {
    "question_id":"skillcertpro_e2_q36",
    "question": "You are working for an e-commerce company that predicts daily sales based on various features (e.g., customer behavior, weather, promotions) using a pre-trained machine learning model. To ensure real-time performance and cost efficiency, your team has decided to implement a batch prediction pipeline that precomputes predictions daily and stores them in a centralized data warehouse. You need to query these precomputed batch predictions for live dashboards and reports accessed by different teams, such as marketing and inventory management. Which of the following is the primary benefit of querying precomputed batch predictions for live dashboards and reports in this scenario?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Minimized resource consumption for production servers"},
      {"id": 1, "text": "Real-time model retraining with live data"},
      {"id": 2, "text": "Lower latency for querying predictions"},
      {"id": 3, "text": "Higher accuracy due to real-time feedback"}
    ],
    "ans_id":2,
    "explanation":"Lower latency for querying predictions\n\nThe primary benefit of precomputing batch predictions daily and storing them in a data warehouse for live dashboards and reports is significantly lower query latency. When a dashboard or report needs to display the daily sales predictions, it can simply query the precomputed results from the data warehouse, which is a much faster operation than running the prediction model on the fly for each request. This ensures a responsive and efficient experience for the teams accessing the dashboards and reports."
  },
  {
    "question_id":"skillcertpro_e2_q37",
    "question": "You are working with a machine learning model for a retail company that predicts customer lifetime value (CLV). One of the numerical features, 'average monthly spending,' is highly skewed, with most customers spending relatively small amounts and a few high-spending outliers. You want to monitor this feature for drift after model deployment. Given the skewed distribution, how should you choose between using Jensen Shannon divergence and the Kolmogorov-Smirnov test?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use the Jensen-Shannon divergence to detect differences in mean values between the training and production distributions."},
      {"id": 1, "text": "Apply the Kolmogorov-Smirnov test because it is non-parametric and works well for comparing skewed distributions, focusing on differences in cumulative distribution functions."},
      {"id": 2, "text": "Use the Jensen-Shannon divergence because it measures the total difference in variance between the distributions and is better suited for continuous numerical data like 'average monthly spending.'"},
      {"id": 3, "text": "Use the Jensen-Shannon divergence because it can handle skewed distributions and will give more weight to differences in the distribution's tails."}
    ],
    "ans_id":1,
    "explanation":"Apply the Kolmogorov-Smirnov test because it is non-parametric and works well for comparing skewed distributions, focusing on differences in cumulative distribution functions.\n\nThe Kolmogorov-Smirnov (K-S) test is a non-parametric test that compares the cumulative distribution functions (CDFs) of two samples. It is sensitive to differences in the location, scale, and shape of the distributions, making it suitable for detecting drift in a numerical feature like 'average monthly spending,'especially when the distribution is skewed and might not follow a specific parametric form. The K-S test does not assume any particular distribution for the data."
  },
  {
    "question_id":"skillcertpro_e2_q38",
    "question": "Your machine learning team has recently implemented multiple webhooks to trigger different Databricks Jobs for model retraining and deployment. One of the webhooks is now obsolete, and you need to delete it to prevent unnecessary Job executions. The webhook you need to delete was registered to trigger a model retraining Job when new data is uploaded to a cloud storage bucket. What is the correct procedure to delete this webhook from the Databricks workspace?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Go to the Databricks model registry, find the associated model, and delete the webhook linked to the model's versioning."},
      {"id": 1, "text": "Call the Databricks REST API, using the 'webhooks/delete' endpoint, passing the webhook ID as a parameter to identify which webhook to delete."},
      {"id": 2, "text": "Edit the Databricks Job configuration and remove the trigger condition associated with the webhook, which will automatically disable the webhook."},
      {"id": 3, "text": "Navigate to the Databricks Jobs UI, locate the specific Job triggered by the webhook, and remove the webhook from the Job configuration settings."}
    ],
    "ans_id":1,
    "explanation":"Call the Databricks REST API, using the 'webhooks/delete' endpoint, passing the webhook ID as a parameter to identify which webhook to delete.\n\nThe Databricks REST API provides comprehensive control over various aspects of the Databricks workspace, including managing webhooks. To delete a specific webhook, you would typically use the appropriate API endpoint (likely under a webhooks resource) and provide the unique identifier (ID) of the webhook you wish to remove. This ensures that the correct webhook is targeted for deletion."
  },
  {
    "question_id":"skillcertpro_e2_q39",
    "question": "A healthcare company has developed a machine learning model to predict patient readmission risks. The model is hosted using Databricks Model Serving and is deployed in both the Production and Staging stages to ensure continuous improvements through testing. The Production stage handles predictions for real-time patient data, while the Staging stage is used for evaluating new model versions. The data science team needs to run a query on both the Production and Staging models to compare their performance on a set of real-time patient records. Which of the following describes the correct approach to query these models simultaneously for evaluation purposes?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Query both models in parallel using separate HTTP endpoints for the Production and Staging stages."},
      {"id": 1, "text": "Send real-time queries to the Production model and batch queries to the Staging model for comparison"},
      {"id": 2, "text": "Switch the Staging model to Production for a short period to test its performance, then switch back to the original model."},
      {"id": 3, "text": "Query the Production model first, then use the same predictions for the Staging model to avoid overloading resources."}
    ],
    "ans_id":0,
    "explanation":"Query both models in parallel using separate HTTP endpoints for the Production and Staging stages.\n\n Databricks Model Serving provides dedicated HTTP endpoints for each deployed model version, and these endpoints are independent of the model's stage (Production or Staging). To compare the performance of the models in both stages on the same set of real-time patient records, the correct approach is to:\n-Identify the HTTP endpoints: Obtain the serving endpoint URLs for the model deployed in the Production stage and the model deployed in the Staging stage.\n-Send parallel requests: For each real-time patient record (or a representative sample), send a prediction request to both the Production endpoint and the Staging endpoint concurrently.\n-Compare responses: Analyze the predictions returned by both models for each record to evaluate their performance and identify any differences.\n\nThis method allows for a direct, side-by-side comparison of the models' outputs on the same data under real-time serving conditions without disrupting the active Production service."
  },
  {
    "question_id":"skillcertpro_e2_q40",
    "question": "You are tasked with deploying a machine learning model built using a specific framework, and you want to ensure that the model is compatible across different platforms for serving, retraining, and experimentation. Which of the following statements correctly describe the role of MLflow flavors and how they can be used? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "You can use MLflow flavors to save models in a way that automatically optimizes them for cloud deployment on AWS, Azure, and GCP."},
      {"id": 1, "text": "MLflow flavors enable you to package a model in a standard format, allowing it to be loaded back and used with any framework regardless of the original framework used to train the model."},
      {"id": 2, "text": "Flavors define different APIs for different environments (e.g., Python, R, Java), ensuring that models logged using MLflow can be deployed in those respective environments without requiring additional conversion steps."},
      {"id": 3, "text": "MLflow flavors allow you to log preprocessing steps as part of the model package, which ensures that data transformations are always applied consistently, regardless of how the model is loaded or served."}
    ],
    "ans_ids":[2,3],
    "explanation":"Flavors define different APIs for different environments (e.g., Python, R, Java), ensuring that models logged using MLflow can be deployed in those respective environments without requiring additional conversion steps.\nMLflow flavors provide a standardized way to save and load models for various serving and inference environments. For example, the Python flavor (mlflow.pyfunc) allows loading and using models as generic Python functions, while other flavors might provide specific APIs for deployment in Java or other languages. This facilitates cross-platform compatibility.\n\n  MLflow flavors allow you to log preprocessing steps as part of the model package, which ensures that data transformations are always applied consistently, regardless of how the model is loaded or served. \nMany MLflow flavors support saving not just the model artifacts but also associated preprocessing steps (e.g., scalers, encoders) as part of the MLflow Model. When the model is loaded using the flavor-specific API, these preprocessing steps are also loaded and can be applied automatically during inference,ensuring consistency between training and deployment."
  }
]