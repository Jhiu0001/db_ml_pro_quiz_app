[
  {
    "question_id":"udemy_e2_q41",
    "question": "A Machine Learning Engineer has configured Lakehouse Monitoring to track performance drift for a recommendation model deployed in Databricks Model Serving. The inference table captures model predictions, features, and outcomes. Over time, the engineer observes a decline in precision, and Lakehouse Monitoring reports statistically significant changes in several categorical feature distributions. The engineer must design an alerting mechanism that immediately notifies stakeholders when drift metrics exceed defined thresholds and trigger retraining workflows automatically.\n\nWhich approach best meets these operational requirements?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Configure Lakehouse Monitoring alerts to publish to Databricks Alerts, integrate them with webhooks to trigger a Databricks Workflow for retraining, and notify teams through Slack or email."},
      {"id": 1, "text": "Export monitoring metrics to Unity Catalog and run manual SQL queries to identify drift exceeding thresholds."},
      {"id": 2, "text": "Periodically check drift metrics via notebooks and initiate retraining manually when changes are detected."},
      {"id": 3, "text": "Implement a Databricks SQL dashboard to visualize metric deviations and refresh daily to monitor drift trends."}
    ],
    "ans_id":0,
    "explanation":"Configure Lakehouse Monitoring alerts to publish to Databricks Alerts, integrate them with webhooks to trigger a Databricks Workflow for retraining, and notify teams through Slack or email.\n\nThis design enables a fully automated closed-loop MLOps workflow, which is the goal in production-grade ML systems. The pipeline works as follows:\n1.Lakehouse Monitoring continuously tracks drift (feature drift and performance drift).\n2.When a defined threshold is exceeded, an alert is raised.\n3.The alert triggers a webhook.\n4.The webhook automatically calls a Databricks Workflow.\n5.That workflow retrains the model using fresh data.\n6.The new model can be evaluated against the existing production model.\n7.If it performs better, it is promoted automatically using the MLflow Model Registry.\n\nThis ensures:\n*Immediate detection of drift.\n*Automatic remediation with retraining.\n*Zero human delay in triggering pipelines.\n*Full traceability of model lineage and promotion decisions."
  },
  {
    "question_id":"udemy_e2_q42",
    "question": "A retail company has implemented a recommendation model that must undergo validation before deployment to production. The validation process includes testing the feature engineering logic, verifying pipeline reproducibility, and evaluating inference latency within the serving endpoint. The ML team wants to ensure that these tests are environment-specific (dev, test, prod) while maintaining centralized control and auditability.\n\nWhich strategy best aligns with Databricks MLOps best practices?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Deploy all models in a single shared workspace to avoid environment duplication and perform manual validation tests."},
      {"id": 1, "text": "Manually test models in each environment using notebooks and track results in separate MLflow experiments."},
      {"id": 2, "text": "Create environment-specific Databricks Asset Bundles (DABs) with parameterized configurations for features, model registry paths, and serving endpoints, and execute validation workflows as part of a CI/CD pipeline."},
      {"id": 3, "text": "Use Unity Catalog to copy feature tables across environments and validate models via local Python scripts executed outside Databricks."}
    ],
    "ans_id":2,
    "explanation":"Create environment-specific Databricks Asset Bundles (DABs) with parameterized configurations for features, model registry paths, and serving endpoints, and execute validation workflows as part of a CI/CD pipeline.\n\nThis approach matches Databricks MLOps best practices because:\n*Each environment (dev / test / prod) has its own configuration but uses the same declarative bundle → ensures reproducibility.\n*Feature logic, model registry references, and serving endpoints are parameterized → eliminates environment drift.\n*Validation is automated, not manual, and executes as part of the promotion lifecycle.\n*Auditability comes from CI/CD logs and MLflow artifacts tied to each validation run.\n*This also enables rollback if validation fails before promotion."
  },
  {
    "question_id":"udemy_e2_q43",
    "question": "A Machine Learning Engineer has configured automated retraining for a demand forecasting model. The model’s accuracy is monitored through Lakehouse Monitoring using an inference table that stores predictions and actuals. After a few weeks, the monitoring dashboard shows no drift in features, but there is a gradual decline in prediction accuracy. The engineer suspects a data quality issue affecting the model inputs.\n\nWhat is the most effective next step to validate and mitigate the issue?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Define custom metrics in Lakehouse Monitoring to track input null ratios, missing feature counts, and data completeness, then correlate these metrics with model accuracy degradation."},
      {"id": 1, "text": "Rebuild the entire feature pipeline from scratch and retrain the model immediately."},
      {"id": 2, "text": "Disable Lakehouse Monitoring temporarily and enable Delta Live Tables expectations to detect missing features."},
      {"id": 3, "text": "Increase the data collection frequency to capture more samples and average out anomalies in the dataset."}
    ],
    "ans_id":0,
    "explanation":"Define custom metrics in Lakehouse Monitoring to track input null ratios, missing feature counts, and data completeness, then correlate these metrics with model accuracy degradation.\n\nThis validates (or rules out) a data-quality cause without dismantling the pipeline and gives you actionable alerts and thresholds you can wire into remediation (e.g., backfills, imputations, or source fixes). "
  },
  {
    "question_id":"udemy_e2_q44",
    "question": "A Machine Learning Engineer is managing multiple model versions in MLflow for a customer segmentation pipeline. The team has noticed inconsistencies between dev and prod environments where the wrong model version was deployed, leading to mismatched predictions. The engineer wants to enforce a consistent, traceable deployment process that ensures only validated models are promoted and deployed in production while preserving lineage and rollback capability.\n\nWhich approach should the engineer implement?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Store production-ready models as pickled artifacts in Unity Catalog and overwrite them upon every new retraining."},
      {"id": 1, "text": "Tag each model manually in the MLflow UI before deploying to production to indicate readiness."},
      {"id": 2, "text": "Use MLflow model version aliases (e.g., 'Staging', 'Production') for environment-specific deployment, managed through a CI/CD pipeline that promotes versions only after passing automated validation checks."},
      {"id": 3, "text": "Deploy new models directly from the dev workspace to Model Serving endpoints, ensuring consistent naming conventions between environments."}
    ],
    "ans_id":2,
    "explanation":"Use MLflow model version aliases (e.g., 'Staging', 'Production') for environment-specific deployment, managed through a CI/CD pipeline that promotes versions only after passing automated validation checks.\n\n Use MLflow model version aliases (such as 'Staging' and 'Production') and manage promotion through a CI/CD pipeline that updates the alias only after automated validation passes.\n\nThis solves the exact problem described:\n*Ensures the correct model version is always referenced in prod\n*Prevents accidental deployment of unvalidated models\n*Preserves lineage and promotion history\n*Allows instant rollback by reassigning the alias\n*Decouples 'what is deployed' from 'which version number it is'"
  },
  {
    "question_id":"udemy_e2_q45",
    "question": "A Machine Learning Engineer maintains an MLOps architecture where multiple Databricks workspaces share access to models and monitoring data under Unity Catalog governance. The engineering team wants to monitor endpoint health metrics such as latency, request throughput, error rate, and CPU utilization for all active serving endpoints. They also want to correlate these infrastructure metrics with model performance degradation detected by Lakehouse Monitoring.\n\nWhich solution provides the most comprehensive and scalable observability setup?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Integrate Lakehouse Monitoring with Databricks Metrics API and Cloud Monitoring tools (e.g., Azure Monitor or CloudWatch) to collect endpoint-level system metrics and correlate them with model drift signals."},
      {"id": 1, "text": "Use MLflow metrics logging to track system resource utilization during inference requests."},
      {"id": 2, "text": "Create a Databricks dashboard using SQL queries on the Feature Store to manually track model latency and drift."},
      {"id": 3, "text": "Configure a Databricks Job to periodically query serving endpoints and log results to a Delta table for manual visualization."}
    ],
    "ans_id":0,
    "explanation":"Integrate Lakehouse Monitoring with Databricks Metrics API and Cloud Monitoring tools (e.g., Azure Monitor or CloudWatch) to collect endpoint-level system metrics and correlate them with model drift signals.\n\nThis is the only solution that provides:\n*Model-level observability (via Lakehouse Monitoring — drift, precision, recall, calibration, etc.)\n*Infrastructure-level observability (via Metrics API + CloudWatch/Azure Monitor — latency, load, failures)\n*Cross-workspace governance under Unity Catalog\n*Scalability across multiple endpoints\n*Automated correlation between system degradation and model performance degradation"
  },
  {
    "question_id":"udemy_e2_q46",
    "question": "A Machine Learning Engineer at a global e-commerce platform manages a fraud detection model that must handle tens of thousands of real-time API requests per second. During deployment of a new model version, business stakeholders require that no downtime occurs and that the system can safely validate the new model’s performance before full rollout. The engineer must minimize customer impact and ensure a rollback mechanism is available in case the new version performs poorly.\n\nWhich deployment strategy best satisfies these requirements?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Implement a blue-green deployment where the new model version is deployed to a parallel endpoint, traffic is switched only after validation, and rollback is achieved by reverting to the previous endpoint."},
      {"id": 1, "text": "Use a canary deployment where 100% of traffic is directed to the new model immediately, relying on Lakehouse Monitoring for rollback decisions."},
      {"id": 2, "text": "Replace the production model directly in MLflow and restart the serving endpoint to automatically propagate updates to all clients."},
      {"id": 3, "text": "Deploy both model versions in the same endpoint and randomly distribute requests between them without validation monitoring."}
    ],
    "ans_id":0,
    "explanation":"Implement a blue-green deployment where the new model version is deployed to a parallel endpoint, traffic is switched only after validation, and rollback is achieved by reverting to the previous endpoint.\n\nA blue-green deployment is the correct approach in this scenario because it allows the new model version to be deployed to a parallel (green) endpoint while the current model (blue) continues serving all traffic. Once validation confirms that the new model behaves as expected under real production load, traffic can be safely switched over. If any degradation is detected, rollback is instantaneous by routing traffic back to the original (blue) version — with zero downtime.\n\nThis matches the requirements of:\n*no customer-facing interruption,\n*safe live validation before full rollout,\n*immediate rollback capability,\n*minimal business impact at high request throughput."
  },
  {
    "question_id":"udemy_e2_q47",
    "question": "A Data Scientist has developed a recommendation model that includes a custom feature transformation logic using TensorFlow and scikit-learn preprocessing steps. The model must be registered in Unity Catalog for centralized governance and deployed through Databricks Model Serving. However, the model’s inference logic is not compatible with standard MLflow flavors.\n\nWhat is the correct approach to deploy this model while preserving its custom preprocessing and inference behavior?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Convert the model into ONNX format and deploy it directly through Databricks Model Serving without MLflow registration."},
      {"id": 1, "text": "Store the TensorFlow and scikit-learn models separately in Delta tables and use Databricks Feature Store to join them during inference."},
      {"id": 2, "text": "Use Databricks Jobs to run inference notebooks that re-implement preprocessing steps manually each time predictions are requested."},
      {"id": 3, "text": "Wrap the model logic into a custom PyFunc model, log it as an MLflow model, and register it to Unity Catalog before deploying via Databricks Model Serving."}
    ],
    "ans_id":3,
    "explanation":"Wrap the model logic into a custom PyFunc model, log it as an MLflow model, and register it to Unity Catalog before deploying via Databricks Model Serving.\n\nThis is the correct solution because PyFunc allows you to:\n*encapsulate both scikit-learn and TensorFlow pipelines,\n*handle custom logic that standard MLflow flavors cannot,\n*preserve end-to-end inference consistency,\n*register under Unity Catalog for governance,\n*serve through Model Serving with full lineage and versioning."
  },
  {
    "question_id":"udemy_e2_q48",
    "question": "A Machine Learning Engineer is deploying a customer churn prediction model through Databricks Model Serving. The model will serve predictions to both a web application (real-time inference) and a nightly batch pipeline. The engineer must ensure scalability for real-time traffic while maintaining cost efficiency for low-traffic periods. The team also wants to minimize prediction latency while maintaining automated scaling control.\n\nWhich deployment configuration best meets these operational goals?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Configure the Model Serving endpoint with auto-scaling enabled to dynamically adjust compute resources based on request volume and set an appropriate min_workers value to handle baseline load."},
      {"id": 1, "text": "Deploy the model as a batch Databricks Job and expose its output via Delta tables consumed by both batch and online systems."},
      {"id": 2, "text": "Keep the endpoint at a fixed compute configuration with maximum concurrency to handle traffic spikes consistently."},
      {"id": 3, "text": "Use a notebook-based REST API implementation with manual scaling through the Databricks REST API."}
    ],
    "ans_id":0,
    "explanation":"Configure the Model Serving endpoint with auto-scaling enabled to dynamically adjust compute resources based on request volume and set an appropriate min_workers value to handle baseline load.\n\nThe best deployment approach is to enable auto-scaling on the Databricks Model Serving endpoint and configure an appropriate min_workers value so that:\n*there is always enough baseline capacity for low-latency real-time predictions,\n*the endpoint automatically scales up during traffic spikes,\n*and it scales down during idle/low-traffic periods to reduce cost.\nThis design allows a single serving endpoint to support both real-time inference (web app) and batch inference (nightly pipeline) without maintaining separate compute infrastructure.\n\nWhy this is the correct solution\nThis scenario has three operational constraints:\n1.Real-time inference must be low-latency → baseline workers should stay warm.\n2.Batch inference must be supported using the same model → no separate deployment path.\n3.Cost efficiency is required → autoscaling avoids paying for peak capacity during quiet periods.\nAuto-scaling is the native serving mechanism designed for elastic ML workloads in Databricks Model Serving and is the recommended approach for mixed traffic patterns."
  },
  {
    "question_id":"udemy_e2_q49",
    "question": "A Machine Learning Engineer is responsible for deploying multiple MLflow models as REST endpoints for various business units. Each model has different dependency versions (TensorFlow, scikit-learn, PyTorch), and manual dependency conflicts have previously caused serving failures. The team now wants a unified and reliable deployment mechanism that isolates environments, ensures dependency consistency, and provides version tracking within Databricks.\n\nWhich approach should the engineer implement?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Package each model using MLflow's conda.yaml specification when logging, allowing Databricks Model Serving to automatically create isolated environments for each model version."},
      {"id": 1, "text": "Deploy all models under a single Model Serving endpoint to reuse dependencies and reduce environment creation time."},
      {"id": 2, "text": "Install all framework versions manually on the serving cluster before model registration to avoid dependency mismatch errors."},
      {"id": 3, "text": "Build Docker images with all possible dependencies and deploy them using external Kubernetes services integrated with Databricks."}
    ],
    "ans_id":0,
    "explanation":"Package each model using MLflow's conda.yaml specification when logging, allowing Databricks Model Serving to automatically create isolated environments for each model version.\n\nThe engineer should package each model with its own conda.yaml (or environment.yaml) when logging it to MLflow, so Databricks Model Serving automatically creates isolated environments per model version, preventing dependency conflicts between TensorFlow, PyTorch, and scikit-learn models.\nThis ensures:\n*Each model has its own reproducible environment\n*No dependency collisions across business units\n*Version tracking remains native to MLflow + Unity Catalog\n*Serving failures due to conflicting packages are eliminated"
  },
  {
    "question_id":"udemy_e2_q50",
    "question": "A Machine Learning team at a financial services firm needs to deploy a high-value risk prediction model. Before routing full production traffic, they want to gradually introduce the new model alongside the existing one and compare prediction outcomes on a subset of live requests. The deployment must provide fine-grained control over the traffic percentage directed to each model version while enabling real-time performance monitoring to make an informed promotion decision.\n\nWhich deployment strategy and configuration should the team choose?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use a canary deployment on Databricks Model Serving, deploying both model versions as separate served entities, and incrementally shift traffic to the new model while monitoring accuracy and latency metrics."},
      {"id": 1, "text": "Replace the existing model in MLflow with the new version and observe performance metrics post-deployment."},
      {"id": 2, "text": "Deploy the new model as a blue-green deployment, switch all traffic to it immediately, and roll back only if failure occurs."},
      {"id": 3, "text": "Run the new model in a separate notebook job and manually compare results against the old model using SQL queries in Delta tables."}
    ],
    "ans_id":0,
    "explanation":"Use a canary deployment on Databricks Model Serving, deploying both model versions as separate served entities, and incrementally shift traffic to the new model while monitoring accuracy and latency metrics.\n\nThis approach is designed for:\n*Fine-grained traffic control (e.g., 1% → 10% → 25% → 50% → 100%)\n*Side-by-side comparison of old vs. new\n*Production realism without full exposure\n*Risk mitigation for high-value/financial models\n*Rollback safety by reducing the canary percentage to zero instantly\nThis provides the real-time behavioral validation required before full promotion."
  },
  {
    "question_id":"udemy_e2_q51",
    "question": "A Machine Learning Engineer deployed multiple models for different regions using Databricks Model Serving. Each endpoint must support authenticated access, log prediction requests for audit compliance, and allow integration with the company's web applications using Python SDKs instead of manually handling REST calls. Additionally, the solution must align with Databricks governance and MLflow tracking standards.Which configuration best fulfills these enterprise-grade deployment requirements?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Wrap each endpoint in a Flask API to log all incoming requests manually before forwarding them to Databricks Model Serving."},
      {"id": 1, "text": "Expose the model endpoints directly to the internet using unauthenticated public URLs for easier integration with external systems."},
      {"id": 2, "text": "Use the MLflow Deployments SDK to query model endpoints programmatically, enforce access control with Unity Catalog permissions, and enable request logging through Databricks Model Serving's built-in logging and audit features."},
      {"id": 3, "text": "Store prediction results in a Delta table through a notebook job scheduled after each inference call."}
    ],
    "ans_id":2,
    "explanation":"Use the MLflow Deployments SDK to query model endpoints programmatically, enforce access control with Unity Catalog permissions, and enable request logging through Databricks Model Serving's built-in logging and audit features.\n\nThis satisfies all enterprise-grade requirements:\n*Authenticated access → handled via UC + workspace identity + token-based auth\n*Audit logging → native to Databricks Model Serving\n*Python SDK integration → MLflow Deployments SDK (no manual REST handling)\n*MLflow alignment → models stay in UC-backed registry with tracking and lineage\nThis is the standard pattern for governed, production ML serving on Databricks."
  },
  {
    "question_id":"udemy_e2_q52",
    "question": "A machine learning engineer is setting up a webhook that should trigger a Databricks Job (job_id) whenever any version of the model model moves into any MLflow Model Registry stage.They start with the following partial configuration:\n\njob_json = {\n  'model_name': model,\n  'events': ['_____'],\n  'description': 'Job webhook trigger',\n  'status': 'Active',\n  'job_spec': {\n    'job_id': job_id,\n    'workspace_url': url,\n    'access_token': token\n    }\n  }\n\nresponse = http_request(\n  host_creds=host_creds,\n  endpoint=endpoint,\n  method='POST',\n  json=job_json\n  )\n\nWhich option correctly fills in the blank so the webhook fires whenever a model version transitions to any stage?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "MODEL_VERSION_CREATED"},
      {"id": 1, "text": "MODEL_VERSION_TRANSITIONED_TO_PRODUCTION"},
      {"id": 2, "text": "MODEL_VERSION_TRANSITIONED_TO_STAGING"},
      {"id": 3, "text": "MODEL_VERSION_TRANSITIONED_STAGE"},
      {"id": 4, "text": "MODEL_VERSION_TRANSITIONED_TO_STAGING,MODEL_VERSION_TRANSITIONED_TO_PRODUCTION"}
    ],
    "ans_id":3,
    "explanation":"MODEL_VERSION_TRANSITIONED_STAGE\n\nTo trigger a webhook for any stage transition (Staging, Production, Archived, etc.), MLflow provides a single wildcard-style event:\n'MODEL_VERSION_TRANSITIONED_STAGE'\nThis event fires whenever a model version transitions to any stage, regardless of which stage it moves into."
  }
]

