[
  {
    "question_id":"skillcertpro_e3_q31",
    "question": "You are part of a healthcare company that processes thousands of insurance claims daily. The company has built a machine learning model to identify potential fraud in the claims data based on historical patterns. The claims data is stored in a low-cost object storage system that supports large-scale batch processing but has slower access times. You have been tasked with deploying the model for fraud detection. Which of the following strategies would be most appropriate, given the performance of your data storage and the nature of the task?", 
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Real-time deployment is required since fraud detection needs to happen instantly to prevent further claims from being processed."},
      {"id": 1, "text": "Stream processing is required to capture fraud as it happens and alert the claims team in real-time."},
      {"id": 2, "text": "Batch deployment is ideal because claims processing can be done at regular intervals, and fraud detection doesn't need immediate action."},
      {"id": 3, "text": "Switching to high-performance storage is necessary to enable real-time fraud detection for immediate prevention of suspicious claims."}
    ],
    "ans_id":2,
    "explanation":"Batch deployment is ideal because claims processing can be done at regular intervals, and fraud detection doesn't need immediate action.\n\n Given that the claims data is processed daily (indicating a batch-oriented workflow) and stored in a low-cost object storage with slower access times, a batch deployment strategy aligns well with the existing infrastructure and the nature of the task. Fraud detection in insurance claims, while important, often doesn't require immediate, millisecond-level responses for every single claim as it arrives. Processing claims in batches (e.g., daily) allows the system to leverage the large-scale processing capabilities of the object storage and perform fraud detection on the aggregated data at regular intervals. The results can then be used to flag suspicious claims for further review."
  },
  {
    "question_id":"skillcertpro_e3_q32",
    "question": "You are working on a machine learning experiment where you need to maintain multiple versions of a dataset stored as a Delta table. You want to compare model performance across different dataset versions. Which of the following approaches correctly enables version control and retrieval of historical data from the Delta table for experimentation? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Enable Delta Time Travel by specifying a timestamp when writing to the Delta table."},
      {"id": 1, "text": "Enable Delta Time Travel by specifying a version number when reading from the Delta table."},
      {"id": 2, "text": "Overwrite the Delta table to ensure you have only the latest version for consistency."},
      {"id": 3, "text": "Use the RESTORE command to revert to a specific version of the Delta table."},
      {"id": 4, "text": "Use the VACUUM command to clean up older versions of the Delta table before experimentation."}
    ],
    "ans_ids":[1,3],
    "explanation":"Enable Delta Time Travel by specifying a version number when reading from the Delta table.\nDelta Lake's Time Travel feature allows you to query historical versions of a Delta table. By specifying a version number in your query, you can retrieve the exact state of the table at that point in time. This is crucial for comparing model performance across different dataset versions, as you can consistently use the same dataset version for each model evaluation.\n\nUse the RESTORE command to revert to a specific version of the Delta table.\nThe RESTORE command in Delta Lake allows you to revert the table to a previous state, identified by either a version number or a timestamp. While this is a more drastic action than simply querying a historical version, it's useful if you need to perform further operations or analysis on a specific historical state of the table. Like Time Travel for reading, RESTORE ensures you can work with a consistent dataset version."
  },
  {
    "question_id":"skillcertpro_e3_q33",
    "question": "You are tasked with building a real-time fraud detection system for an e-commerce platform. The system processes a continuous stream of payment transaction data from users in different geographic locations. This data arrives in a stream where some transactions can be delayed due to network issues, causing out-of-order events. However, the fraud detection model relies heavily on the sequence of transactions to identify unusual patterns such as rapid purchases from the same account. How would you handle the out-of-order data while ensuring that your real-time fraud detection model operates efficiently using Databricks Structured Streaming?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use processing-time windows to ensure all transactions are processed as soon as they arrive, ignoring event-time ordering."},
      {"id": 1, "text": "Use a fixed micro-batch interval of 5 minutes to minimize the impact of out-of-order events."},
      {"id": 2, "text": "Configure your Structured Streaming pipeline to automatically reorder out-of-order events by their event time without any additional configurations."},
      {"id": 3, "text": "Use event-time processing and set a watermark to discard late data beyond a certain threshold."}
    ],
    "ans_id":3,
    "explanation":"Use event-time processing and set a watermark to discard late data beyond a certain threshold.\n\n For real-time fraud detection where the sequence of transactions is crucial and data can arrive out of order,event-time processing with a watermark is the most effective approach in Databricks Structured Streaming.\n\n*Event-time processing: This ensures that the time used for windowing and ordering operations is based on the timestamp of the event itself (when the transaction occurred), rather than when it was processed by the streaming pipeline (processing time). This is essential for correctly capturing the sequence of events even if they arrive late.\n*Watermark: A watermark is a threshold that tells Structured Streaming how late events you expect to see Events arriving after the watermark for a given window are considered 'late' and can be either discarded or processed differently (depending on the configuration). Setting an appropriate watermark allows the system to balance latency (waiting for potentially late data) with completeness and efficiency (not holding onto potentially very late data indefinitely). This approach helps in maintaining the correct sequence for fraud detection while managing the complexities of out-of-order data in a real-time stream."
  },
  {
    "question_id":"skillcertpro_e3_q34",
    "question": "You are managing a machine learning project that involves multiple iterations of model development. Your team has developed several models, each with different hyperparameter settings, and now you need a system to track, version, and deploy models in a consistent manner. Youve decided to use Databricks Model Registry to handle this part of the lifecycle. What are the primary purposes of using a Model Registry in Databricks for model lifecycle management, and what are the typical user interactions involved? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Users can use the Model Registry to annotate models with key metadata, perform model reviews, and approve models for deployment."},
      {"id": 1, "text": "The Model Registry is used to track all training experiments and the metrics for each training run."},
      {"id": 2, "text": "The Model Registry ensures that only models with high accuracy are automatically moved to production."},
      {"id": 3, "text": "The Model Registry facilitates versioning, staging, and transitioning models across different deployment environments such as 'Staging' and 'Production'."},
      {"id": 4, "text": "The Model Registry can automatically optimize model hyperparameters during registration."}
    ],
    "ans_ids":[0,3],
    "explanation":"Users can use the Model Registry to annotate models with key metadata, perform model reviews, and approve models for deployment.\n The Databricks Model Registry provides a collaborative environment for managing the model lifecycle. Users can add descriptive metadata (tags, descriptions), review model versions, leave comments, and manage the approval status of models for different stages of deployment. This facilitates governance and ensures that models are properly vetted before being moved to production.\n\nThe Model Registry facilitates versioning, staging, and transitioning models across different deployment environments such as 'Staging' and 'Production'.\nA core functionality of the Model Registry is to provide versioning for registered models as new iterations are developed. It also allows you to assign stages to model versions (e.g., 'Staging', 'Production', 'Archived').This enables a structured workflow for transitioning models through different deployment environments,ensuring a consistent and controlled deployment process."
  },
  {
    "question_id":"skillcertpro_e3_q35",
    "question": "You are working on a machine learning model and want to track various artifacts such as SHAP plots, custom visualizations, feature data, and metadata using MLflow. You also want to ensure that these artifacts are accessible for later analysis and comparison across different runs. What is the most effective approach to log and view these artifacts using MLflow?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Log all artifacts using mlflow.log_metrics() and mlflow.log_params() to store both SHAP plots and metadata in a structured and comparable format."},
      {"id": 1, "text": "Log SHAP plots, custom visualizations, and feature data as artifacts using mlflow.log_artifact() while metadata is logged using mlflow.log_dict() for structured metadata storage."},
      {"id": 2, "text": "Use mlflow.log_artifacts() to log SHAP plots, feature data, and custom visualizations,and mlflow.log_params() to log metadata such as model configuration and hyperparameters."},
      {"id": 3, "text": "Utilize mlflow.log_artifact() for logging individual SHAP plots and images, and mlflow.log_metrics() to log custom visualizations as a form of metric data for easy comparison."}
    ],
    "ans_id":1,
    "explanation":"Log SHAP plots, custom visualizations, and feature data as artifacts using mlflow.log_artifact() while metadata is logged using mlflow.log_dict() for structured metadata storage.\n\nThis approach correctly utilizes MLflow's capabilities for logging different types of information.mlflow.log_artifact() is the appropriate function for logging files of any format, including image files (like SHAP plots and custom visualizations) and data files (like feature data, which could be CSV or other formats). These artifacts are stored in the MLflow run's artifact URI and can be easily viewed and downloaded later.mlflow.log_dict() is specifically designed for logging structured data (like dictionaries or JSON-serializable objects) as artifacts. This is ideal for storing metadata in a well-organized and readable format, making it easy to inspect and compare configurations or results across different runs."
  },
  {
    "question_id":"skillcertpro_e3_q36",
    "question": "You are tasked with tracking a machine learning experiment that trains multiple models using different hyperparameters. You decide to programmatically log parameters, metrics, and the model in an MLflow experiment using the Databricks environment. Which of the following is the correct way to track this experiment in Python?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "with mlflow.start_run():\nmlflow.log_param('alpha', 0.5)\nmlflow.log_metric('accuracy', 0.95)\nmlflow.log_artifact('model.pkl')"},
      {"id": 1, "text": "with mlflow.run():\nmlflow.log_params({'alpha': 0.5})\nmlflow.log_metrics({'accuracy': 0.95})\nmlflow.save_model('model.pkl', 'model')"},
      {"id": 2, "text": "with mlflow.start_run():\nmlflow.log_param('alpha', 0.5)\nmlflow.log_metric('accuracy', 0.95)\nmlflow.log_model('model.pkl', artifact_path='model')"},
      {"id": 3, "text": "mlflow.run():\nmlflow.log_param('alpha', 0.5)\nmlflow.log_metric('accuracy', 0.95)\nmlflow.save_model('model.pkl', 'model')"}
    ],
    "ans_id":2,
    "explanation":"with mlflow.start_run():\nmlflow.log_param('alpha', 0.5)\nmlflow.log_metric('accuracy', 0.95)\nmlflow.log_model('model.pkl', artifact_path='model')\n\n* 'with mlflow.start_run():' initiates a new MLflow run. All subsequent logging calls within this block will be associated with this specific run. This is essential for organizing and tracking individual model training attempts.\n* 'mlflow.log_param('alpha', 0.5)' correctly logs a single hyperparameter ('alpha' with a value of 0.5) for this run.\n* 'mlflow.log_metric('accuracy', 0.95)' correctly logs a single evaluation metric ('accuracy' with a value of 0.95) for this run.\n* 'mlflow.sklearn.log_model(model, artifact_path='model')' is the correct function (assuming you are using a scikit-learn model) to log the trained model as an artifact. The 'artifact_path' specifies a subdirectory within the run's artifact storage where the model will be saved. For other model types (e.g., TensorFlow, PyTorch), you would use the corresponding 'mlflow..log_model()' function."
  },
  {
    "question_id":"skillcertpro_e3_q37",
    "question": "You are running a series of machine learning experiments to optimize a regression model's hyperparameters using Hyperopt. You enable MLflows autologging feature to capture metrics and hyperparameters from each trial automatically. After running the experiment, you want to analyze the results and identify the best model based on the RMSE (Root Mean Squared Error) metric. However, you notice that some important hyperparameters and metrics are missing from your logged data. What is the most likely reason for missing hyperparameters and metrics, and how can you ensure that all necessary information is captured in future experiments?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The Hyperopt trials might not be correctly integrated with MLflows autologging. You should modify the objective function to explicitly log hyperparameters and metrics."},
      {"id": 1, "text": "The issue may arise from running asynchronous trials in Hyperopt. You should use synchronous trials and ensure that mlflow.end_run() is called after each trial to properly log all data."},
      {"id": 2, "text": "Hyperopt uses multiple workers, and some of the workers may not be logging their results. You should ensure autologging is enabled on each worker by calling mlflow.autolog() within the worker's function."},
      {"id": 3, "text": "You need to manually log custom metrics like RMSE using mlflow.log_metric() in addition to using autologging because autologging only logs default metrics."}
    ],
    "ans_id":2,
    "explanation":"Hyperopt uses multiple workers, and some of the workers may not be logging their results. You should ensure autologging is enabled on each worker by calling mlflow.autolog() within the worker's function.\n\n When using Hyperopt with multiple workers (which is common for parallelizing the search process), each worker executes trials independently. If MLflow autologging is only enabled in the main process, the child worker processes might not inherit this configuration and thus fail to log their hyperparameters and metrics.To ensure all trials are tracked, including those run by different workers, mlflow.autolog() should be called within the function that each worker executes. This ensures that each worker independently logs its results to MLflow."
  },
  {
    "question_id":"skillcertpro_e3_q38",
    "question": "You are a data scientist working for a manufacturing company that has implemented an IoT-based predictive maintenance system. Sensors installed on machines send performance data in real-time to your cloud-based system, and machine learning models predict potential failures. The volume of sensor data is low since only a few key variables are being monitored, but accurate, low-latency predictions are critical to prevent machine breakdowns. What is the best deployment strategy for your model to ensure the system remains both responsive and scalable?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Batch inference in the cloud, running predictions on aggregated data at the end of each day."},
      {"id": 1, "text": "Running inference on a distributed cluster and sending results every 10 minutes to the edge devices."},
      {"id": 2, "text": "Deploying the model on a centralized server with pre-allocated resources to handle all predictions in real time."},
      {"id": 3, "text": "Using real-time inference in the cloud with an auto-scaling setup that adjusts based on sensor activity."}
    ],
    "ans_id":3,
    "explanation":"Using real-time inference in the cloud with an auto-scaling setup that adjusts based on sensor activity.\n\nGiven the requirements for accurate, low-latency predictions in a system with real-time data streams, deploying the model for real-time inference in the cloud is the most suitable strategy. The low volume of data suggests that a large distributed cluster might be overkill. Auto-scaling is crucial for responsiveness and scalability. Even though the current volume is low, future expansion or temporary bursts in sensor activity should be handled without impacting latency. Cloud-based auto-scaling can dynamically adjust compute resources based on the incoming data stream, ensuring low latency under varying loads and cost-efficiency during periods of low activity."
  },
  {
    "question_id":"skillcertpro_e3_q39",
    "question": "Why are job clusters generally preferred over all-purpose clusters for running machine learning jobs in production environments?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Job clusters have unlimited auto-scaling, unlike all-purpose clusters."},
      {"id": 1, "text": "Job clusters automatically terminate after the job completes, saving costs."},
      {"id": 2, "text": "Job clusters provide better network security compared to all-purpose clusters."},
      {"id": 3, "text": "Job clusters allow interactive development, making them more flexible."}
    ],
    "ans_id":1,
    "explanation":"Job clusters automatically terminate after the job completes, saving costs.\n\nJob clusters in Databricks are designed for running non-interactive, automated workloads like production machine learning jobs. A key advantage is their ephemeral nature: they are automatically provisioned when a job starts and automatically terminated once the job finishes (or upon reaching a defined idle timeout). This on-demand lifecycle optimizes resource utilization and significantly reduces costs compared to keeping an all purpose cluster running continuously, regardless of actual workload."
  },
  {
    "question_id":"skillcertpro_e3_q40",
    "question": "You are tasked with deploying a fraud detection model that processes transaction data daily in a banking environment. The dataset includes hundreds of millions of transactions, and the model must score them in batch mode using Databricks score_batch operation. The data is stored in Delta Lake format, and the processing must be completed in a few hours to meet the organizations reporting requirements. Given this scenario, which of the following is the most important benefit of the score_batch operation in terms of maintaining efficient production-level operations?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Automatically scales to adjust the model based on new transaction patterns"},
      {"id": 1, "text": "Supports fault-tolerant execution with Delta Lakes ACID properties"},
      {"id": 2, "text": "Scores data directly from external API sources without additional ETL"},
      {"id": 3, "text": "Provides a low-latency response time for scoring individual transactions"}
    ],
    "ans_id":1,
    "explanation":"Supports fault-tolerant execution with Delta Lakes ACID properties\n\nIn a production environment dealing with hundreds of millions of transactions daily, reliability and data integrity are paramount. The score_batch operation in Databricks, when used with data stored in Delta Lake,benefits from Delta Lake's ACID (Atomicity, Consistency, Isolation, Durability) properties. This ensures that even if failures occur during the batch scoring process, the data remains consistent and the operation can be retried or recovered without data corruption or loss. This fault tolerance is crucial for maintaining efficient and stable production-level operations."
  }
]