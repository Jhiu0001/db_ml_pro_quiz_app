[
  {
    "question_id":"skillcertpro_e1_q41",
    "question": "You have a production model deployed in a Databricks environment that ingests real-time data to predict supply chain disruptions. What is the most robust approach to monitor the quality of the incoming data to ensure the model predictions remain reliable?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Set up a Databricks Delta Lake and configure schema enforcement to reject any records that do not match the training data structure."},
      {"id": 1, "text": "Continuously monitor the data for missing values and handle them with simple imputation techniques (e.g., mean or median imputation)."},
      {"id": 2, "text": "Use automated monitoring of summary statistics (mean, standard deviation, min/max values) for the input data features."},
      {"id": 3, "text": "Implement a feature monitoring system that compares the real-time features with their expected ranges, calculated from the training data distribution."}
    ],
    "ans_id":3,
    "explanation":"Implement a feature monitoring system that compares the real-time features with their expected ranges, calculated from the training data distribution. \nThis approach provides a robust way to monitor the quality of incoming data for feature drift and anomalies that could impact model reliability. By establishing expected ranges (or distributions) for each feature based on the training data, the monitoring system can detect when real-time data falls outside these ranges or deviates significantly. This can indicate data quality issues, sensor malfunctions, or genuine shifts in the underlying data distribution that might require model retraining or investigation."
  },
  {
    "question_id":"skillcertpro_e1_q42",
    "question": "You have a large dataset stored in Delta format and want to deploy your machine learning model in batch mode to process the data in parallel. After the predictions are computed, you want to save the results in a different Delta table for later use. Which of the following code snippets correctly implements this batch deployment approach while considering scalability and parallel processing?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "from pyspark.sql import functions as F\nfrom mlflow import pyfunc\nmodel = pyfunc.load_model(model_uri='models:/my-model/1')\ndf = spark.read.format('delta').load('/path/to/large_input_data')\ndf = df.repartition(200)\npredictions = model.predict(df)\npredictions.write.format('delta').mode('overwrite').save(\n'/path/to/output_data'\n)"},
      {"id": 1, "text": "from mlflow import pyfunc\nmodel = pyfunc.load_model(model_uri='models:/my-model/1')\ndf = spark.read.format('delta').load('/path/to/large_input_data')\npredictions = model.predict(df)\npredictions.write.format('delta').save('/path/to/output_data')"},
      {"id": 2, "text": "from mlflow import pyfunc\nmodel = pyfunc.load_model(model_uri='models:/my-model/1')\ndf = spark.read.format('delta').load('/path/to/large_input_data')\npredictions = model.predict(df)\npredictions.coalesce(1).write.format('delta').save('/path/to/output_data')"},
      {"id": 3, "text": " from pyspark.sql import functions as F\nfrom mlflow import pyfunc\nmodel = pyfunc.load_model(model_uri='models:/my-model/1')\ndf = spark.read.format('json').load('/path/to/large_input_data')\npredictions = model.transform(df)\npredictions.write.format('parquet').save('/path/to/output_data')"}
    ],
    "ans_id":0,
    "explanation":"from pyspark.sql import functions as F\nfrom mlflow import pyfunc\nmodel = pyfunc.load_model(model_uri='models:/my-model/1')\ndf = spark.read.format('delta').load('/path/to/large_input_data')\ndf = df.repartition(200)\npredictions = model.predict(df)\npredictions.write.format('delta').mode('overwrite').save(\n'/path/to/output_data'\n) \n\nThis code snippet correctly implements a scalable batch deployment approach for several reasons: \nLoading Model with MLflow: It uses 'mlflow.pyfunc.load_model' to load the MLflow model, which is the recommended way to deploy models managed by MLflow in a Databricks environment.\nReading Delta Data: It reads the input data efficiently using 'spark.read.format('delta').load(…)', leveraging Delta Lake's optimized data access.\nParallel Processing with Repartition: The 'df.repartition(200)' step is crucial for scalability. It explicitly repartitions the DataFrame into 200 partitions (you would adjust this number based on your cluster size anddata volume), ensuring that the 'model.predict(df)' operation can be executed in parallel across the workernodes of your Databricks cluster, thus leveraging Spark's distributed computing capabilities for large datasets.\nSaving Results as Delta: It saves the predictions back into a Delta table using'predictions.write.format('delta').save(…)', which provides the benefits of Delta Lake (ACID transactions,versioning, etc.) for the output data.\nOverwrite Mode: Using 'mode('overwrite')' ensures that each batch run replaces the previous output, which might be the desired behavior for a regular batch prediction job."
  },
  {
    "question_id":"skillcertpro_e1_q43",
    "question": "A retail company has a machine learning model that predicts customer churn. The model is trained and saved in Databricks. Every month, the company wants to apply this model to the latest customer interaction data stored in a Delta Lake table and save the predictions in another Delta Lake table for the marketing team. The company also wants to minimize resource usage and ensure that the model can handle increasing data volumes as the business grows. Which of the following batch deployment strategies should the company use?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use a scheduled Databricks Job to load the latest customer data, apply the model in a Spark DataFrame, and save the predictions as a Delta Lake table with partitioning."},
      {"id": 1, "text": "Manually load the model into the notebook, apply the predictions in Pandas, and save the results to an external database via JDBC."},
      {"id": 2, "text": "Deploy the model as a Databricks MLflow model serving endpoint, and use a batch query to send data for predictions at regular intervals."},
      {"id": 3, "text": "Use Databricks Repos to version the notebooks and manually run predictions every month, saving the results to a Parquet file."}
    ],
    "ans_id":0,
    "explanation":"Use a scheduled Databricks Job to load the latest customer data, apply the model in a Spark DataFrame, and save the predictions as a Delta Lake table with partitioning.\nThis strategy aligns perfectly with the requirements of monthly batch predictions, scalability, resource efficiency, and using Delta Lake.\n\n-Scheduled Databricks Job: Automating the process with a Databricks Job ensures the predictions are run regularly without manual intervention.\n-Loading Latest Data: Databricks Jobs can be configured to access the latest data in the Delta Lake table. \n-Applying Model in Spark DataFrame: Leveraging Spark DataFrames allows for distributed processing of the data, which is essential for handling large datasets and ensuring scalability as data volumes grow. Applying the model within Spark (often using MLflow's pyfunc.load_model and then a UDF or Spark ML's transform if it's a Spark ML model) enables parallel computation across the cluster.\n-Saving as Delta Lake with Partitioning: Saving the predictions as a Delta Lake table provides the benefits of Delta Lake (ACID transactions, versioning, etc.). Partitioning the output table based on relevant columns (e.g., month, customer segment) can further optimize query performance for the marketing team's downstream analysis.\n-Resource Efficiency: Databricks Jobs can be configured with appropriate cluster sizes that scale based on the workload, optimizing resource usage. Clusters can also be set to auto-terminate after the job completes."
  },
  {
    "question_id":"skillcertpro_e1_q44",
    "question": "You are managing a machine learning platform that uses the MLflow Model Registry in Databricks. You want to notify external services whenever a model transitions between lifecycle stages (e.g., from Staging to Production). For this purpose, you plan to use HTTP webhooks. Which of the following scenarios is the most appropriate use case for HTTP webhooks in Databricks Model Lifecycle Automation?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Using HTTP webhooks to automatically delete a model in the Archived stage after 30 days."},
      {"id": 1, "text": "Triggering a Databricks job to retrain the model automatically when a model's accuracy score drops below a threshold in Production."},
      {"id": 2, "text": "Periodically polling the MLflow Model Registry to retrieve the latest model version and update external systems."},
      {"id": 3, "text": "Notifying a monitoring system whenever a model in the Staging phase is promoted to Production, so the system can adjust its configurations accordingly."}
    ],
    "ans_id":3,
    "explanation":"Notifying a monitoring system whenever a model in the Staging phase is promoted to Production, so the system can adjust its configurations accordingly. \nThis is the most appropriate use case for HTTP webhooks in Databricks Model Lifecycle Automation.Webhooks are designed for real-time, event-driven communication. When a model transitions to the Production stage, external monitoring systems likely need to be informed immediately to adjust their monitoring parameters, traffic routing, or alerting rules for the newly promoted model. HTTP webhooks provide a mechanism for this immediate notification."
  },
  {
    "question_id":"skillcertpro_e1_q45",
    "question": "You are tasked with managing multiple machine learning models in a production environment using Databricks. Your team wants to keep track of model versions, monitor performance, and transition models between various stages (e.g., 'Staging,' 'Production'). What is the primary purpose of the MLflow Model Registry in this scenario?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The MLflow Model Registry primarily serves as a repository for trained models, but it does not support versioning or stage transitions."},
      {"id": 1, "text": "The MLflow Model Registry stores raw datasets and tracks data versioning."},
      {"id": 2, "text": "The MLflow Model Registry is used to track hyperparameters and tuning experiments, but it does not manage model versions or deployments."},
      {"id": 3, "text": "The MLflow Model Registry is designed to register, track, and manage model versions, while also supporting stage transitions such as 'Staging' and 'Production'."}
    ],
    "ans_id":3,
    "explanation":"The MLflow Model Registry is designed to register, track, and manage model versions, while also supporting stage transitions such as 'Staging' and 'Production.'\nThis statement accurately describes the primary purpose of the MLflow Model Registry. It provides a centralized hub for managing the lifecycle of machine learning models, including:\n\n-Registration: Storing trained models in a structured and organized manner. \n-Version Tracking: Automatically versioning models as they are updated or retrained, allowing for easy comparison and rollback.\n-Stage Transitions: Defining and managing the lifecycle stages of models (e.g., 'Staging', 'Production','Archived') to control their deployment and usage. \n-Metadata Storage: Storing relevant metadata about each model version, such as its creation time, associated runs, and descriptions."
  },
  {
    "question_id":"skillcertpro_e1_q46",
    "question": "Your organization has a machine learning pipeline in Databricks that processes customer data to predict churn. You want to automate model retraining whenever new data is ingested and ensure that only the best performing model is deployed to production. The pipeline should also handle versioning of models in the MLflow model registry. Which approach should you take to ensure model retraining and versioning are handled correctly?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Implement a Databricks workflow where data ingestion triggers a Databricks Job that retrains the model, logs it into MLflow, compares the performance with the current production model, and updates the model version if the new model performs better."},
      {"id": 1, "text": "Retrain the model daily using Databricks Jobs, but disable versioning in MLflow to avoid complexity, as you only need the best-performing model."},
      {"id": 2, "text": "Use Databricks Jobs to schedule the retraining pipeline every day, and set up a manual approval process for model deployment."},
      {"id": 3, "text": "Create a Databricks AutoML experiment that retrains the model daily and automatically promotes the new model to production if it achieves a better performance score."}
    ],
    "ans_id":0,
    "explanation":"Implement a Databricks workflow where data ingestion triggers a Databricks Job that retrains the model,logs it into MLflow, compares the performance with the current production model, and updates the model version if the new model performs better \nThis approach provides a fully automated and well-structured solution for continuous model retraining and deployment.\n\n-Data Ingestion Trigger: Automating retraining based on new data ingestion ensures the model stays up-to-date with the latest customer behavior, which is crucial for churn prediction. \n-Databricks Jobs: Databricks Jobs are designed for reliable and scheduled execution of tasks, making them suitable for the retraining pipeline. \n-MLflow Integration: Logging the retrained model into MLflow provides model tracking, reproducibility, and a central registry for managing model versions. Performance Comparison: Comparing the new model's performance against the current production model ensures that only better-performing models are considered for deployment. \n-Automated Versioning: MLflow's versioning capabilities allow you to track the lineage and performance of different model iterations, which is essential for auditing and rollback purposes. \n-Conditional Deployment: Automatically updating the model version in MLflow (e.g., transitioning it to the 'Production' stage) based on performance ensures that the best model is always serving predictions."
  },
  {
    "question_id":"skillcertpro_e1_q47",
    "question": "You are working with MLflow to track machine learning experiments in a large-scale production environment.You want to ensure that your experiment tracking includes hyperparameters, model metrics, and artifact versions across various runs and that the experiments are reproducible. Which of the following steps would best help you to track and manage experiments efficiently?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use mlflow.log_artifacts() to store the dataset, model files, and additional artifacts for each experiment,ensuring all relevant files are version-controlled."},
      {"id": 1, "text": "Automatically assign experiment IDs using mlflow.start_run() in a context manager, track parameters, and use the MLflow Model Registry to manage different versions of your models."},
      {"id": 2, "text": "Log all parameters and metrics using mlflow.log_params() and mlflow.log_metrics() within each experiment run and use a different experiment name for each run."},
      {"id": 3, "text": "Configure your MLflow tracking server to use a file-based backend store to allow experiment tracking on multiple machines, ensuring ease of scalability."}
    ],
    "ans_id":1,
    "explanation":"Automatically assign experiment IDs using mlflow.start_run() in a context manager, track parameters, and use the MLflow Model Registry to manage different versions of your models.\n This option outlines a comprehensive approach to efficient experiment tracking and management in a production environment using MLflow:\n-mlflow.start_run() in a context manager: This automatically handles the creation and termination of MLflow runs, ensuring proper tracking and preventing resource leaks. It also automatically assigns unique run IDs within an experiment.\n-Tracking Parameters: Logging hyperparameters using mlflow.log_param() is crucial for reproducibility, as it records the exact configuration used for each run. \n-Tracking Metrics: Logging model performance metrics using mlflow.log_metric() allows for comparison between different runs and versions. \n-MLflow Model Registry: The Model Registry is essential for managing different versions of your trained models, tracking their lifecycle stages (e.g., Staging, Production), and facilitating deployment. This ensures that you have a centralized and version-controlled system for your models."
  },
  {
    "question_id":"skillcertpro_e1_q48",
    "question": "You are a machine learning engineer for a retail company that sells products both online and in-store. The company has built a machine learning model to forecast sales demand based on historical sales data, seasonality, promotional events, and economic indicators. Your team is considering how to deploy this model to optimize stock levels and reduce wastage. You are tasked with determining whether batch deployment is appropriate for this use case. Which deployment strategy would you recommend, and why?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Batch deployment is ideal because sales forecasts are not needed in real-time, and forecasts can be updated at regular intervals."},
      {"id": 1, "text": "Hybrid deployment, which combines real-time and batch updates, is ideal because it balances real-time needs and overall operational efficiency."},
      {"id": 2, "text": "Online (real-time) deployment is the best approach since forecasts must be updated continuously as new sales data comes in."},
      {"id": 3, "text": "Deploy the model using stream processing so that each sales transaction immediately triggers an updated forecast for stock levels."}
    ],
    "ans_id":0,
    "explanation":"Batch deployment is ideal because sales forecasts are not needed in real-time, and forecasts can be updated at regular intervals.\nBatch deployment is well-suited for sales forecasting in this scenario because stock level adjustments and procurement decisions are typically made at regular intervals (e.g., daily, weekly). Real-time, per-transaction forecasts are generally not necessary. Updating forecasts in batches allows for efficient processing of historical data and the incorporation of various factors like seasonality and promotional events over a defined period. This approach is often more cost-effective and manageable for demand forecasting compared to continuous real-time updates."
  },
  {
    "question_id":"skillcertpro_e1_q49",
    "question": "You are monitoring a production model that predicts housing prices. One of the most important features in your model is 'square footage,' a numerical variable. You have noticed that over time, the model's performance has degraded, and you suspect drift in the distribution of the 'square footage' feature. Given that the 'square footage' feature follows a non-normal distribution, which drift detection method would be most appropriate to detect changes in this feature's distribution?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Apply the Kolmogorov-Smirnov test because it is a non-parametric test that compares two cumulative distributions and works well regardless of the feature distribution shape."},
      {"id": 1, "text": "Apply the Kolmogorov-Smirnov test only if the feature follows a normal distribution since it is designed for normally distributed data."},
      {"id": 2, "text": "Use the Jensen-Shannon divergence to directly compare the mean values of the numerical feature between training and production data."},
      {"id": 3, "text": "Use the Jensen-Shannon divergence because it compares probability distributions and is well-suited for detecting changes in non-normal numerical features."}
    ],
    "ans_id":0,
    "explanation":"Apply the Kolmogorov-Smirnov test because it is a non-parametric test that compares two cumulative distributions and works well regardless of the feature distribution shape. \n The Kolmogorov-Smirnov (KS) test is a non-parametric statistical test that compares the cumulative distribution functions (CDFs) of two samples. Because it does not assume any specific underlying distribution (like normality), it is well-suited for detecting drift in numerical features that follow a non-normal distribution, such as 'square footage.' The KS test can effectively identify if the distribution of the feature in the production data has shifted significantly compared to the training data."
  },
  {
    "question_id":"skillcertpro_e1_q50",
    "question": "You are deploying a machine learning model in a streaming environment where data is ingested in real-time from a sensor network. Each sensor generates only a small number of records per second, but the predictions need to be processed with minimal delay to trigger immediate actions in an industrial setting. You are evaluating the benefits of using real-time inference for this scenario to meet strict latency requirements. What is the primary advantage of using real-time inference for a small number of records in situations where fast prediction computations are critical?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Real-time inference reduces the need for maintaining an in-memory state for each record, thus improving scalability for small-scale prediction workloads."},
      {"id": 1, "text": "Real-time inference allows you to batch the small number of records and process them together, reducing the overhead of individual predictions."},
      {"id": 2, "text": "Real-time inference allows the use of complex, multi-step ensemble models in streaming environments,which would otherwise be too slow for batch processing."},
      {"id": 3, "text": "Real-time inference minimizes latency by processing each incoming record as soon as it arrives, enabling immediate actions based on the prediction."}
    ],
    "ans_id":3,
    "explanation":"Real-time inference minimizes latency by processing each incoming record as soon as it arrives, enabling immediate actions based on the prediction. \n\nThe primary advantage of real-time inference in scenarios with low per-second record volume but strict latency requirements is the ability to process each data point immediately upon arrival. This minimizes the delay between data ingestion and obtaining a prediction, which is crucial for triggering timely actions in industrial settings where even a short lag could have significant consequences."
  }
]