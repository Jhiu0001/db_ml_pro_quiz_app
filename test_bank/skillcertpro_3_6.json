[
  {
    "question_id":"skillcertpro_e3_q51",
    "question": "You are tasked with deploying a fraud detection model for a financial institution. The model runs in a batch process every hour and writes its fraud predictions to a Delta table. Analysts frequently query this Delta table for fraud trends across different types of financial transactions, such as credit card payments, wire transfers, and online purchases. The dataset grows rapidly due to the high volume of transactions, and query performance has started to degrade. To improve query performance, you decide to partition the Delta table. Which column should you partition the table by to improve query performance for analysts looking for fraud trends across different transaction types?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Transaction Amount"},
      {"id": 1, "text": "Customer ID"},
      {"id": 2, "text": "Transaction ID"},
      {"id": 3, "text": "Transaction Type"}
    ],
    "ans_id":3,
    "explanation":"Transaction Type\n\nPartitioning the Delta table by the 'Transaction Type' column is the most effective strategy for improving query performance in this scenario. Analysts frequently query the data to understand fraud trends across different transaction types (credit card payments, wire transfers, online purchases). By partitioning the table by this column, queries that filter on 'Transaction Type' will only need to scan the relevant partitions, significantly reducing the amount of data processed and speeding up query execution."
  },
  {
    "question_id":"skillcertpro_e3_q52",
    "question": "Your company develops models for fraud detection. You need to automate a workflow that is triggered whenever a model moves from staging to production in the Databricks Model Registry. The job should run a series of validation checks before finally deploying the model to a production environment. Which of the following steps should be included in setting up this automated job in Databricks to trigger when the model moves between stages? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Create a Manual Notification System to Approve Job Execution"},
      {"id": 1, "text": "Write a Script that Polls the Model Registry Every Hour"},
      {"id": 2, "text": "Configure a Webhook for Staging to Production Transition"},
      {"id": 3, "text": "Integrate the Webhook with a Databricks Job Trigger"},
      {"id": 4, "text": "Disable Automation to Allow for Manual Inspection Before Job Execution"}
    ],
    "ans_ids":[2,3],
    "explanation":"Configure a Webhook for Staging to Production Transition\nThe Databricks Model Registry supports the creation of webhooks that can be triggered by specific events, such as a model version transitioning from one stage to another (e.g., Staging to Production). Configuring a webhook for this specific transition event is a direct and efficient way to automatically initiate a downstream process whenever a model is promoted.\n\nIntegrate the Webhook with a Databricks Job Trigger\n Once a webhook is configured for the 'Staging to Production' transition, it needs to be integrated with a Databricks Job. When the webhook is triggered by the model stage change, it will send an HTTP request to a specific endpoint. This endpoint should be configured to initiate the Databricks Job that runs the validation checks and deployment steps."
  },
  {
    "question_id":"skillcertpro_e3_q53",
    "question": "You are performing batch inference on a large dataset stored in a Delta table. The table contains customer data, and the model needs to predict churn. The data is partitioned by the customer_region column to improve query performance, and you want to ensure that the batch inference process takes full advantage of this partitioning.Given that you are using the customer_region column as a partition key, which of the following approaches will best utilize the partitioning to speed up batch processing?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "data = spark.read.format('delta').load('/delta/customer_data')\nfiltered_data = data.filter(data.customer_region == 'North America')\npredictions = model.predict(filtered_data)"},
      {"id": 1, "text": "data = spark.read.format('delta').load('/delta/customer_data')\nfiltered_data = data.filter(\ndata.customer_region == 'North America'\n).repartition(5)\npredictions = model.predict(filtered_data)"},
      {"id": 2, "text": "data = spark.read.format('delta').load('/delta/customer_data')\nfiltered_data = data.where(\ndata.customer_region == 'North America'\n).coalesce(1)\npredictions = model.predict(filtered_data)"},
      {"id": 3, "text": "data = (\nspark.read.format('delta')\n.option('partitionColumn', 'customer_region')\n.load('/delta/customer_data')\n)\nfiltered_data = data.filter(data.customer_region == 'North America')\npredictions = model.predict(filtered_data)"}
    ],
    "ans_id":0,
    "explanation":"data = spark.read.format('delta').load('/delta/customer_data')\nfiltered_data = data.filter(data.customer_region == 'North America')\npredictions = model.predict(filtered_data) \n\nWhen a Delta table is partitioned by 'customer_region', Spark's query optimizer automatically leverages this partitioning when you apply a filter on that column. In this option, after loading the Delta table, the '.filter(data.customer_region == 'North America')' operation will push down the filter to the data source layer. This means Spark will only read the partitions corresponding to 'North America' from the storage, significantly reducing the I/O and speeding up the data loading and processing for that specific region. The subsequent 'model.predict(filtered_data)' will then operate only on this reduced dataset."
  },
  {
    "question_id":"skillcertpro_e3_q54",
    "question": "You are deploying a machine learning model for predictive maintenance in a manufacturing environment. The system must monitor data from sensors in real-time and trigger alerts if equipment is likely to fail soon. The business logic involves not only checking model predictions but also assessing environmental conditions, equipment usage patterns, and maintenance schedules. What is the best way to integrate this business logic into a real-time streaming deployment?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Train the model with environmental conditions, equipment usage patterns, and maintenance schedules so it directly predicts the need for maintenance."},
      {"id": 1, "text": "Deploy separate models for each aspect (environmental conditions, equipment usage, etc.) and combine their predictions in a post-processing step."},
      {"id": 2, "text": "Embed the business logic as custom transformations within the streaming pipeline, using real-time sensor data to update the models predictions."},
      {"id": 3, "text": "Use a rules-based system to handle the complex business logic outside of the streaming deployment."}
    ],
    "ans_id":2,
    "explanation":"Embed the business logic as custom transformations within the streaming pipeline, using real-time sensor data to update the models predictions.This approach allows for a tightly integrated and dynamic real-time decision-making process. By embedding the business logic (considering environmental conditions, usage patterns, schedules) as custom transformations within the streaming pipeline, you can augment the model's raw predictions with these contextual factors in real-time. This enables the system to trigger alerts based on a more holistic assessment of the situation, leading to more accurate and actionable predictive maintenance insights. The pipeline processes the continuous stream of sensor data, applies the transformations (incorporating the business logic), and then uses the model to generate predictions, all within the low-latency requirements of a real-time system."
  },
  {
    "question_id":"skillcertpro_e3_q55",
    "question": "You are managing the deployment of a recommendation engine for a video streaming service. Your team uses a machine learning model that predicts the next best recommendation for users based on their watch history, search queries, and viewing preferences. To optimize system performance, you precompute the recommendations every hour using a batch job. These recommendations are stored and queried by the live system to display personalized video recommendations. What is a key advantage of using precomputed batch predictions for this recommendation engine in terms of system performance?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Ability to provide highly dynamic and interactive experience"},
      {"id": 1, "text": "Improves model interpretability through feature importance analysis"},
      {"id": 2, "text": "Reduces the need for real-time inference, decreasing latency"},
      {"id": 3, "text": "Prevents stale data by continuously updating the model"}
    ],
    "ans_id":2,
    "explanation":"Reduces the need for real-time inference, decreasing latency\n\nThe primary advantage of using precomputed batch predictions for a recommendation engine in terms of system performance is the significant reduction in latency during the serving of recommendations. When a user requests recommendations, the system can simply look up the precomputed results from storage (e.g., a database or cache) instead of running the complex machine learning model in real-time. This lookup operation is much faster than real-time inference, leading to lower latency and a more responsive user experience."
  },
  {
    "question_id":"skillcertpro_e3_q56",
    "question": "You want to automate the lifecycle of a machine learning model by triggering a batch inference job in Databricks each time a model version is promoted to 'Production.' Which of the following steps correctly implements this automation using Model Registry Webhooks and Databricks Jobs?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use a Databricks Job to monitor the Model Registry at fixed intervals, detect when a model is in 'Production,' and trigger the inference pipeline."},
      {"id": 1, "text": "Manually transition the model to 'Production' and then manually start the Databricks Job for batch inference every time a model update is made."},
      {"id": 2, "text": "Set up a Model Registry Webhook with an event type MODEL_VERSION_REGISTERED and configure the Databricks Job to run every time a model is registered."},
      {"id": 3, "text": "Create a Model Registry Webhook with an event trigger MODEL_VERSION_TRANSITIONED_STAGE and filter it for the 'Production' stage. Set the Databricks Job to trigger when this event occurs to start the batch inference job."}
    ],
    "ans_id":3,
    "explanation":"Create a Model Registry Webhook with an event trigger MODEL_VERSION_TRANSITIONED_STAGE and filter it for the 'Production' stage. Set the Databricks Job to trigger when this event occurs to start the batch inference job.\n\n This is the correct approach to automate a job based on a model transitioning to the 'Production' stage in the MLflow Model Registry.Model Registry Webhook with MODEL_VERSION_TRANSITIONED_STAGE: This event type specifically triggers when a model version's stage is changed.\n*Filter for 'Production' stage: By filtering for the 'Production' stage, the webhook will only be activated when a model version reaches this specific deployment stage. \n*Trigger Databricks Job: The webhook needs to be configured to send a notification (e.g., an HTTP request) to a service that can then trigger the Databricks Job responsible for running the batch inference pipeline. Databricks provides mechanisms to trigger jobs via webhooks or the Jobs API."
  },
  {
    "question_id":"skillcertpro_e3_q57",
    "question": "You have deployed a machine learning model that generates daily predictions for customer behavior across millions of records. These predictions are stored in a large Delta table and queried frequently to generate reports. You notice that querying the table is taking longer than expected. Which of the following optimizations would help reduce the time required to read the prediction results?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use caching to store the entire table in memory"},
      {"id": 1, "text": "Use Z-ordering based on the most frequently queried columns"},
      {"id": 2, "text": "Partition the table by a non-discriminative column"},
      {"id": 3, "text": "Repartition the table into fewer, larger files"}
    ],
    "ans_id":1,
    "explanation":"Use Z-ordering based on the most frequently queried columns\n\nZ-ordering is a data locality optimization technique in Delta Lake that aims to colocated related data on disk.By Z-ordering the Delta table based on the columns that are most frequently used in the queries (e.g.,customer ID, prediction date, or any other columns commonly used in filters), you can significantly improve read performance. When queries filter on these Z-ordered columns, the query engine needs to read fewer blocks of data from storage because the relevant information is clustered together. This reduces disk I/O and speeds up query execution."
  },
  {
    "question_id":"skillcertpro_e3_q58",
    "question": "Your team has built multiple versions of a fraud detection model in Databricks, and you need to manage them through their lifecycle from development to production. The goal is to ensure smooth transitions between stages, control who can update the models, and allow other teams to access the best-performing model for inference. You've decided to leverage the Model Registry in Databricks to streamline these processes. Which of the following actions best describes how you can manage your models using Databricks Model Registry to ensure seamless lifecycle management?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use the Model Registry to store and track different versions of the model and automatically promote the best version to production based on performance metrics."},
      {"id": 1, "text": "Use the Model Registry to assign models to various lifecycle stages (e.g., Staging, Production) and manually promote or transition models between stages as needed."},
      {"id": 2, "text": "The Model Registry in Databricks is primarily designed to handle metadata for training runs, and it does not support features like versioning or promotion of models to different stages."},
      {"id": 3, "text": "Automatically archive older versions of the model in the Model Registry as new versions are added,without requiring manual intervention."}
    ],
    "ans_id":1,
    "explanation":"Use the Model Registry to assign models to various lifecycle stages (e.g., Staging, Production) and manually promote or transition models between stages as needed.\n\nThe Databricks Model Registry provides a structured way to manage the lifecycle of machine learning models. It allows you to register different versions of a model and assign them to various stages, such as 'Staging', 'Production', and 'Archived.' The transition of models between these stages can be done manually through the UI or programmatically via the API, providing control over the deployment process. This ensures a smooth transition as models move from development to production and allows for human oversight and approval at critical points."
  },
  {
    "question_id":"skillcertpro_e3_q59",
    "question": "You are a Machine Learning Engineer at a healthcare organization that utilizes machine learning models to predict patient readmissions. You have a productionized model that monitors patient health data over time. Recently, you observed that the models accuracy is decreasing, and you suspect data drift. The data includes structured features like patient age, length of stay, and diagnosis codes. You must identify the most effective way to detect and manage data drift, ensuring that the model remains accurate. Which of the following approaches would be most effective for detecting data drift and maintaining model performance in production?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Monitor feature distributions and compare them with historical data using statistical tests like the Kolmogorov-Smirnov (KS) test."},
      {"id": 1, "text": "Automatically update model hyperparameters whenever drift is detected."},
      {"id": 2, "text": "Retrain the model periodically on new data regardless of whether data drift has occurred."},
      {"id": 3, "text": "Use cross-validation on new data to evaluate model performance and identify drift."}
    ],
    "ans_id":0,
    "explanation":"Monitor feature distributions and compare them with historical data using statistical tests like the Kolmogorov-Smirnov (KS) test.\n\nMonitoring feature distributions is a direct and effective way to detect data drift. By comparing the distributions of input features (age, length of stay, diagnosis codes) in the current production data with their distributions in the historical training data (or previous production data), you can identify statistically significant shifts. The Kolmogorov-Smirnov (KS) test is a suitable statistical test for comparing the cumulative distribution functions of two samples and can detect various types of distributional changes in numerical features. For categorical features (like diagnosis codes), Chi-Square tests or other appropriate methods can be used. Detecting drift early allows for timely intervention to maintain model performance."
  },
  {
    "question_id":"skillcertpro_e3_q60",
    "question": "You are managing a Databricks pipeline that automates various stages of the machine learning lifecycle, such as training, model evaluation, and deployment. You are considering whether to use a job cluster or an all-purpose cluster to execute these tasks efficiently. Which of the following statements correctly identify the advantages of using job clusters over all-purpose clusters for automating tasks in the ML lifecycle on Databricks? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Job clusters are automatically terminated after the job completes, reducing resource costs."},
      {"id": 1, "text": "Job clusters allow for interactive development and collaboration across multiple teams in real-time."},
      {"id": 2, "text": "Job clusters have less isolation compared to all-purpose clusters, increasing the likelihood of resource conflicts."},
      {"id": 3, "text": "Job clusters are created specifically for the duration of a job, ensuring dedicated resources without contention from other workloads."},
      {"id": 4, "text": "Job clusters automatically handle model versioning and deployment without user intervention."}
    ],
    "ans_ids":[0,3],
    "explanation":"Job clusters are automatically terminated after the job completes, reducing resource costs.\nA significant advantage of job clusters is their ephemeral nature. They are provisioned when a job starts and automatically terminated upon completion (or after a defined idle timeout). This ensures that compute resources are only used when actively needed for the automated tasks, leading to substantial cost savings compared to running an all-purpose cluster continuously.\n\nJob clusters are created specifically for the duration of a job, ensuring dedicated resources without contention from other workloads.\nJob clusters are designed to provide dedicated resources for the execution of a specific job. This isolation from other potentially running workloads on the same Databricks workspace helps ensure consistent performance and avoids resource contention that might occur on a shared, all-purpose cluster. This dedicated environment is beneficial for the reliability and reproducibility of automated ML pipeline tasks."
  }
]