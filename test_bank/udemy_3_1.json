[
  {
    "question_id":"udemy_e3_q1",
    "question": "A machine learning engineer is reviewing the label distribution of a deployed classification model and suspects that the proportion of each class in the incoming data has shifted compared to past data.\n\nWhich statistical technique can the engineer apply to verify this suspicion?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Chi-squared goodness-of-fit test (one-way)"},
      {"id": 1, "text": "Jensen-Shannon divergence"},
      {"id": 2, "text": "Chi-squared test for independence (two-way)"},
      {"id": 3, "text": "Kolmogorov-Smirnov (KS) test"}
    ],
    "ans_id":0,
    "explanation":"The one-way Chi-squared goodness-of-fit test is the appropriate statistical test when comparing:\n\n*Observed class frequencies in new incoming data vs.\n*Expected class frequencies derived from historical dataThis test determines whether the observed label distribution differs significantly from the expected distribution. If the p-value is below a chosen threshold (e.g., 0.05), the engineer can conclude that label distribution drift has occurred.\n\nThis exactly matches the scenario where the engineer wants to confirm if class proportions have shifted over time."
  },
  {
    "question_id":"udemy_e3_q2",
    "question": "A machine learning engineer observes that a production model is experiencing concept drift.\n\nWhat is the likely consequence of this type of drift?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The model will respond faster"},
      {"id": 1, "text": "The model's predictive quality will improve"},
      {"id": 2, "text": "The model will respond slower"},
      {"id": 3, "text": "The model's predictive quality will degrade"}
    ],
    "ans_id":3,
    "explanation":"The model's predictive quality will degrade\n\n Concept drift occurs when the relationship between the input features and the target variable changes over time.This means the model's learned patterns no longer match the current reality, causing predictions to become less accurate.As a result, the model's performance declines, often requiring retraining with more recent data."
  },
  {
    "question_id":"udemy_e3_q3",
    "question": "Label drift refers to a shift in which component of a machine learning system?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The mapping between features and the target outcome"},
      {"id": 1, "text": "The distribution of the model's predicted labels"},
      {"id": 2, "text": "The underlying distribution of the actual target variable"},
      {"id": 3, "text": "The distribution of one or more input features"}
    ],
    "ans_id":2,
    "explanation":"The underlying distribution of the actual target variable\n\n Label drift (also called target drift) occurs when the true labels in the real-world data change over time, even if the model or features remain the same.This means the distribution of the actual target variable shifts — for example:\n\n*Fraud definition changes over time\n*Customer behavior changes\n*Disease diagnosis criteria change\n\nWhen labels drift, the model is effectively predicting the wrong or outdated target distribution, leading to degraded performance."
  },
  {
    "question_id":"udemy_e3_q4",
    "question": "A machine learning engineer discovers drift in a live ML system and concludes that a newly trained model needs to replace the existing one.\n\nWhich condition must be satisfied before rolling out the updated model?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "All of these conditions must be satisfied before deploying the replacement model"},
      {"id": 1, "text": "The updated model must outperform the current one on a randomly selected portion of the dataset"},
      {"id": 2, "text": "The updated model must outperform the current one across the entire available dataset"},
      {"id": 3, "text": "The updated model must show better performance than the current one on the most recent data samples"}
    ],
    "ans_id":3,
    "explanation":"The updated model must show better performance than the current one on the most recent data samples\n\nWhen drift is detected, the primary concern is that recent real-world data has changed.Therefore, before replacing the production model, the updated model must demonstrate improved performance specifically on the most recent (post-drift) data, not on older or randomly selected data.This ensures that:\n\n*The new model is adapted to the current data distribution\n*The updated model addresses the cause of drift\n*Deployment decisions are aligned with real-world performance needs\n\nThis is the core requirement in MLOps for drift-driven model refresh."
  },
  {
    "question_id":"udemy_e3_q5",
    "question": "A machine learning engineer is setting up a system to monitor feature drift. Their planned workflow includes:\n\n1.Capture the distribution of each feature in the training dataset\n2.Deploy the trained model to production\n3.Capture the distribution of each feature from live inference data\n4.?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Retrain the model to incorporate any newly introduced features"},
      {"id": 1, "text": "Compare actual feature values with the model's predicted outputs"},
      {"id": 2, "text": "Evaluate the model's prediction latency"},
      {"id": 3, "text": "Apply a statistical test to check whether feature distributions have shifted over time"}
    ],
    "ans_id":3,
    "explanation":"Apply a statistical test to check whether feature distributions have shifted over time. After collecting:\n\n1.Training feature distributions, and\n2.Live (production) feature distributions,\n\nthe final step is to compare these two distributions to detect feature drift.This is done using statistical drift tests such as:\n*Kolmogorov-Smirnov test (for continuous features)\n*Chi-square test (for categorical features)\n*Jensen-Shannon divergence (distribution similarity measure)\n\nThese tests determine whether the production feature values have deviated significantly from the training baseline.This aligns with standard ML monitoring practice:baseline → live data → statistical comparison → drift detection"
  },
  {
    "question_id":"udemy_e3_q6",
    "question": "A machine learning engineer must choose an appropriate deployment approach for a new ML system. The system requires centrally executed predictions with extremely low response times, but the volume of predictions is very small at any given moment.\n\nWhich deployment approach best meets these constraints?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Streaming inference"},
      {"id": 1, "text": "Real-time inference"},
      {"id": 2, "text": "Batch processing"},
      {"id": 3, "text": "On-device/edge deployment"}
    ],
    "ans_id":1,
    "explanation":"Real-time inference\n\n The system needs:\n*Very low response time (extremely fast predictions)\n*Low prediction volume (few requests at any given time)\n*Centrally executed predictions (not on-device)\n\nThese requirements align perfectly with real-time (online) inference, where the model is hosted on a server and invoked via an API, providing millisecond-level latency even for small workloads.Real-time inference is designed for:\n*Per-request prediction\n*Immediate results\n*Low-latency serving\n*Irregular or low-volume traffic\n\nThis is the standard approach when performance matters more than throughput."
  },
  {
    "question_id":"udemy_e3_q7",
    "question": "Which statement best characterizes batch deployment in machine learning?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Predictions are generated instantly as soon as features are received"},
      {"id": 1, "text": "None of these options accurately represent batch deployment"},
      {"id": 2, "text": "Predictions are precomputed in advance and saved for future lookup"},
      {"id": 3, "text": "Predictions are produced immediately upon data arrival and then stored for later use"}
    ],
    "ans_id":2,
    "explanation":"Predictions are precomputed in advance and saved for future lookup\n\nBatch deployment refers to generating predictions periodically (e.g., hourly, daily) for a large set of inputs and storing those predictions so they can be served quickly later.This is ideal when:\n*Low latency is required at serving time\n*The input data does not change frequently\n*Predictions can be computed ahead of time"
  },
  {
    "question_id":"udemy_e3_q8",
    "question": "Which technology helps support real-time deployment by bundling an application together with all of its required tools and libraries?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Structured Streaming"},
      {"id": 1, "text": "Docker"},
      {"id": 2, "text": "Click"},
      {"id": 3, "text": "Flask"}
    ],
    "ans_id":1,
    "explanation":"Docker\n\nDocker packages an application together with all of its dependencies—libraries, runtime, configuration—into a portable container.This makes it ideal for real-time ML deployment, because:\n*The environment is consistent across dev, test, and production\n*Startup is fast\n*The service runs reliably on any infrastructure\n*It integrates well with real-time serving frameworks and APIs\n\nContainerization is the standard foundation for deploying low-latency ML services."
  },
  {
    "question_id":"udemy_e3_q9",
    "question": "A machine learning engineer has trained a scikit-learn random forest model, logged it with MLflow, and now wants to run the model in parallel during inference.\n\nWhich operation can be used to generate a function suitable for parallel deployment of the registered scikit-learn model?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Deploying a scikit-learn model in parallel is not supported"},
      {"id": 1, "text": "mlflow.spark.pandas_udf"},
      {"id": 2, "text": "mlflow.sklearn.spark.udf"},
      {"id": 3, "text": "mlflow.pyfunc.spark.udf"}
    ],
    "ans_id":3,
    "explanation":"mlflow.pyfunc.spark.udf\n\nFor scikit-learn models logged with MLflow, the standard method to enable parallel inference using Spark is to convert the logged model into a Spark UDF using:\nmlflow.pyfunc.spark_udf()\nThis creates a PySpark user-defined function that can be applied across Spark DataFrames, allowing parallel, distributed inference—even for scikit-learn models.This is the officially supported and Databricks-recommended approach for running non-Spark MLflow models (such as sklearn, xgboost, keras, pytorch) in parallel using Spark."
  },
  {
    "question_id":"udemy_e3_q10",
    "question": "A machine learning engineer is building a fraud-detection system. Whenever a credit-card transaction occurs, the system must instantly analyze the data and output a prediction to decide whether the transaction should be approved.\n\nWhich deployment approach fits these requirements?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "On-device/edge deployment"},
      {"id": 1, "text": "Real-time inference"},
      {"id": 2, "text": "Batch processing"},
      {"id": 3, "text": "Streaming inference"}
    ],
    "ans_id":1,
    "explanation":"Fraud detection for credit-card transactions requires:\n*Instant decision-making\n*Very low latency\n*Per-request prediction\n*Centralized model execution (common in financial systems)\n\nThis exactly matches real-time (online) inference, where each transaction triggers an immediate prediction through an API-based model endpoint."
  }
]