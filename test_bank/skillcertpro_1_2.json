[
    {
    "question_id":"skillcertpro_e1_q11",
    "question": "You have deployed a machine learning model that predicts customer churn based on several input features such as user engagement, service complaints, and account age. Over time, you notice that the models performance on predicting churn has decreased. After investigating, you find that the distribution of your target variable (churn rate) has changed, but the input features' distributions have remained mostly the same. Which of the following explains what type of drift is occurring in this scenario, and what should be your next step?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "Feature drift is occurring, and you should retrain the model using recent data with updated feature distributions."},
        {"id": 1, "text": "Feature drift is occurring, and you should increase the regularization of your model to make it more robust to changes in the data."},
        {"id": 2, "text": "Label drift is occurring, and you should monitor the feature distributions to detect any changes that may affect the models performance in the future."},
        {"id": 3, "text": "Label drift is occurring, and you should adjust your model to account for the new target distribution,possibly by rebalancing the dataset or retraining the model."}
    ],
    "ans_id":3,
    "explanation":"Label drift is occurring, and you should adjust your model to account for the new target distribution,possibly by rebalancing the dataset or retraining the model.\n\nLabel Drift: The scenario describes a change in the distribution of the target variable (churn rate) while the input feature distributions remain relatively stable. This is the definition of label drift (also known as concept drift in the outcome variable). The relationship between the features and the target has changed.\n\n Next Step: When label drift occurs, the model trained on the old target distribution is no longer optimal. You need to adapt the model to the new reality.\nThis often involves:\n\nRebalancing the dataset: If the class imbalance in the target variable has changed, you might need to adjust the class weights or resample the data during training.\n\nRetraining the model: The most common approach is to retrain the model using more recent data that reflects the new target distribution. This allows the model to learn the updated relationship between the features and the churn outcome." 
    },
    {
    "question_id":"skillcertpro_e1_q12",
    "question": "You are tasked with loading a registered machine learning model from the Databricks Model Registry for batch inference on a large dataset. The model is registered under the name 'CustomerChurnModel', and you need to ensure that the correct model version is used for inference. You want to load the model with the Databricks load_model API for batch processing. Which of the following steps would be the correct approach to load and apply the registered model for batch inference?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "from mlflow.pyfunc import load_model \n model = load_model('CustomerChurnModel') \n predictions = model.predict(batch_data)"},
        {"id": 1, "text": "from mlflow.models import load_model \n model = load_model('models:/CustomerChurnModel/1') \n predictions = model.predict(batch_data)"},
        {"id": 2, "text": "from mlflow.pyfunc import load_model \n model = load_model('models:/CustomerChurnModel/Production') \n predictions = model.batch_predict(batch_data)"},
        {"id": 3, "text": "from mlflow import load_model \n model = load_model('models:/CustomerChurnModel/latest') \n predictions = model.predict(batch_data)"}
    ],
    "ans_id":1,
    "explanation":"from mlflow.models import load_model \nmodel = load_model('models:/CustomerChurnModel/1') \npredictions = model.predict(batch_data) \n\nThis is a correct approach to load a specific version (version '1' in this case) of a registered MLflow model for batch inference.\n\n'mlflow.models.load_model': This function is used to load MLflow models, including those from the Model Registry, as generic Python functions ('pyfunc' models). \n\nModel URI: The URI 'models:/CustomerChurnModel/1' correctly specifies the location of the registered model in the Model Registry, including the model name and the specific version number.\n\n'predict()' method: For 'pyfunc' models loaded with 'load_model', the standard method for making predictions on a batch of data (typically a Pandas DataFrame or a Spark DataFrame) is 'predict()'" 
    },
    {
    "question_id":"skillcertpro_e1_q13",
    "question": "You are responsible for deploying a machine learning model that predicts stock prices based on real-time market data from various financial exchanges. The model must generate predictions continuously, as stock prices fluctuate rapidly throughout the trading day. These predictions are stored in a time-series format so that traders can access both real-time and historical predictions for decision-making. You need to implement a streaming solution in Databricks that continuously processes incoming market data and writes predictions to a time-based store for querying. Which solution best fulfills the need for continuous predictions and efficient time-based storage in Databricks?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "Use Structured Streaming to process market data and store predictions in a Delta Lake table partitioned by both time and stock symbol."},
        {"id": 1, "text": "Use Structured Streaming to process market data in real-time and store predictions in a Delta Lake table partitioned by stock symbol."},
        {"id": 2, "text": "Run an Apache Spark job that stores each prediction individually in a NoSQL database for real-time access."},
        {"id": 3, "text": "Schedule a batch job every 30 seconds to process market data and store predictions in a Delta Lake table."}
        ],
    "ans_id":0,
    "explanation":"Use Structured Streaming to process market data and store predictions in a Delta Lake table partitioned by both time and stock symbol.\nThis solution best addresses the requirements for continuous predictions and efficient time-based storage:\n\nStructured Streaming: It is designed for processing real-time, continuous data streams with low latency, which is crucial for predicting rapidly changing stock prices.\n\nDelta Lake: Delta Lake provides ACID transactions, scalable metadata handling, and efficient querying on large datasets, making it suitable for storing time-series data.\n\nPartitioning by Time and Stock Symbol: Partitioning the Delta Lake table by both time (e.g., date, hour) and stock symbol significantly optimizes query performance when traders need to access historical predictions for specific stocks or predictions within a certain time range. This is essential for time-series analysis." 
    },
    {
    "question_id":"skillcertpro_e1_q14",
    "question": "You are conducting a series of machine learning experiments and saving intermediate results to a Delta table. You need to ensure that you can revert to a previous version of the Delta table in case you need to analyze or re-run past experiments. Which of the following is the correct approach to revert to a specific version of a Delta table?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "Use the TIME TRAVEL feature by specifying the versionAsOf or timestampAsOf parameter in the SELECT statement."},
        {"id": 1, "text": "Perform a DELETE operation on the table and use VACUUM to clean up the current version, effectively reverting the table."},
        {"id": 2, "text": "Use the RESTORE command to revert to a specific version by providing the version number."},
        {"id": 3, "text": "Use the MERGE INTO command with a historical Delta table version to overwrite the current version."}
        ],
    "ans_id":0,
    "explanation":"Use the TIME TRAVEL feature by specifying the versionAsOf or timestampAsOf parameter in the SELECT statement. \nThis is the correct approach to access a previous version of a Delta table for analysis or re-running past experiments without modifying the current state of the table. Time Travel allows you to query historical snapshots of the table as they existed at a specific version or timestamp. This is ideal for analysis and reproducibility without disrupting ongoing operations" 
    },
    {
    "question_id":"skillcertpro_e1_q15",
    "question": "You are tasked with deploying a machine learning model that predicts product demand for a retail company. The data for these predictions is updated weekly, and the models predictions are used to make inventory decisions. Immediate feedback on demand is not necessary since inventory is only adjusted once per week. To optimize for cost and efficiency, you are considering whether batch deployment or real-time deployment is more appropriate. Which of the following best explains why batch deployment is the most suitable option for this use case? (Select two)",
    "type":"MS",
    "choices": [
        {"id": 0, "text": "Batch deployment is preferred because predictions are required weekly, not continuously, which aligns with the models data update frequency."},
        {"id": 1, "text": "Batch deployment allows for better resource optimization, but it cant be used for large datasets due to memory constraints."},
        {"id": 2, "text": "Batch deployment is suitable because it ensures that predictions are available in real-time as inventory changes"},
        {"id": 3, "text": "Batch deployment reduces computational cost since predictions can be run in bulk once per week, aligned with the inventory update schedule"},
        {"id": 4, "text": "Batch deployment should be avoided because it does not allow for immediate prediction generation as inventory levels fluctuate"}
        ],
    "ans_ids":[0,3],
    "explanation":"text"
    },    
    {
    "question_id":"skillcertpro_e1_q16",
    "question": "You work for a streaming video service that provides personalized recommendations to users based on their watch history, viewing habits, and demographic data. The recommendation engine is powered by a machine learning model that receives real-time updates as user behavior changes. Over time, you've noticed that the model's recommendations seem less relevant to users, leading to a drop in user engagement. You want to set up a drift detection workflow for this streaming data environment to identify both concept drift and feature drift. Which workflow would be the most appropriate for detecting and handling both types of drift in this streaming environment?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "Continuously monitor data for changes in feature distributions using a sliding window approach, and retrain the model if the variance between windows exceeds a threshold."},
        {"id": 1, "text": "Monitor only the input features for drift, as changes in feature distribution are the primary cause of performance degradation in streaming data models."},
        {"id": 2, "text": "Apply a fixed schedule to retrain the model at regular intervals, based on the assumption that drift occurs periodically."},
        {"id": 3, "text": "Monitor both feature distributions and model prediction accuracy in real-time, applying statistical tests like Kolmogorov-Smirnov (KS) tests for feature drift and error rate monitoring for concept drift. Retrain the model when a drift is detected."}
        ],
    "ans_id":3,
    "explanation":"Monitor both feature distributions and model prediction accuracy in real-time, applying statistical tests like Kolmogorov-Smirnov (KS) tests for feature drift and error rate monitoring for concept drift. Retrain the model when a drift is detected.\nThis workflow provides the most comprehensive and proactive approach to detecting and handling both feature drift and concept drift in a real-time streaming environment.\n\nReal-time Monitoring: Continuous monitoring is essential for detecting drift as soon as it occurs in a dynamic streaming environment.\n\nFeature Drift Detection: Using statistical tests like the Kolmogorov-Smirnov (KS) test (for numerical features) or other appropriate tests (like Chi-squared for categorical features or Jensen-Shannon Divergence) allows you to quantify the difference in feature distributions between a baseline (e.g., training data or a recent window) and the current streaming data.\n\nConcept Drift Detection: Monitoring model prediction accuracy (e.g., by comparing predictions to actual user engagement metrics like clicks or watch time, if feedback is available in near real-time) directly assesses whether the relationship between input features and the target variable is changing. A drop in accuracy signals concept drift.\n\nTriggered Retraining: Retraining the model when a significant drift is detected (either in features or concept) allows the model to adapt to the new data patterns and maintain performance. This event-driven retraining is more effective than fixed schedules."
    },
    {
    "question_id":"skillcertpro_e1_q17",
    "question": "Your organization is using MLflow to manage models across different teams. As part of the model lifecycle,you want to automate the transition of a model from the Staging to the Production stage, but only after the model has been thoroughly tested and approved. What is the most effective way to implement this process in MLflow?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "Implement stage transition policies in MLflow that allow models to move from Staging to Production only if specific performance criteria are met. Stage transition policies ensure that models are only promoted when they meet the necessary requirements, preventing models that havent been properly vetted from being deployed to production. This workflow can also be integrated with CI/CD pipelines for more robust, automated model management."},
        {"id": 1, "text": "Create a custom script to monitor model performance and automatically move any model in the Staging stage to Production if it has the highest accuracy."},
        {"id": 2, "text": "Use the MLflow UI to automatically transition all models from Staging to Production after 24 hours."},
        {"id": 3, "text": "Manually move the models from Staging to Production based on team feedback, and use the MLflow UI to track performance history."}
        ],
    "ans_id":0,
    "explanation":" Implement stage transition policies in MLflow that allow models to move from Staging to Production only if specific performance criteria are met. Stage transition policies ensure that models are only promoted when they meet the necessary requirements,preventing models that havent been properly vetted from being deployed to production. This workflow can also be integrated with CI/CD pipelines for more robust, automated model management.\n MLflow's stage transition policies (or governance rules) provide a mechanism to automate the promotion of models based on predefined criteria. These policies can check for things like minimum performance metrics,successful testing results (perhaps logged as tags or artifacts), or required approvals before allowing a model to move to the 'Production' stage. Integrating this with a CI/CD pipeline makes the process fully automated and ensures that only validated models are deployed"
    },
    {
    "question_id":"skillcertpro_e1_q18",
    "question": "In a Databricks machine learning workflow, you are tasked with deploying a batch inference pipeline for a trained model. The model is designed to predict customer churn based on historical data, and predictions are required weekly. Which of the following is the best approach to ensure the batch inference pipeline scales efficiently and meets the operational requirement?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "Register the model in the Databricks Model Registry, use MLflow to serve it, and rely on Databricks AutoML to generate batch predictions."},
        {"id": 1, "text": "Use a standard Databricks notebook to load the trained model and apply it to a batch of data stored in Databricks Delta Lake, scheduling the notebook to run weekly."},
        {"id": 2, "text": "Convert the model into a REST API service and make real-time predictions using Databricks Jobs triggered by a scheduled batch process."},
        {"id": 3, "text": "Use Databricks Jobs to automate a batch inference workflow, utilizing the trained model stored in MLflow and reading input data directly from Databricks Delta Lake."}
        ],
    "ans_id":3,
    "explanation":"Use Databricks Jobs to automate a batch inference workflow, utilizing the trained model stored in MLflow and reading input data directly from Databricks Delta Lake.This approach leverages the strengths of Databricks for scalable and reliable batch processing.\n\nDatabricks Jobs: Jobs are designed for running non-interactive workloads like batch inference in a scheduled and automated manner. They provide robust execution, monitoring, and error handling.\n\nMLflow Model Registry: Storing the trained model in the Model Registry provides versioning, stage management, and a centralized location for accessing the model for deployment. Databricks Jobs can easily load models from the registry.\n\nDatabricks Delta Lake: Reading input data directly from Delta Lake ensures efficient and reliable data access, benefiting from Delta Lake's performance optimizations and data reliability features.\n\nScalability: Spark, which underlies Databricks Jobs, can efficiently process large datasets in parallel, ensuring the batch inference pipeline scales well with increasing data volumes" 
    },
    {
    "question_id":"skillcertpro_e1_q19",
    "question": "You are developing a machine learning pipeline that uses models developed in different frameworks such as scikit-learn, TensorFlow, and PyTorch. Your goal is to ensure seamless deployment of these models within a unified platform. You are considering using MLflows PyFunc flavor for the model packaging and serving process. Which of the following statements best describe the advantages of using the PyFunc flavor in MLflow when managing and deploying models in a diverse machine learning pipeline? (Select two)",
    "type":"MS",
    "choices": [
        {"id": 0, "text": "PyFunc only supports scikit-learn models and is limited when dealing with models from deep learning frameworks."},
        {"id": 1, "text": "PyFunc allows models built with different frameworks to be used with a common API."},
        {"id": 2, "text": "PyFunc restricts the use of custom preprocessing steps and requires models to follow a predefined data preprocessing schema."},
        {"id": 3, "text": "Models saved as PyFunc can be deployed in any Python environment without requiring access to the original training environment."},
        {"id": 4, "text": "Using PyFunc makes model interpretability across frameworks easy without additional configuration or transformation."}
        ],
    "ans_ids":[1,4],
    "explanation":" PyFunc allows models built with different frameworks to be used with a common API. The primary advantage of MLflow's PyFunc flavor is that it provides a standardized interface (predict method) for interacting with models regardless of the underlying framework used to train them (scikit-learn, TensorFlow, PyTorch, etc.). This abstraction simplifies the deployment and serving process, as the same code can be used to load and score models from different sources. \n\n Models saved as PyFunc can be deployed in any Python environment without requiring access to the original training environment. PyFunc models encapsulate the model's prediction logic and any necessary dependencies (within reasonable limits). This makes them highly portable and deployable in various Python environments, as long as the required libraries are installed. It reduces the dependency on the exact training environment, promoting easier deployment."
    },
    {
    "question_id":"skillcertpro_e1_q20",
    "question": "You need to perform batch inference using a model that has been registered in the Databricks Model Registry. You want to load the model from the 'Staging' stage and run inference on a distributed DataFrame in a Databricks environment. Which of the following steps is the correct method to achieve this?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "from mlflow.pyfunc import load_model\nmodel = load_model('models:/ChurnModel/latest')\npredictions = batch_df.withColumn('predictions', model.predict(batch_df))"},
        {"id": 1, "text": "from mlflow.pyfunc import load_model\nmodel = load_model('models:/ChurnModel/Production')\npredictions = model.apply(batch_df)"},
        {"id": 2, "text": "from mlflow.models import load_model\nmodel = load_model('models:/ChurnModel/Staging')\npredictions = model.transform(batch_df)"},
        {"id": 3, "text": "from mlflow.pyfunc import load_model\nmodel = load_model('models:/ChurnModel/Staging')\npredictions = batch_df.withColumn('predictions', model.predict(batch_df))"}
        ],
    "ans_id":3,
    "explanation":"from mlflow.pyfunc import load_model\nmodel = load_model('models:/ChurnModel/Staging')\npredictions = batch_df.withColumn('predictions', model.predict(batch_df)) \n This is the correct method to load a model from the 'Staging' stage of the MLflow Model Registry and apply it for batch inference on a Spark DataFrame.\n\n'from mlflow.pyfunc import load_model': This imports the correct function for loading MLflow models as generic Python functions ('pyfunc' models), which is suitable for applying to Spark DataFrames using UDFs.\n\n'load_model('models:/ChurnModel/Staging')': This correctly specifies the URI to load the model named 'ChurnModel' from the 'Staging' stage of the Model Registry.\n\n'batch_df.withColumn('predictions', model.predict(batch_df))': This is the standard way to apply a 'pyfunc' model to a Spark DataFrame for batch inference. You typically wrap the 'model.predict()' call within a Spark User Defined Function (UDF) to apply it to each row or partition of the DataFrame in a distributed manner."
    }
]