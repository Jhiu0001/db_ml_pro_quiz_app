[
  {
    "question_id":"skillcertpro_e1_q31",
    "question": "You are developing a machine learning pipeline in Databricks and want to track various stages of the workflow using nested runs in MLflow. These stages include data preprocessing, model training, and hyperparameter tuning. You want to ensure that each stage is tracked within the context of the overall pipeline. What is the correct approach to track these nested runs in MLflow?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use mlflow.set_experiment() at the start of each stage to track them as nested runs within the same experiment."},
      {"id": 1, "text": "Use with mlflow.start_run(nested=True): to create nested runs within an existing MLflow run for each stage."},
      {"id": 2, "text": "Start a new MLflow run for each stage independently using mlflow.start_run() without closing the previous one."},
      {"id": 3, "text": "Avoid using nested runs and track each stage as a separate experiment using mlflow.create_experiment()."}
    ],
    "ans_id":1,
    "explanation":"Use with mlflow.start_run(nested=True): to create nested runs within an existing MLflow run for each stage. This is the correct approach for tracking nested runs in MLflow. By using the nested=True parameter within a with mlflow.start_run(): block, you can create child runs that are logically grouped under a parent run. This allows you to structure your MLflow tracking to represent the different stages of your pipeline (data preprocessing, model training, hyperparameter tuning) as distinct, yet related, runs within the overarching pipeline execution."
  },
  {
    "question_id":"skillcertpro_e1_q32",
    "question": "You have trained a machine learning model on a large dataset and are preparing to deploy it in a batch environment. The model will be used to make predictions on a new set of data at regular intervals. You are evaluating the benefits of using Databricks' score_batch operation in this context. Which of the following statements describe the practical benefits of using the score_batch operation for batch inference in Databricks? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "The score_batch operation is optimized for offline, scheduled jobs, which allows predictions to be run periodically without affecting real-time systems."},
      {"id": 1, "text": "The score_batch operation cannot scale beyond a single machine and is only suitable for small datasets."},
      {"id": 2, "text": "The score_batch operation can handle large datasets efficiently by distributing the workload across multiple nodes in a cluster."},
      {"id": 3, "text": "The score_batch operation is used primarily for real-time prediction, where inference is needed immediately after data is ingested."},
      {"id": 4, "text": "The score_batch operation only supports structured data and cannot process semi-structured or unstructured data."}
    ],
    "ans_ids":[0,2],
    "explanation":"The score_batch operation is optimized for offline, scheduled jobs, which allows predictions to be run periodically without affecting real-time systems. The score_batch operation in Databricks is specifically designed for batch inference scenarios, where predictions are made on a set of data at once, typically in an offline or scheduled manner. This makes it ideal for periodic prediction tasks without impacting the performance or latency of real-time applications. \n\n The score_batch operation can handle large datasets efficiently by distributing the workload across multiple nodes in a cluster. Databricks' score_batch operation leverages the distributed computing capabilities of Spark. It can efficiently process large datasets by parallelizing the inference workload across the nodes of a Databricks cluster. This scalability is a significant advantage for batch prediction tasks involving substantial amounts of data"
  },
  {
    "question_id":"skillcertpro_e1_q33",
    "question": "You have a workflow that automatically transitions a model version from 'None' to 'Staging' based on performance metrics. You also want to notify a third-party service via a webhook whenever a new model version is created. Which of the following code blocks ensures the webhook is triggered when a new model version is created?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "client.transition_model_version_stage(\nname='my-model',\nversion=2,\nstage='Staging',\narchive_existing_versions=True\n)"},
      {"id": 1, "text": "client.get_model_version(\nname='my-model',\nversion=2\n)"},
      {"id": 2, "text": "client.create_webhook(\nevent='MODEL_VERSION_CREATED',\nmodel_name='my-model',\nurl='https://webhook-endpoint.com'\n)"},
      {"id": 3, "text": "client.create_webhook(\nevent='MODEL_VERSION_TRANSITIONED_STAGE',\nmodel_name='my-model',\nstage='Staging',\nurl='https://webhook-endpoint.com'\n)"}
    ],
    "ans_id":2,
    "explanation":"client.create_webhook(\nevent='MODEL_VERSION_CREATED',\nmodel_name='my-model',\nurl='https://webhook-endpoint.com'\n) \n\nThis code block directly uses the create_webhook function with the event parameter set to 'MODEL_VERSION_CREATED'. This configuration ensures that the specified webhook URL will be triggered every time a new version of the model named 'my model' is created in the MLflow Model Registry."
  },
  {
    "question_id":"skillcertpro_e1_q34",
    "question": "An e-commerce company has deployed a machine learning model that predicts user behavior for future purchases. The model runs batch predictions on millions of users, and the results are stored in a Delta Lake table. The data science team runs frequent queries on this table to identify users with high purchase probabilities, often filtering by user ID and purchase probability score. As the table size grows, query performance is slowing down. The team decides to use Z-ordering to optimize query performance. Which of the following steps would best optimize the query performance for filtering by both user ID and purchase probability score?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Z-order the Delta Lake table by date, without changing the current partitioning or adding any additional sort columns."},
      {"id": 1, "text": "Z-order the Delta Lake table by purchase probability score and user ID, and change the table format to ORC for better performance."},
      {"id": 2, "text": "Z-order the Delta Lake table by both user ID and purchase probability score, while keeping the existing partitioning strategy by date."},
      {"id": 3, "text": "Partition the Delta Lake table by user ID and apply Z-ordering on purchase probability score."}
    ],
    "ans_id":2,
    "explanation":"Z-order the Delta Lake table by both user ID and purchase probability score, while keeping the existing partitioning strategy by date.\n Z-ordering is a data locality optimization technique that aims to colocate related data on disk. When you frequently filter by both user ID and purchase probability score, Z-ordering by these two columns will group rows with similar values for both columns together. This significantly reduces the amount of data that needs to be scanned when querying with filters on these columns, thus improving query performance. Keeping the existing partitioning by date is beneficial if the team also frequently queries data within specific date ranges, as it allows for partition pruning in addition to the benefits of Z-ordering."
  },
  {
    "question_id":"skillcertpro_e1_q35",
    "question": "In a Databricks machine learning workflow, you're tasked with conducting experiment tracking for a complex deep learning model to ensure reproducibility and analysis of model performance over time. Which of the following statements accurately describe best practices for tracking experiments in Databricks? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Metrics like accuracy, precision, and recall should be logged at multiple intervals during training, not just after the model finishes training."},
      {"id": 1, "text": "Use the autolog function in MLflow for automated logging of parameters and metrics."},
      {"id": 2, "text": "Always log all hyperparameters, even if they dont affect model performance."},
      {"id": 3, "text": "It is better to manually log parameters and metrics for complete control over the experiment tracking process."},
      {"id": 4, "text": "Logging model artifacts is not necessary if the model is already stored in the Databricks model registry."}
    ],
    "ans_ids":[0,1],
    "explanation":"Metrics like accuracy, precision, and recall should be logged at multiple intervals during training, not just after the model finishes training.\n\nLogging metrics at various stages of training (e.g., per epoch or batch) provides valuable insights into the model's learning progress over time. This allows for:\nMonitoring Convergence: Tracking how metrics evolve helps in understanding if and when the model is converging.\nIdentifying Issues Early: Anomalous metric behavior during training can signal problems with the model architecture, hyperparameters, or data.\nComparing Training Runs: Detailed training curves enable a more nuanced comparison of different experiments. \nEarly Stopping Decisions: Observing validation metrics during training can inform decisions about early stopping to prevent overfitting. \n\nUse the autolog function in MLflow for automated logging of parameters and metrics.\n MLflow's autolog feature automatically logs parameters, metrics, and sometimes even artifacts for many popular machine learning libraries (e.g., scikit-learn, TensorFlow, PyTorch). This significantly simplifies the experiment tracking process by:\nReducing Boilerplate Code: Developers don't need to manually specify which parameters and metrics to log.\nEnsuring Consistency: Autologging captures standard information consistently across experiments.\nImproving Efficiency: It streamlines the logging process, allowing data scientists to focus on model development rather than manual tracking."
  },
  {
    "question_id":"skillcertpro_e1_q36",
    "question": "When deploying a machine learning model in Databricks for real-time inference on streaming data using Structured Streaming, which of the following is the most appropriate approach for ensuring scalability and reliability in the production environment?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "You must ensure that the model inference is performed on streaming data using only stateless transformations to avoid potential memory leaks or state inconsistencies over time."},
      {"id": 1, "text": "Deploy the model on a cluster with auto-scaling enabled and leverage Spark's built-in fault tolerance to ensure scalability and resilience during inference on streaming data."},
      {"id": 2, "text": "It is recommended to first collect all the incoming data into memory before running inference to maximize model accuracy and reduce latency in streaming environments."},
      {"id": 3, "text": "Run the model inference as a standalone operation on a single worker node to ensure deterministic results from the streaming data."}
    ],
    "ans_id":1,
    "explanation":"Deploy the model on a cluster with auto-scaling enabled and leverage Spark's built-in fault tolerance to ensure scalability and resilience during inference on streaming data.\n\n This approach directly addresses the requirements of scalability and reliability for real-time inference on streaming data in Databricks.\nAuto-scaling: Enabling auto-scaling on the cluster allows the system to dynamically adjust the number of worker nodes based on the incoming data rate. This ensures that the application can handle fluctuating workloads and maintain performance without manual intervention.\nSpark's Fault Tolerance: Structured Streaming in Spark is built with fault tolerance in mind. It provides mechanisms to handle failures of worker nodes and ensure that data processing continues without loss. This is crucial for maintaining the reliability of the real-time inference pipeline."
  },
  {
    "question_id":"skillcertpro_e1_q37",
    "question": "You are working on a machine learning project using Databricks Delta Lake for managing your datasets. Your dataset is stored in a Delta table and is being regularly updated by different team members. You realize that an incorrect data update was made, and you need to revert the Delta table to a previous version from three versions ago, ensuring that the dataset is accurate for training. However, the same Delta table is still being actively updated by other processes, and you want to avoid interrupting these operations. What is the best approach to handle this situation?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Overwrite the current Delta table with the previous version, using the RESTORE command to revert to three versions ago."},
      {"id": 1, "text": "Pause all current updates to the Delta table, restore it to the previous version, and resume the updates after ensuring data accuracy."},
      {"id": 2, "text": "Use the TIME TRAVEL feature to query the Delta table as it was three versions ago, without restoring or modifying the current table."},
      {"id": 3, "text": "Use the VACUUM command to delete older versions of the Delta table before restoring to prevent any conflicts."}
    ],
    "ans_id":2,
    "explanation":"Use the TIME TRAVEL feature to query the Delta table as it was three versions ago, without restoring or modifying the current table.\n Delta Lake's Time Travel feature allows you to query historical versions of your table without affecting the current state. By using Time Travel, you can access the data as it existed three versions ago to use for training, while the ongoing updates by other processes continue uninterrupted on the current version of the table. This approach provides a non-destructive way to retrieve historical data for specific purposes without interfering with live operations."
  },
  {
    "question_id":"skillcertpro_e1_q38",
    "question": "You are managing a recommendation system for an e-commerce platform. The model suggests products based on user profiles and purchasing history. Over time, the model's recommendations have become less relevant, and click-through rates (CTRs) have dropped. An investigation revealed that a major product category was added to the platform, and users are engaging with the platform in new ways. You want to implement a system to monitor and track these changes to ensure your model adapts appropriately. Which of the following monitoring strategies is most effective for detecting and managing both feature drift and concept drift in your recommendation system?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use drift detection methods like Jensen-Shannon Divergence and Wasserstein Distance to compare feature distributions and trigger alerts when deviations are detected."},
      {"id": 1, "text": "Regularly adjust hyperparameters of the model based on the current user behavior data and retrain the model to keep it aligned with new patterns."},
      {"id": 2, "text": "Track the precision, recall, and F1 score of the model on the test set to monitor concept drift and retrain the model based on performance metrics."},
      {"id": 3, "text": "Collect user feedback on the recommendations and incorporate it into a manual review process to determine when model retraining is needed."}
    ],
    "ans_id":0,
    "explanation":" Use drift detection methods like Jensen-Shannon Divergence and Wasserstein Distance to compare feature distributions and trigger alerts when deviations are detected.\n This strategy directly addresses the detection of feature drift. By continuously monitoring the distributions of input features used by the model and comparing them to a baseline (e.g., the training data distribution or a recent window of production data), methods like Jensen-Shannon Divergence and Wasserstein Distance can quantify the degree of change. Significant deviations trigger alerts, indicating potential feature drift. Detecting feature drift is a crucial first step in understanding if the input data the model is seeing in production has changed compared to what it was trained on, which can be a precursor to concept drift."
  },
  {
    "question_id":"skillcertpro_e1_q39",
    "question": "A manufacturing company has implemented an IoT system to monitor machinery health. They have deployed sensors on the equipment to collect real-time data and want to use machine learning to predict potential failures before they happen. The company needs fast predictions to notify operators as soon as abnormal behavior is detected, reducing the risk of machine downtime. Only a small number of predictions are needed at any given moment, based on the frequency of sensor alerts. What is the most critical advantage of using real time inference in this context?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Real-time inference minimizes prediction lag, ensuring that operators are immediately notified of potential issues."},
      {"id": 1, "text": "Real-time inference increases the models overall accuracy by processing fewer records in real-time."},
      {"id": 2, "text": "Real-time inference enables continuous retraining of the model, improving its long-term performance."},
      {"id": 3, "text": "Real-time inference simplifies the integration of sensor data from multiple sources into a unified model."}
    ],
    "ans_id":0,
    "explanation":"Real-time inference minimizes prediction lag, ensuring that operators are immediately notified of potential issues.\n In a predictive maintenance scenario for machinery health, the most critical advantage of real-time inference is the immediate detection of abnormal behavior. Minimal prediction lag means that as soon as sensor data indicates a potential failure, the ML model can generate a prediction and trigger an alert for the operators. This timely notification is crucial for preventing machine downtime, allowing for proactive maintenance interventions before a breakdown occurs."
  },
  {
    "question_id":"skillcertpro_e1_q40",
    "question": "You are a machine learning engineer at a financial services company. The company has developed a single-node machine learning model to assess the credit risk of loan applicants based on their financial history, income, and other factors. The model has been trained and tested on a single node, and now you are tasked with deploying it to evaluate thousands of loan applications in parallel as part of a batch process. The model is not designed to handle distributed data processing on its own. Your team has decided to use Apache Spark and UDFs (User Defined Functions) to scale the model across the data. Which of the following deployment strategies would be the most appropriate and why?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Deploy the model by converting it into a Spark UDF, which can be applied to each partition of the data,enabling parallel processing"},
      {"id": 1, "text": "Use Spark's MLlib library to convert the single-node model into a distributed version for parallel batch processing."},
      {"id": 2, "text": "Train the model in a distributed manner across Spark clusters, as converting a single-node model to a UDF is inefficient for large datasets."},
      {"id": 3, "text": "Deploy the model on a single node without parallel processing, as Spark UDFs are not designed to handle machine learning models effectively."}
    ],
    "ans_id":0,
    "explanation":"Deploy the model by converting it into a Spark UDF, which can be applied to each partition of the data, enabling parallel processing.\nThis is the most appropriate strategy for scaling a pre-trained, single-node ML model for batch inference on a large dataset using Apache Spark.\n\nLeveraging Spark's Parallelism: Spark distributes data across multiple partitions. By wrapping the single-node model's prediction logic within a UDF, this function can be applied to each row (or batch of rows within a partition) in parallel across the Spark cluster's worker nodes. This allows you to effectively parallelize the inference process without needing to redesign the underlying model for distributed computation. \nEase of Integration: UDFs provide a relatively straightforward way to integrate existing Python-based ML models (common for single-node development) with Spark DataFrames. You can load the model into the driver node's memory and then define a UDF that uses this loaded model to make predictions on the data within each partition.\nSuitability for Batch Processing: Batch inference naturally aligns with Spark's data processing paradigm, where large datasets are processed in parallel. UDFs are well-suited for applying a consistent transformation (in this case, prediction) across a batch of data."
  }
]