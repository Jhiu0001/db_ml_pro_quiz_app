[
  {
    "question_id":"skillcertpro_e3_q11",
    "question": "You have trained a machine learning model in a Databricks notebook, and now you want to register this model in the Databricks Model Registry to version and manage it. Which of the following steps are required to register a new model programmatically using the Databricks MLflow API?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use the mlflow.log_model() function first to log the model, then call the mlflow.register_model() function to register it."},
      {"id": 1, "text": "Save the model locally on your machine and upload it manually to the Model Registry using the Databricks UI."},
      {"id": 2, "text": "Use the mlflow.create_model_version() method to directly create a new model version without logging the model."},
      {"id": 3, "text": "Save the model in any format (e.g., .sav, .pkl), then manually convert it into MLflow format before registering it with the Model Registry."}
    ],
    "ans_id":0,
    "explanation":"Use the mlflow.log_model() function first to log the model, then call the mlflow.register_model() function to register it. \n\nThis is the standard programmatic workflow for registering a model in the MLflow Model Registry using the Databricks MLflow API. mlflow.log_model(model, artifact_path='model'): This function is called within an MLflow run to save the trained model as an artifact of that run. The artifact_path specifies a subdirectory within the run's artifact storage where the model files will be saved. The log_model() function is specific to the MLflow flavor of the model (e.g., mlflow.sklearn.log_model() for scikit-learn models, mlflow.tensorflow.log_model() for TensorFlow models, or the generic mlflow.pyfunc.log_model() for custom Python models). mlflow.register_model(model_uri, name): After the model has been logged as an artifact of an MLflow run, you use the mlflow.register_model() function to register it in the Model Registry. The model_uri argument points to the location of the logged model artifact (typically in the format runs://model), and the name argument specifies the name you want to give to the registered model in the Model Registry. This creates a new registered model with the specified name and adds the logged model as the first version."
  },
  {
    "question_id":"skillcertpro_e3_q12",
    "question": "You are deploying a new machine learning model in production and want to compare it against the current model to ensure the new model performs better for your users. You decide to run an A/B test. Which of the following statements best describes how to properly run the A/B test in this scenario?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Deploy the new model to all users for a week, then roll back to the old model for a week, comparing the performance of both models."},
      {"id": 1, "text": "Deploy the new model to 50% of users and the old model to the other 50% simultaneously, measuring the performance on both sets over time."},
      {"id": 2, "text": "Deploy the new model to all users and use the test set to validate its performance against the old model."},
      {"id": 3, "text": "Deploy both models to the same users in parallel and compare their performance on identical inputs."}
    ],
    "ans_id":1,
    "explanation":"Deploy the new model to 50% of users and the old model to the other 50% simultaneously, measuring the performance on both sets over time.\n\nThis is the standard and most effective way to conduct an A/B test in a production environment. By serving the new model to a subset of users (e.g., 50%) and the existing model to the remaining users concurrently, you can directly compare their performance on live traffic under the same conditions. This simultaneous comparison eliminates the influence of temporal factors or changes in user behavior that might occur if the models were deployed at different times. Measuring performance over time allows you to gather sufficient data to make statistically significant conclusions about which model performs better based on your chosen metrics (e.g., conversion rates, user engagement, error rates)."
  },
  {
    "question_id":"skillcertpro_e3_q13",
    "question": "A marketing team has developed a machine learning model to optimize future marketing campaigns based on customer response data. The customer interaction data is stored in a distributed, low-cost storage system that supports large-scale batch processing but has relatively slow read times. The model's goal is to predict customer segments that are likely to respond to future campaigns. The marketing team runs campaigns monthly, and decisions on the next campaign are made after analyzing customer responses from previous campaigns. Which deployment strategy would be the most effective, given the storage constraints and the use case?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Switching to a higher-performance storage solution is essential to allow real-time decision-making during the campaigns."},
      {"id": 1, "text": "Real-time deployment is necessary because customer preferences change quickly, and you need to respond immediately to changing behavior."},
      {"id": 2, "text": "Stream processing is required to adjust campaign strategies based on live customer interactions and deliver personalized ads in real-time."},
      {"id": 3, "text": "Batch deployment is ideal because the data doesn't need to be accessed in real-time, and predictions can be updated monthly before each campaign."}
    ],
    "ans_id":3,
    "explanation":"Batch deployment is ideal because the data doesn't need to be accessed in real-time, and predictions can be updated monthly before each campaign.\n\n Given that the marketing campaigns are run monthly and the decisions for the next campaign are made after analyzing the responses from the previous one, a batch deployment strategy aligns perfectly with the use case. The data is stored in a low-cost storage system with slow read times, which is suitable for large-scale batch processing that happens periodically (monthly in this case). The predictions for customer segments likely to respond to future campaigns can be generated in batch before the start of each campaign, and these predictions can then be used to optimize the targeting for the upcoming month. There's no need for real-time access or processing given the monthly campaign cycle."
  },
  {
    "question_id":"skillcertpro_e3_q14",
    "question": "You have a machine learning model that has been deployed to the Production stage using Databricks Model Serving. The model serves predictions in real-time. Which of the following statements correctly describes how to query the model in the Production stage for real-time predictions?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "You can query the model's REST API endpoint with JSON data for real-time predictions, and Databricks automatically scales the infrastructure to handle requests."},
      {"id": 1, "text": "You must manually trigger the model's serving endpoint each time new data arrives to generate predictions."},
      {"id": 2, "text": "You can only query the model using batch data files through Databricks Jobs, as real-time queries are not supported."},
      {"id": 3, "text": "You can query the model, but predictions are generated asynchronously, meaning you need to check back later for the results."}
    ],
    "ans_id":0,
    "explanation":"You can query the models REST API endpoint with JSON data for real-time predictions, and Databricks automatically scales the infrastructure to handle requests.\n\n Databricks Model Serving deploys your registered MLflow models as scalable REST API endpoints. To get real-time predictions, you send HTTP POST requests to this endpoint with the input data formatted as JSON. Databricks Model Serving automatically manages the underlying infrastructure, scaling it up or down based on the volume of incoming requests to ensure low latency and high availability for your real-time predictions."
  },
  {
    "question_id":"skillcertpro_e3_q15",
    "question": "You are a machine learning engineer for a retail company that sells products both online and in-store. The company has built a machine learning model to forecast sales demand based on historical sales data, seasonality, promotional events, and economic indicators. Your team is considering how to deploy this model to optimize stock levels and reduce wastage. You are tasked with determining whether batch deployment is appropriate for this use case. Which deployment strategy would you recommend, and why?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Online (real-time) deployment is the best approach since forecasts must be updated continuously as new sales data comes in."},
      {"id": 1, "text": "Batch deployment is ideal because sales forecasts are not needed in real-time, and forecasts can be updated at regular intervals."},
      {"id": 2, "text": "Hybrid deployment, which combines real-time and batch updates, is ideal because it balances real-time needs and overall operational efficiency."},
      {"id": 3, "text": "Deploy the model using stream processing so that each sales transaction immediately triggers an updated forecast for stock levels."}
    ],
    "ans_id":1,
    "explanation":"Batch deployment is ideal because sales forecasts are not needed in real-time, and forecasts can be updated at regular intervals.\n\n Given that the goal is to optimize stock levels and reduce wastage based on sales forecasts, and the inputs include historical data, seasonality, promotional events (which are typically planned in advance), and economic indicators (which change relatively slowly), a real-time forecast updated with every single sale is likely unnecessary and potentially computationally expensive. Stock level adjustments are usually made at regular intervals (e.g., daily, weekly) based on demand forecasts. Therefore, a batch deployment strategy, where the model runs periodically (e.g., daily or weekly) to generate updated sales forecasts, would be sufficient to inform these stock level decisions. This approach aligns well with the nature of the inputs and the frequency of the required outputs."
  },
  {
    "question_id":"skillcertpro_e3_q16",
    "question": "You have a machine learning model registered in the Databricks Model Registry. Your goal is to deploy the model using Databricks Model Serving to handle real-time API requests. Which of the following steps ensures that your model is properly deployed and accessible via an API endpoint?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "client.transition_model_version_stage(\nname='my-model',\nversion=1,\nstage='Production'\n)"},
      {"id": 1, "text": "model_serve.create_endpoint(\nmodel_name='my-model',\nmodel_version='1',\ncompute_target='MLComputeCluster',\nenable_logging=True\n)"},
      {"id": 2, "text": "client.deploy_model(\nmodel_name='my-model',\nversion=1,\nserving_type='batch',\nendpoint='https://my-model-endpoint.com'\n)"},
      {"id": 3, "text": "client.create_webhook(\nevent='MODEL_VERSION_DEPLOYED',\nmodel_name='my-model',\nurl='https://my-model-endpoint.com'\n)"}
    ],
    "ans_id":1,
    "explanation":"\nmodel_serve.create_endpoint(\nmodel_name='my-model',\nmodel_version='1',\ncompute_target='MLComputeCluster',\nenable_logging=True\n) \n\nThis option correctly describes the process of deploying a model for real-time serving using Databricks Model Serving. The 'create_endpoint' function (or a similar function provided by the Databricks Model Serving client library) is used to create a service endpoint that hosts the specified model version. You need to provide the model name and version from the Model Registry, the compute target where the model will be served (e.g., an ML Compute Cluster), and optionally enable logging. Once this endpoint is created, it becomes accessible via an API for real-time inference requests."
  },
  {
    "question_id":"skillcertpro_e3_q17",
    "question": "A retail company has deployed a machine learning model to forecast weekly sales based on various numeric inputs, including store traffic, average basket size, and inventory level. The company has noticed that the model's predictions are less reliable in recent weeks, and there is suspicion that some of the numeric features may have drifted due to seasonal changes. You are tasked with implementing a monitoring system to detect feature drift using summary statistic monitoring. Which of the following numeric feature drift detection strategies is most appropriate for this scenario?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Periodically retrain the model with the latest data and check for significant changes in model coefficients."},
      {"id": 1, "text": "Monitor feature interactions and correlations within the production dataset to detect potential drift."},
      {"id": 2, "text": "Regularly compute and monitor the correlation between each numeric feature and the target variable (weekly sales)."},
      {"id": 3, "text": "Track and compare the mean and standard deviation of each numeric feature in both the training and production datasets over time."}
    ],
    "ans_id":3,
    "explanation":"Track and compare the mean and standard deviation of each numeric feature in both the training and production datasets over time.\n\n Monitoring summary statistics like the mean and standard deviation is a fundamental and effective way to detect distributional shifts in numeric features. If seasonal changes are causing the store traffic, average basket size, or inventory levels to behave differently than they did during the training period, these changes will likely manifest as shifts in the central tendency (mean) or the spread (standard deviation) of these features in the production data compared to the training data. By regularly computing and comparing these statistics, you can identify when significant drift occurs and trigger further investigation or model retraining."
  },
  {
    "question_id":"skillcertpro_e3_q18",
    "question": "A financial institution is using a machine learning model to predict creditworthiness for loan applicants. The model has been in production for six months, and the data science team is monitoring it for drift to ensure it continues making accurate predictions. They have noticed that the distribution of applicant income in recent loan applications differs significantly from the data the model was trained on. What type of drift is this, and how should the team handle it?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Concept drift: retrain the model to account for changes in the relationship between income and creditworthiness."},
      {"id": 1, "text": "Structural drift: redesign the models architecture to better handle the income distribution differences."},
      {"id": 2, "text": "Label drift: review the labels (creditworthiness) assigned to loan applications to ensure consistency over time."},
      {"id": 3, "text": "Data drift: update the models training data to reflect the new income distribution in the loan applications."}
    ],
    "ans_id":3,
    "explanation":"Data drift: update the models training data to reflect the new income distribution in the loan applications.\n\nThe observation that the distribution of applicant income in recent loan applications differs significantly from the training data is a clear example of data drift (specifically, feature drift). This means that the input features the model relies on have changed their statistical properties over time. The most appropriate way to handle this type of drift is to update the model's training data with more recent data that reflects the new income distribution. This allows the model to learn the relationship between the shifted income levels and creditworthiness, helping to maintain its accuracy on the current applicant pool."
  },
  {
    "question_id":"skillcertpro_e3_q19",
    "question": "You have a machine learning model in production that predicts customer loan defaults. Over the past few months, the model's performance has declined, as seen in a drop in key metrics like accuracy and F1-score. You've identified significant drift in multiple numeric features, particularly those related to customer income and loan-to-value ratios. Under which of the following circumstances would retraining and redeploying the model likely be the most appropriate solution to the observed drift?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The drift is caused by an increase in outliers in the data."},
      {"id": 1, "text": "The model's hyperparameters are incorrectly tuned, but the data distribution remains consistent."},
      {"id": 2, "text": "The drift is caused by seasonal variations in the data."},
      {"id": 3, "text": "The feature distributions have changed due to changes in government regulations."}
    ],
    "ans_id":3,
    "explanation":"The feature distributions have changed due to changes in government regulations.\n\nWhen significant and persistent changes in feature distributions are caused by external factors like new government regulations, the underlying relationship between the features and the target variable (loan default) is likely to have also changed. The model, trained on historical data under different regulations, will no longer accurately capture these new relationships. In such cases, simply adjusting the existing model or its hyperparameters is unlikely to be sufficient. Retraining the model on a dataset that incorporates the impact of the new regulations is necessary to allow it to learn the updated patterns and maintain predictive performance."
  },
  {
    "question_id":"skillcertpro_e3_q20",
    "question": "You have a trained machine learning model that currently runs in a batch inference pipeline. The pipeline processes incoming data in batches at a set interval (e.g., daily) and outputs predictions. You need to convert this batch pipeline into a streaming pipeline to process data in real-time using Spark Structured Streaming on Databricks. What steps should you take to successfully deploy this model in a streaming context while maintaining accuracy and consistency? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Checkpoint the streaming pipeline to maintain state and recover from failure without data loss."},
      {"id": 1, "text": "Use the foreachBatch function to apply the model inference on each micro-batch of data."},
      {"id": 2, "text": "Use a sliding window for aggregation over streaming data to replace batch processing."},
      {"id": 3, "text": "Convert the batch DataFrame into a streaming DataFrame by using .toStream()."},
      {"id": 4, "text": "Convert the incoming data stream into a static batch before feeding it into the model for inference."}
    ],
    "ans_ids":[1,2],
    "explanation":"Use the foreachBatch function to apply the model inference on each micro-batch of data.\nIn Spark Structured Streaming, foreachBatch allows you to apply a function to each micro-batch of the streaming DataFrame. This is the standard way to integrate a pre-trained machine learning model (which typically operates on DataFrames) into a streaming pipeline. You load the model outside the foreachBatch function and then use it to make predictions on the DataFrame representing the current micro-batch.\n\nUse a sliding window for aggregation over streaming data to replace batch processing.\n If the original batch pipeline relied on aggregations over a time window (e.g., daily averages), you need to replicate this logic in the streaming pipeline. Spark Structured Streaming provides windowing operations (like window and slidingWindow) that allow you to perform aggregations over continuous streams of data based on time intervals. This ensures that the streaming model receives features that are consistent with those used in the batch pipeline."
  }
]