[
  {
    "question_id":"udemy_e2_q11",
    "question": "What advantage does logging a model signature with an MLflow model provide?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The model will possess a distinct identifier within the MLflow experiment"},
      {"id": 1, "text": "Input data schema validation is enabled during model serving"},
      {"id": 2, "text": "Real-time serving tools can be used for model deployment"},
      {"id": 3, "text": "The model's security is maintained by the developer"},
      {"id": 4, "text": "Input data schema is automatically transformed to align with the signature"}
    ],
    "ans_id":1,
    "explanation":"Input data schema validation is enabled during model serving\n\n When you log a model along with a signature (via the signature= parameter in mlflow.<flavor>.log_model() or by passing an input_example), MLflow captures a ModelSignature that defines the expected input schema (column names and types) and the output schema. During serving—whether via mlflow models serve or Databricks Model Serving—MLflow will automatically validate incoming request payloads against this schema and raise an error if they don't conform. This built-in check guards against runtime failures due to malformed or unexpected input data  "
  },
  {
    "question_id":"udemy_e2_q12",
    "question": "Suppose a machine learning engineer has registered an sklearn model in the MLflow Model Registry utilizing the sklearn model flavor with UI model_uri. Which operation is suitable for loading the model as an sklearn object for batch deployment?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "mlflow.spark.load_model(model_uri)"},
      {"id": 1, "text": "mlflow.pyfunc.read_model(model_uri)"},
      {"id": 2, "text": "mlflow.sklearn.read_model(model_uri)"},
      {"id": 3, "text": "mlflow.pyfunc.load_model(model_uri)"},
      {"id": 4, "text": "mlflow.sklearn.load_model(model_uri)"}
    ],
    "ans_id":4,
    "explanation":"mlflow.sklearn.load_model(model_uri) \n\n The mlflow.sklearn.load_model() function loads an MLflow-registered scikit-learn model as a native sklearn object, preserving its exact class and methods (e.g., predict, transform). When you pass a Model Registry URI such as models:/{model_name}/{version_or_stage}, MLflow retrieves the model artifact and deserializes it using the sklearn flavor, giving you back the original estimator instance for batch scoring or further inspection"
  },
  {
    "question_id":"udemy_e2_q13",
    "question": "A data scientist has configured a machine learning pipeline to automatically log a data visualization with each run. They aim to view these visualizations in Databricks. Where in Databricks can these data visualizations be found?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The MLflow Model Registry Model page"},
      {"id": 1, "text": "The Artifacts section of the MLflow Experiment page"},
      {"id": 2, "text": "Data visualizations cannot be viewed in Databricks through logging"},
      {"id": 3, "text": "The Artifacts section of the MLflow Run page"},
      {"id": 4, "text": "The Figures section of the MLflow Run page"}
    ],
    "ans_id":3,
    "explanation":"The Artifacts section of the MLflow Run page\n\nWhen you call mlflow.log_figure() (or any artifact-logging API) inside an MLflow run on Databricks, the resulting files—including your plots—are stored as artifacts of that run. In the Databricks MLflow UI, you navigate to the specific run's detail page and expand the Artifacts pane to see and preview your logged figures alongside models, metrics, and other files"
  },
  {
    "question_id":"udemy_e2_q14",
    "question": "In a continuous integration, continuous deployment (CI/CD) process for machine learning pipelines, which event typically initiates the automated testing process?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The introduction of a new cost-efficient SQL endpoint"},
      {"id": 1, "text": "Machine learning pipelines do not require CI/CD pipelines"},
      {"id": 2, "text": "The introduction of a new feature table in the Feature Store"},
      {"id": 3, "text": "The deployment of a new cost-efficient job cluster"},
      {"id": 4, "text": "The introduction of a new model version in the MLflow Model Registry"}
    ],
    "ans_id":4,
    "explanation":"The introduction of a new model version in the MLflow Model Registry\n\n In a well-designed MLOps CI/CD pipeline, the arrival of a new model version in the MLflow Model Registry is the primary trigger for: (Automated validation,Unit and integration tests, Bias checks,Performance evaluation, Canary deployments, Automated promotion rules)\nThis aligns directly with how Databricks MLflow workflows are structured:\nA new model version → CI/CD pipeline triggers tests → model is validated → optional automatic stage transition.\nThis is the standard and recommended trigger for ML-specific CI/CD automation."
  },
  {
    "question_id":"udemy_e2_q15",
    "question": "What is the purpose of the context parameter in the predict method of Python models for MLflow?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The context parameter facilitates the specification of the version of the registered MLflow Model to be utilized based on the current scenario of the application"},
      {"id": 1, "text": "The context parameter aids in documenting the performance of a model post-deployment"},
      {"id": 2, "text": "The context parameter enables the inclusion of pertinent details of the business case to assist downstream users in understanding the model's purpose"},
      {"id": 3, "text": "The context parameter permits the provision of the model with custom if-else logic tailored to the application's current scenario"},
      {"id": 4, "text": "The context parameter allows the provision of access to objects like preprocessing models or custom configuration files for the model"}
    ],
    "ans_id":4,
    "explanation":"The context parameter allows the provision of access to objects like preprocessing models or custom configuration files for the model \n\n When you implement a custom PythonModel by subclassing mlflow.pyfunc.PythonModel, MLflow passes a PythonModelContext object into both the load_context() and predict() methods. This context contains:\n*An artifacts dictionary mapping artifact names (as declared at log time) to their local filesystem paths \n*Configuration entries (via context.conf) and other runtime metadata \n\nYou typically use load_context(context) to deserialize heavy artifacts (e.g. a scikit-learn pipeline or a tokenizer) from context.artifacts, and then refer to those loaded objects inside your predict(context, model_input) implementation. Having context available in predict ensures you can access any necessary preloaded models, lookup tables, or custom config files without hard-coding paths"
  },
  {
    "question_id":"udemy_e2_q16",
    "question": "What advantage does the python_function(pyfunc) model flavor offer over the built-in library-specific model flavors?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The python_function model flavor provides no discernible advantages compared to the built-in library-specific model flavors."},
      {"id": 1, "text": "Deploying models using python_function enables parallelizable deployment, enhancing operational efficiency."},
      {"id": 2, "text": "With python_function, models can be deployed without concerning oneself with the specific library used for model creation."},
      {"id": 3, "text": "Storing models in an MLmodel file is facilitated by the python_function model flavor."},
      {"id": 4, "text": "Python_function allows for deploying models without worrying about the deployment environment, whether it's batch, streaming, or real-time."}
    ],
    "ans_id":2,
    "explanation":"With python_function, models can be deployed without concerning oneself with the specific library used for model creation.The python_function (pyfunc) flavor in MLflow serves as a unified, library-agnostic interface for Python models. Any model logged with a built-in flavor (e.g., TensorFlow, SparkML, scikit-learn) automatically includes the pyfunc flavor, allowing it to be loaded and invoked via the same mlflow.pyfunc.load_model() API, regardless of the underlying framework. This interoperability means that downstream deployment tools and serving infrastructures can treat all models uniformly, without special handling per library \n\n"
  },
  {
    "question_id":"udemy_e2_q17",
    "question": "In an existing machine learning pipeline, a machine learning engineer is manually refreshing a model. The pipeline employs the MLflow Model Registry named 'project'. The engineer intends to introduce a new version of the model into 'project'. Which of the following MLflow operations can the engineer utilize to achieve this objective?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Register a model:mlflow.register_model"},
      {"id": 1, "text": "Update a registered model: MlflowClient.update_registered_model"},
      {"id": 2, "text": "Add a model version: mlflow.add_model_version"},
      {"id": 3, "text": "Retrieve a model version: MlflowClient.get_model_version"},
      {"id": 4, "text": "The engineer needs to create an entirely new MLflow Model Registry model."}
    ],
    "ans_id":0,
    "explanation":"Register a model:mlflow.register_model \n\n This function takes a model artifact (e.g., from a run's artifacts:/model path) and adds it as a new version under an existing registered model name.\n\nIf the model already exists in the Model Registry, mlflow.register_model():\n*Does NOT create a new registered model\n*Automatically creates a new model version inside the existing one\n*Tracks the new version just as needed for manual refresh\nExactly matching the engineer's goal:Introduce a new version of the model into 'project'."
  },
  {
    "question_id":"udemy_e2_q18",
    "question": "A machine learning engineer is incorporating the following code block into a batch deployment pipeline:\n\n inference_df=(spark.read.schema(schema).format('delta').table('inference'))\n predictions_df = inference_df.withColumn('prediction',predict(*inference_df.columns))\n\n Which of the following modifications is necessary to ensure this code block functions properly when the inference table is a stream source?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Replace 'inference' with the path to the location of the Delta table."},
      {"id": 1, "text": "Replace schema(schema) with option('maxFilesPerTrigger', 1)."},
      {"id": 2, "text": "Replace spark.read with spark.readStream."},
      {"id": 3, "text": "Replace format('delta') with format('stream')."},
      {"id": 4, "text": "Replace predict with a function optimized for streaming predictions."}
    ],
    "ans_id":2,
    "explanation":"Replace spark.read with spark.readStream. \n\n*Crucial Difference: The spark.read method is designed for batch processing of data at rest. To work with continuously arriving data in a streaming context, you need to use spark.readStream. This method initializes a streaming DataFrame that can process new data as it arrives."
  },
  {
    "question_id":"udemy_e2_q19",
    "question": "A ML engineer is seeking to advance their model version in the MLflow Model Registry from the Staging stage to the Production stage using the MLflow Client. Which of the following code snippets could they utilize to achieve this task?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "client.transition_model_stage (\nname=model,\nversion=model_version,\nfrom='Staging',\nto='Production')"},
      {"id": 1, "text": "client.transition_model_stage (\nname=model,\nversion=model_version,\nstage='Production')"},
      {"id": 2, "text": "client.transition_model_version_stage (\nname=model,\nversion=model_version,\nstage='Production')"},
      {"id": 3, "text": "client.transition_model_version_stage (\nname=model,\nversion=model_version,\nstage='Staging')"},
      {"id": 4, "text": "client.transition_model_version_stage (\nname=model,\nversion=model_version,\nfrom='Staging',\nto='Production')"}
    ],
    "ans_id":2,
    "explanation":"client.transition_model_version_stage (\nname=model,\nversion=model_version,\nstage='Production') \n\n The MLflow Python client provides the transition_model_version_stage method to move a specific model version into a given stage. Specifying stage='Production' on the desired model_version transitions that version from Staging (or any other stage) into Production. This single call is sufficient; you do not need to supply from/to parameters or use a different method name."
  },
  {
    "question_id":"udemy_e2_q20",
    "question": "Instructed by their machine learning engineering manager, all engineers on a team are tasked with appending text descriptions to each model project within the MLflow Model Registry. They've commenced with the 'model' project and aim to incorporate the text into the model_description variable using the provided line of code:\nclient = MlflowClient() client.update_registered_model( name='model', \ndescription=model_description ) \nWhich adjustment is necessary for the team to successfully execute the task?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Replace update_registered_model with update_model_version."},
      {"id": 1, "text": "No changes are required."},
      {"id": 2, "text": "Replace description with artifact."},
      {"id": 3, "text": "Replace client.update_registered_model with mlflow."},
      {"id": 4, "text": "Include a Python model as an argument to update_registered_model."}
    ],
    "ans_id":1,
    "explanation":"No changes are required. \n\n The team wants to update the description of a registered MLflow model. The correct MLflow API for this operation is exactly: \nclient.update_registered_model(\nname='model',\ndescription=model_description\n)This method:\n-Updates the registered model's description (not a specific version)\n-Requires only the name and the description text\n-Does not require any model artifact, version ID, or additional arguments\nTherefore, the given code is already valid and complete."
  }
]