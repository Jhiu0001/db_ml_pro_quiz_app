[
  {
    "question_id":"skillcertpro_e3_q21",
    "question": "You are monitoring a machine learning model in production that predicts customer churn for a telecom company. Over time, the model's accuracy has declined, and you suspect data drift. Which of the following options best describes concept drift and how it might be causing the model's performance to degrade?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The distribution of the input features (X) remains the same, but the relationship between features and the target (Y) has changed over time."},
      {"id": 1, "text": "Both the input features (X) and the relationship between features and the target (Y) have remained stable over time, but the models performance has decreased due to model overfitting."},
      {"id": 2, "text": "The model's performance is declining because of data quality issues, unrelated to any form of drift."},
      {"id": 3, "text": "The distribution of the input features (X) has changed over time, while the relationship between features and target (Y) remains the same."}
    ],
    "ans_id":0,
    "explanation":"The distribution of the input features (X) remains the same, but the relationship between features and the target (Y) has changed over time.\n\n Concept drift specifically refers to a situation where the statistical properties of the target variable (Y) change with respect to the input features (X) over time. In the context of customer churn, this means that the factors influencing why a customer might churn are changing, even if the distribution of the customer characteristics themselves (the input features) remains relatively stable. For example, a new competitor entering the market might cause a different segment of customers to churn than before, altering the relationship the model learned. This change in the underlying relationship leads to a decline in the model's predictive accuracy because it's no longer aligned with the current dynamics of churn."
  },
  {
    "question_id":"skillcertpro_e3_q22",
    "question": "A telecommunications company uses a machine learning model to predict customer churn. The model has been operational for over a year, and the team has implemented a monitoring system to detect different types of drift. Recently, the monitoring system has flagged a significant change in the percentage of customers labeled as churned in the models output, even though the input feature distributions have remained relatively stable. What type of drift is likely occurring, and how should the team address it?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Label drift: review the criteria used to label customers as churned and possibly retrain the model with updated labels."},
      {"id": 1, "text": "Concept drift: recalibrate the models output to account for new patterns in customer behavior."},
      {"id": 2, "text": "Structural drift: alter the models architecture to handle changes in customer churn patterns."},
      {"id": 3, "text": "Data drift: update the input data by incorporating new customer information to account for changing churn rates."}
    ],
    "ans_id":0,
    "explanation":"Label drift: review the criteria used to label customers as churned and possibly retrain the model with updated labels.\n\n The scenario describes a significant change in the percentage of customers labeled as 'churned' by the model, while the input feature distributions remain stable. This is indicative of label drift. Label drift occurs when the distribution of the target variable (in this case, the churn label) changes over time, independent of changes in the input features. This often happens because the definition or the process of labeling the target variable has changed. The team should first review the criteria and process used to label customers as churned to ensure consistency. If the labeling criteria have indeed changed, the model might need to be retrained with a dataset that reflects the new labeling scheme to accurately predict churn under the current definition."
  },
  {
    "question_id":"skillcertpro_e3_q23",
    "question": "You are managing the ML model lifecycle for an e-commerce company. The team is implementing an automated CI/CD pipeline to train, validate, and deploy models using Databricks. One requirement is to notify the operations team via a webhook when a model successfully passes validation and is deployed to production. The webhook endpoint has already been provided by the operations team, and the trigger must occur right after the model deployment is confirmed. Which of the following code blocks correctly triggers the webhook after the model is deployed?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "if model_deployment_status == 'success':\nimport requests\nrequests.post(\n'https://ops-team-webhook.com',\njson={'status': 'Model deployed'},\n)"},
      {"id": 1, "text": "if model_deployment_status == 'success':\nimport requests\nrequests.post(\n'https://ops-team-webhook.com',\ndata={'message': 'Model deployment failed'},\n)"},
      {"id": 2, "text": "if model_deployment_status == 'success':\nrequests.post('https://ops-team-webhook.com')"},
      {"id": 3, "text": "if model_deployment_status == 'success':\nimport requests\nrequests.get('https://ops-team-webhook.com')"}
    ],
    "ans_id":0,
    "explanation":"if model_deployment_status == 'success':\nimport requests\nrequests.post(\n'https://ops-team-webhook.com',\njson={'status': 'Model deployed'},\n)\n\nThis code block correctly implements the requirement of triggering a webhook upon successful model deployment.\n- It checks if the 'model_deployment_status' is 'success'. This assumes that the preceding steps in the CI/CD pipeline have determined the deployment outcome and set this variable accordingly.\n- It imports the 'requests' library, which is essential for making HTTP requests in Python. \n- It uses 'requests.post()' to send an HTTP POST request to the webhook endpoint provided by the operations team (''https://ops-team-webhook.com'&#8242;). POST is a common HTTP method for sending data to a server, which is appropriate for notifying the operations team about an event.\n- It includes a JSON payload ('json={'status': 'Model deployed'}') in the request body. This provides structured information to the webhook endpoint about the event that occurred."
  },
  {
    "question_id":"skillcertpro_e3_q24",
    "question": "While managing multiple MLflow experiments, you want to retrieve all runs that have an accuracy metric greater than 0.9. You also want to retrieve the parameters associated with those runs. What is the correct way to programmatically query the MLflow experiment data for this use case?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "import mlflow\nclient = mlflow.MlflowClient()\nruns = client.search_runs(experiment_ids=['0'],\nfilter_string='accuracy > 0.9')\nfor run in runs:\nprint(run.params)"},
      {"id": 1, "text": "import mlflow\nfrom mlflow.entities import ViewType\nclient = mlflow.MlflowClient()\nruns = client.search_runs(experiment_ids=['0'],\nfilter_string='metrics.accuracy > 0.9',\nrun_view_type=ViewType.ACTIVE_ONLY)\nfor run in runs:\nprint(run.data.params)"},
      {"id": 2, "text": "import mlflow\nclient = mlflow.MlflowClient()\nruns = client.search_runs(experiment_ids=[0],\nfilter_string='metrics['accuracy'] > 0.9')\nfor run in runs:\nprint(run.params)"},
      {"id": 3, "text": "import mlflow\nclient = mlflow.MlflowClient()\nruns = client.query_runs(experiment_ids=[0],\nfilter_string='metrics.accuracy > 0.9')\nfor run in runs:\nprint(run.params)"}
    ],
    "ans_id":1,
    "explanation":"import mlflow\nfrom mlflow.entities import ViewType\nclient = mlflow.MlflowClient()\nruns = client.search_runs(experiment_ids=['0'],\nfilter_string='metrics.accuracy > 0.9',\nrun_view_type=ViewType.ACTIVE_ONLY)\nfor run in runs:\nprint(run.data.params) \n\nThis option correctly uses the 'search_runs' method of the 'MlflowClient' to query runs based on a metric.\n* It correctly filters on the 'accuracy' metric using the syntax 'metrics.accuracy > 0.9'.\n* It accesses the parameters of the retrieved runs using 'run.data.params'.\n* Importing 'ViewType' and using 'run_view_type' is valid, although not strictly necessary for this specific query focusing on active runs (which is the default). Using it explicitly makes the intent clear."
  },
  {
    "question_id":"skillcertpro_e3_q25",
    "question": "You are tasked with monitoring potential drift in the daily_sales feature, a continuous numerical variable representing the total sales in dollars each day for an e-commerce model. You want to choose between using Jensen-Shannon divergence and the Kolmogorov-Smirnov test to detect significant changes in the distribution of daily_sales. Which of the following is the best strategy for choosing between these two methods?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use Jensen-Shannon divergence if you need a probabilistic interpretation of the drift and Kolmogorov-Smirnov if you want to detect the largest point of difference between the distributions"},
      {"id": 1, "text": "Jensen-Shannon divergence should be used for detecting short-term drift, while Kolmogorov-Smirnov is for long-term drift monitoring"},
      {"id": 2, "text": "Use Jensen-Shannon divergence if the data contains missing values and Kolmogorov-Smirnov if the data is complete"},
      {"id": 3, "text": "Always use Kolmogorov-Smirnov for detecting drift in continuous features, as it is more accurate than Jensen-Shannon divergence"}
    ],
    "ans_id":0,
    "explanation":"Use Jensen-Shannon divergence if you need a probabilistic interpretation of the drift and Kolmogorov-Smirnov if you want to detect the largest point of difference between the distributions. \n\n*Jensen-Shannon Divergence: This metric provides a measure of the similarity between two probability distributions. It's based on the Kullback-Leibler divergence and is symmetric and always finite. A lower JSD indicates more similar distributions, and a higher value indicates greater dissimilarity. This makes it suitable when a probabilistic interpretation of how much the distributions have shifted is desired.\n*Kolmogorov-Smirnov (KS) Test: The KS test is a non-parametric test that quantifies the maximum distance between the cumulative distribution functions (CDFs) of two samples. The KS statistic represents the largest vertical difference between the two empirical CDFs. Therefore, it is particularly good at detecting where the distributions differ the most."
  },
  {
    "question_id":"skillcertpro_e3_q26",
    "question": "A telecommunications company has deployed a batch prediction model to predict customer churn. The predictions are stored in a Delta Lake table, which is partitioned by the date of the prediction batch. Over time, the size of the table has grown significantly, and the data science team has noticed that reading predictions based on customer ID is becoming slower. The team wants to optimize read performance for queries that frequently filter by customer ID and plan to use Z-ordering. Which of the following approaches would best optimize the table for queries based on customer ID while keeping the current partitioning strategy?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Partition the Delta Lake table by customer ID instead of the current partitioning by date, then apply Z ordering on date of prediction."},
      {"id": 1, "text": "Z-order the Delta Lake table by customer ID and date of prediction."},
      {"id": 2, "text": "Remove the partitioning by date and use Z-ordering by prediction score to improve query performance."},
      {"id": 3, "text": "Z-order the Delta Lake table by customer ID, without changing the existing partitioning by prediction date."}
    ],
    "ans_id":3,
    "explanation":"Z-order the Delta Lake table by customer ID, without changing the existing partitioning by prediction date.\n\nZ-ordering is a data skipping technique that rearranges the data in a multi-dimensional space to optimize query performance, especially when filtering on multiple columns. In this case, Z-ordering by customer ID will cluster related customer IDs together, allowing the query engine to skip large portions of the data that don't match the customer ID being queried. Maintaining the existing partitioning by prediction date is crucial because it already provides a significant performance boost for time-series queries. Combining partitioning by date with Z-ordering by customer ID provides the best of both worlds: efficient filtering by customer ID and efficient time-based queries."
  },
  {
    "question_id":"skillcertpro_e3_q27",
    "question": "You are working for a retail company that collects real-time data from in-store sensors, tracking customer movement and product interactions. You have developed a recommendation model to suggest products to customers based on their behavior. The model must provide real-time recommendations to customers by performing inference on the live sensor data. You need to implement a system that continuously ingests this sensor data and serves predictions with minimal latency. Which of the following approaches best suits this scenario for continuous inference on the incoming data using Databricks?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Store incoming data in Delta Lake and trigger inference manually when the data reaches a threshold."},
      {"id": 1, "text": "Use Databricks Structured Streaming to continuously ingest and perform inference on the incoming data."},
      {"id": 2, "text": "Leverage Apache Kafka to store sensor data and perform batch inference every hour."},
      {"id": 3, "text": "Use Databricks jobs to run periodic batch predictions on the sensor data."}
    ],
    "ans_id":1,
    "explanation":"Use Databricks Structured Streaming to continuously ingest and perform inference on the incoming data.\n\nDatabricks Structured Streaming is designed for processing and analyzing real-time, continuously flowing data. It allows you to build end-to-end streaming pipelines that can ingest data from various sources, perform transformations and machine learning inference, and write the results to sinks with low latency. This directly addresses the requirement for continuous ingestion and real-time recommendations."
  },
  {
    "question_id":"skillcertpro_e3_q28",
    "question": "You are working as a machine learning engineer in an organization that employs multiple teams to develop machine learning models for various use cases. Your team has just registered a new model in the Databricks Model Registry, and you want to ensure that the model is ready for use by other teams. To facilitate this, you need to clearly mark its lifecycle stage and communicate its readiness for use. Which of the following best describes the appropriate steps for interacting with the Model Registry to manage this models lifecycle and ensure other teams use the correct version?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Automatically transition the model to 'Production' once the model achieves acceptable performance during training without further review."},
      {"id": 1, "text": "Assign the model to the 'Staging' stage initially, and only promote it to 'Production' after validation and testing are complete, ensuring other teams are informed about the correct version."},
      {"id": 2, "text": "Leave the model in the 'None' stage until it has been deployed, and let other teams pick whichever version they prefer for their needs."},
      {"id": 3, "text": "Set the models status to 'Production' immediately upon registering it in the Model Registry to indicate it is ready for use by other teams, regardless of its validation or testing results."}
    ],
    "ans_id":1,
    "explanation":"Assign the model to the 'Staging' stage initially, and only promote it to 'Production' after validation and testing are complete, ensuring other teams are informed about the correct version.\n\nThe Databricks Model Registry provides predefined lifecycle stages (None, Staging, Production, Archived) to manage the maturity and readiness of MLflow models.\n-Staging: This stage is intended for models that have undergone initial training and are candidates for deployment. Placing the newly registered model in 'Staging' signals to other teams that it's available for validation and testing in a pre-production environment.\n-Production: This stage signifies that a model has been thoroughly validated, tested, and approved for use in live systems. Promoting the model to 'Production' after successful validation clearly indicates to other teams that this is the recommended version for deployment. \n-Communication: Informing other teams about the model's stage transitions ensures they are aware of its readiness and can use the appropriate version."
  },
  {
    "question_id":"skillcertpro_e3_q29",
    "question": "You are tasked with deploying a machine learning model that predicts customer churn in real time for a telecom company. The model requires up-to-the-minute data on customer activity, including recent phone calls, data usage, and interaction with customer support. The team has decided to use a real-time stream to collect these features and wants to ensure that the model performs well under a Just-in-Time (JIT) data collection strategy. Which of the following approaches would best support the real-time deployment of this model while ensuring accurate JIT feature collection?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use batch processing to aggregate historical data every 24 hours, then load the aggregated features into the model for predictions."},
      {"id": 1, "text": "Use a real-time data pipeline with event-driven architecture to fetch the most recent customer activity data before making each prediction."},
      {"id": 2, "text": "Implement a micro-batch streaming approach that processes feature updates every 10 minutes and queues them for prediction requests."},
      {"id": 3, "text": "Deploy a model with static feature values, assuming that the features do not change frequently and can be periodically updated."}
    ],
    "ans_id":1,
    "explanation":"Use a real-time data pipeline with event-driven architecture to fetch the most recent customer activity data before making each prediction.\n\nReal-time data pipeline with event-driven architecture: This approach is ideal for JIT (Just-in-Time) feature collection. When a prediction request arrives for a customer, the system triggers events to fetch the latest data on their phone calls, data usage, and support interactions from the relevant real-time data sources. This ensures that the model uses the most up-to-date features for each prediction, maximizing accuracy in a dynamic environment like customer churn. The event-driven nature allows for immediate data retrieval upon prediction request, minimizing latency."
  },
  {
    "question_id":"skillcertpro_e3_q30",
    "question": "You've successfully deployed several versions of a model, and now you want to add detailed metadata to specific versions to track experimental configurations, test results, and deployment history. What is the correct way to add metadata to a registered model version in MLflow?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use the Databricks model lifecycle management tool to directly modify the metadata of any model version in MLflow."},
      {"id": 1, "text": "Use the MLflow Model Registry API to add metadata as tags at the model version level, allowing each version to have unique metadata."},
      {"id": 2, "text": "Version-specific metadata: Using the API, you can log metadata specific to each model version, allowing you to manage version histories more effectively."},
      {"id": 3, "text": "Overwrite the existing model version with a new one that contains metadata in the model artifacts."},
      {"id": 4, "text": "Add metadata to the model version by directly modifying the mlruns directory in the MLflow backend store."}
    ],
    "ans_id":1,
    "explanation":"Use the MLflow Model Registry API to add metadata as tags at the model version level, allowing each version to have unique metadata.\n\n*MLflow allows tags at the model version level, meaning each version can have unique metadata.\n*Tags are ideal for storing:\n  -Experimental configurations\n  -Test results\n  -Deployment history\n*This does not overwrite the model artifacts, so the original model remains intact"
  }
]