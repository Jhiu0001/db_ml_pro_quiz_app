[
  {
    "question_id":"skillcertpro_e2_q41",
    "question": "You have a machine learning model deployed for a batch inference pipeline that processes sales transaction data once every hour to predict potential customer churn. You have been asked to convert this pipeline to a streaming deployment that can provide real-time predictions. The data arrives continuously, and each transaction should be processed as it is received. However, the feature engineering steps in the batch pipeline, such as aggregations over past transactions, need to be adapted to work in a streaming environment. Which of the following is the most appropriate approach to convert the batch inference pipeline to a streaming deployment?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Ignore historical data in the streaming deployment and process each transaction individually without aggregation."},
      {"id": 1, "text": "Run the batch pipeline more frequently (e.g., every minute) to simulate real-time behavior."},
      {"id": 2, "text": "Use windowed aggregations in the streaming pipeline to capture and update feature data over time intervals."},
      {"id": 3, "text": "Simply change the data input source from batch to a stream and continue using the same aggregation logic."}
    ],
    "ans_id":2,
    "explanation":"Use windowed aggregations in the streaming pipeline to capture and update feature data over time intervals.\n\nWindowed aggregations in Structured Streaming are the key to adapting batch-oriented feature engineering (like aggregations over past transactions) for a streaming environment. \n-Windowing: Allows you to define time-based windows (e.g., the last hour, the last 7 days) over your streaming data.\n-Aggregation: You can then apply aggregation functions (e.g., count, sum, average) within these windows.\n-State Management: Structured Streaming handles the state of these windows, updating the aggregations as new data arrives within the defined timeframes.\nThis approach allows you to approximate the historical aggregations used in the batch pipeline in a continuous and real-time manner, enabling your churn prediction model to work effectively with streaming data."
  },
  {
    "question_id":"skillcertpro_e2_q42",
    "question": "You are monitoring a machine learning model that has a feature transaction_amount, which is a continuous numerical variable. Recently, you have noticed potential drift in the distribution of this feature over time. You are considering using either the Jensen-Shannon divergence or the Kolmogorov-Smirnov (KS) test to detect this drift. Which of the following statements best describes the difference between the two methods in this context?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Jensen-Shannon divergence is used for continuous data, while the KS test is best for discrete data"},
      {"id": 1, "text": "The KS test is more sensitive to small differences between distributions, whereas Jensen-Shannon divergence is primarily used for categorical feature drift"},
      {"id": 2, "text": "The Jensen-Shannon divergence is only suitable for categorical data, while the KS test is appropriate for both categorical and numerical data"},
      {"id": 3, "text": "The Jensen-Shannon divergence measures the distance between two probability distributions and is more interpretable than the KS test, which compares the maximum difference between cumulative distributions"}
    ],
    "ans_id":3,
    "explanation":"The Jensen-Shannon divergence measures the distance between two probability distributions and is more interpretable than the KS test, which compares the maximum difference between cumulative distributions \n\n-Jensen-Shannon Divergence (JSD): JSD quantifies the dissimilarity between two probability distributions. It's based on the Kullback-Leibler divergence but is symmetric and always finite. It provides a single numerical value representing the 'distance' or difference between the two distributions, which can be seen as a measure of how much they diverge. This value is often considered more directly interpretable as a degree of difference.\n-Kolmogorov-Smirnov (KS) Test: The KS test is a non-parametric test that assesses whether two underlying one-dimensional probability distributions differ. It does this by calculating the maximum absolute difference between the empirical cumulative distribution functions (ECDFs) of the two samples. The result is a KS statistic and a p-value indicating whether the difference is statistically significant. While it detects if the distributions are different, the KS statistic itself might be less intuitively interpretable as a direct measure of the magnitude of the drift compared to the JSD."
  },
  {
    "question_id":"skillcertpro_e2_q43",
    "question": "You are working on a machine learning experiment where you need to track different sets of hyperparameters (e.g., learning rate, batch size) and evaluation metrics (e.g., accuracy, precision) for your models. You are required to log these parameters and metrics manually using MLflow. Which of the following is the correct approach to achieve this?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use mlflow.set_param() to set parameters and mlflow.set_metric() to set evaluation metrics."},
      {"id": 1, "text": "Use mlflow.log_param() to log both parameters and evaluation metrics in the same call."},
      {"id": 2, "text": "Use mlflow.log_params() to log multiple parameters and mlflow.log_metrics() to log multiple evaluation metrics."},
      {"id": 3, "text": "Log all parameters and metrics directly to a local file, and later upload them using mlflow.log_artifact()"}
    ],
    "ans_id":2,
    "explanation":"Use mlflow.log_params() to log multiple parameters and mlflow.log_metrics() to log multiple evaluation metrics.\n\nMLflow provides specific functions for logging parameters and metrics: \n-mlflow.log_params(params): This function is used to log one or more input parameters of your machine learning model. The params argument should be a dictionary where keys are parameter names and values are their corresponding values.\n-mlflow.log_metrics(metrics, step=None): This function is used to log one or more evaluation metrics. The metrics argument should be a dictionary where keys are metric names and values are their corresponding values. The optional step argument can be used to track metrics over time or iterations. This approach ensures that parameters and metrics are correctly categorized and tracked within the MLflow run."
  },
  {
    "question_id":"skillcertpro_e2_q44",
    "question": "You are working on a machine learning project to predict customer churn for a telecom company. Your team uses a Random Forest model in Databricks, and you've integrated MLflow to track your experiments. You aim to improve the model's performance by tuning its hyperparameters. Specifically, you are experimenting with the number of trees (n_estimators), maximum depth of trees (max_depth), and the minimum samples split (min_samples_split). You ran multiple experiments by logging each combination of these hyperparameters in MLflow, and after analyzing the logged metrics, you observe that increasing the n_estimators beyond 500 does not improve performance. However, reducing min_samples_split to very small values introduces overfitting. Which of the following strategies would most likely result in better model performance without overfitting?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Set n_estimators to 300, set max_depth to 15, and set min_samples_split to 5."},
      {"id": 1, "text": "Increase n_estimators to 750, set max_depth to unlimited, and set min_samples_split to 1"},
      {"id": 2, "text": "Increase n_estimators to 1000, set max_depth to 5, and set min_samples_split to 50."},
      {"id": 3, "text": "Increase n_estimators to 1000, set max_depth to 20, and set min_samples_split to 2."}
    ],
    "ans_id":0,
    "explanation":"Set n_estimators to 300, set max_depth to 15, and set min_samples_split to 5.\n\n Rationale: The question describes a hyperparameter tuning scenario for a Random Forest model where increasing n_estimators beyond 500 yields no performance gain, and very small values of min_samples_split lead to overfitting. The goal is to find a hyperparameter combination that improves performance without overfitting.\nLet's analyze each option based on this information:\n\n-n_estimators is reduced from a point (500) where further increases didn't help, potentially reducing computation without sacrificing performance. max_depth is set to 15, which is a moderate value. A very large or unlimited max_depth can lead to overfitting. \n-min_samples_split is set to 5, which is a small but not extremely small value. This should allow the model to learn from finer-grained patterns without splitting nodes with very few samples, thus mitigating overfitting observed with very small values. This option represents a balanced approach based on the observations."
  },
  {
    "question_id":"skillcertpro_e2_q45",
    "question": "A manufacturing company uses a machine learning model to predict machine failure based on historical sensor data. The model is registered in MLflow Model Registry under the 'Failure_Prediction' name. The company plans to run batch predictions every night on the latest sensor data stored in a Delta Lake table. They need to ensure they always use the version of the model tagged as 'production' in the registry. Which of the following is the best approach to load the production version of the model for batch inference?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use mlflow.pyfunc.load_model('models:/Failure_Prediction/latest') to load the latest registered model from the Model Registry."},
      {"id": 1, "text": "Use mlflow.pyfunc.load_model('models:/Failure_Prediction/Production') to load the model tagged as 'production' from the registry."},
      {"id": 2, "text": "Use mlflow.models.load_model('models:/Failure_Prediction/latest') to load the latest version of the model and run predictions."},
      {"id": 3, "text": "Use mlflow.pyfunc.load_model('runs:/latest') to load the latest run where the model was saved, ensuring that the most recent model is used."}
    ],
    "ans_id":1,
    "explanation":"Use mlflow.pyfunc.load_model('models:/Failure_Prediction/Production') to load the model tagged as 'production' from the registry.\n\n This is the correct approach for batch inference when you want to ensure consistent use of the production model version. The URI format 'models://' allows loading the model currently in a specific stage (like Production, Staging, or Archived) from the MLflow Model Registry. This is the recommended and reliable way to always use the model version designated for production use."
  },
  {
    "question_id":"skillcertpro_e2_q46",
    "question": "You are managing multiple machine learning models within Databricks Model Registry. A key requirement for your team is to organize and document each model effectively by adding relevant metadata such as the author, version notes, and performance metrics (e.g., accuracy, F1 score). Your goal is to register a new version of an existing model and ensure that metadata is added correctly to both the registered model and its specific version for tracking purposes. Which of the following actions should you take to add metadata to a registered model and its specific version in Databricks? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Add metadata to a model version using the 'Description' field in the Databricks UI."},
      {"id": 1, "text": "Manually edit the metadata file inside the Databricks file system (DBFS)."},
      {"id": 2, "text": "Utilize the mlflow.set_tag() function to add tags with metadata to the registered model."},
      {"id": 3, "text": "Use the mlflow.log_params() function before registering the model."},
      {"id": 4, "text": "Assign tags to both the registered model and model version using the mlflow.register_model() function."}
    ],
    "ans_ids":[0,2],
    "explanation":"Add metadata to a model version using the 'Description' field in the Databricks UI.\nThe Databricks Model Registry UI allows you to manually add a description for each model version. This is useful for documenting performance metrics, author information, or version-specific notes,and is commonly used by teams to keep track of important details.\n\n Utilize the mlflow.set_tag() function to add tags with metadata to the registered model.\nThis is also correct. You can use **mlflow.set_tag()** to assign custom metadata (like author name, F1 score, etc.) to either a run, a registered model, or a model version by specifying the appropriate context (e.g., model_name, version). Tags are a powerful way to add structured, searchable metadata in the MLflow ecosystem."
  },
  {
    "question_id":"skillcertpro_e2_q47",
    "question": "You are tasked with building a machine learning model using Databricks and MLflow. The dataset you are working with contains missing values, categorical features, and unscaled numerical data. What is the most appropriate preprocessing logic to ensure your model performs optimally when deployed using MLflow?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Impute missing values using forward fill, ordinal encode categorical variables, and scale numerical features using a Power Transformer."},
      {"id": 1, "text": "Remove rows with missing values, one-hot encode categorical variables, and scale numerical features using the Robust Scaler."},
      {"id": 2, "text": "Impute missing values using the mode, leave categorical variables unchanged, and scale numerical features using the Standard Scaler."},
      {"id": 3, "text": "Impute missing values using the mean, one-hot encode categorical variables, and scale numerical features using the Min-Max scaler."}
    ],
    "ans_id":3,
    "explanation":"Impute missing values using the mean: Imputing missing values is essential, especially for numerical data.The mean is often used when the missing values are assumed to be randomly distributed. It preserves the overall distribution of the data without introducing significant bias.\n\nOne-hot encoding categorical variables: One-hot encoding is a widely accepted method for handling categorical variables, as most machine learning algorithms cannot work directly with categorical data. This technique avoids the assumption of any ordinal relationship among categories, making it preferable for unordered categorical features.\n\nScale numerical features using the Min-Max scaler: Scaling is necessary to ensure that all features are on a similar scale, particularly for algorithms sensitive to feature magnitude (e.g., gradient-based models). Min-Max scaling is beneficial when you want to preserve the range of values (0-1), which is crucial when deploying models for inference in various environments."
  },
  {
    "question_id":"skillcertpro_e2_q48",
    "question": "A retail company is using Databricks to manage the lifecycle of a machine learning model that predicts customer churn. The model is currently deployed in the Staging stage for testing and evaluation, while an older version of the model is running in Production for real-time predictions. The data science team wants to assess how the Staging model will perform in real-time with live customer data before promoting it to Production. What is the best approach to test the Staging model s performance while minimizing risk to the live Production system?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Temporarily promote the Staging model to Production and run performance tests on live data before rolling back to the previous version."},
      {"id": 1, "text": "Apply A/B testing, where a portion of real-time queries is directed to the Staging model, and the rest to the Production model, ensuring safe testing."},
      {"id": 2, "text": "Deploy the Staging model alongside the Production model and query both in real-time using their respective endpoints."},
      {"id": 3, "text": "Run real-time queries on the Production model and use its predictions as input for testing the Staging model."}
    ],
    "ans_id":2,
    "explanation":"Deploy the Staging model alongside the Production model and query both in real-time using their respective endpoints.\n\nThis is correct. In Databricks, it's possible to deploy multiple model versions simultaneously, each under different stages like Staging and Production. You can use the MLflow REST API or pyfunc scoring to invoke each model separately via its own endpoint, allowing you to test the Staging model in real-time using live data without affecting the Production system. This is a safe and effective approach for comparison and validation."
  },
  {
    "question_id":"skillcertpro_e2_q49",
    "question": "You are managing a batch prediction pipeline that predicts the demand for a particular product category for a retail chain. The model was trained six months ago, and over time, you've observed discrepancies between predicted and actual demand. A detailed analysis shows that both the feature distributions and the target variable have drifted. Which approach would provide the most comprehensive solution to address both feature and concept drift in this situation?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Recalibrate the model predictions using a post-hoc adjustment based on recent performance metrics without retraining the model."},
      {"id": 1, "text": "Only monitor feature drift, and retrain the model when the feature distributions deviate from the original training data."},
      {"id": 2, "text": "Deploy a completely new model with different features to eliminate the effects of drift, without relying on the current model."},
      {"id": 3, "text": "Implement a robust monitoring system for both feature and concept drift, followed by periodic model retraining using the most recent data."}
    ],
    "ans_id":3,
    "explanation":"Implement a robust monitoring system for both feature and concept drift, followed by periodic model retraining using the most recent data.\n\nThis is correct. A comprehensive solution to handle both feature drift (changes in input data distribution) and concept drift (changes in the relationship between input and output) involves: Monitoring for drift using tools like MLflow, Delta tables, or custom drift detection logic. Periodically retraining the model with up-to-date data to ensure it reflects the latest patterns in both input features and target outcomes.This ensures that the model adapts over time and remains reliable in production."
  },
  {
    "question_id":"skillcertpro_e2_q50",
    "question": "You are tasked with building a machine learning model using the Databricks platform. You need to optimize the model's performance by fine-tuning hyperparameters through cross-validation. Which of the following approaches is most appropriate for performing hyperparameter tuning with cross-validation to avoid overfitting?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Perform grid search on hyperparameters using the entire dataset (training + test) to find the best parameters."},
      {"id": 1, "text": "Split the dataset into training, validation, and test sets; tune hyperparameters on the test set."},
      {"id": 2, "text": "Perform nested cross-validation, where the outer loop is used to evaluate the model, and the inner loop is used for hyperparameter tuning."},
      {"id": 3, "text": "Use k-fold cross-validation on the training set and select the model with the highest validation score."}
    ],
    "ans_id":2,
    "explanation":"Perform nested cross-validation, where the outer loop is used to evaluate the model, and the inner loop is used for hyperparameter tuning.\n\nThis is correct. Nested cross-validation is a best practice for avoiding overfitting during hyperparameter tuning.The inner loop tunes the model using cross-validation on the training folds. The outer loop evaluates the model performance on unseen validation folds. This technique provides an unbiased estimate of model performance and ensures that hyperparameter choices do not leak into the evaluation step, making it especially valuable in high-stakes environments like those encountered in Databricks ML workflows."
  }
]