[
  {
    "question_id":"skillcertpro_e1_q51",
    "question": "You are working on a fraud detection model that will be deployed in a real-time prediction pipeline. To make sure that the model is correctly validated and compatible with the production environment, your team decides to log input examples during the experiment. These examples are meant to serve as reference inputs that can be used to validate model behavior during deployment and testing. After logging the input examples with the model, a new set of data arrives with some additional features that were not present in the original training data. What is the best practice for logging and using input examples in MLflow to ensure smooth deployment and compatibility with the updated data schema?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use the mlflow.log_param() function to store a sample of the input data as parameters for future comparison during deployment."},
      {"id": 1, "text": "Use the mlflow.log_model() function to log the model along with an input example that contains a representative sample of the data used during training."},
      {"id": 2, "text": "Use Delta tables to store all input examples, and manually retrieve them during the deployment process for validation."},
      {"id": 3, "text": "Log input examples using mlflow.log_input_example() separately from the model, as this allows you to change the examples after the model has been logged."}
    ],
    "ans_id":1,
    "explanation":"Use the mlflow.log_model() function to log the model along with an input example that contains a representative sample of the data used during training.\n When logging a model with mlflow.log_model(), you can provide an input_example argument. This is the recommended way to store a sample of the expected input schema along with the model."
  },
  {
    "question_id":"skillcertpro_e1_q52",
    "question": "You are working for an e-commerce platform that uses a machine learning model to detect fraudulent transactions. The current batch pipeline collects data over a 24-hour period, and at the end of the day, it processes the transactions through an anomaly detection model. The company wants to upgrade the batch pipeline to a streaming one so that fraud can be detected in real time as transactions are processed. The model relies on features such as daily transaction count, total transaction value, and user activity history. What is the best approach to convert this batch pipeline into a streaming pipeline using Databricks?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Switch the processing mode to continuous to ensure all data is processed in real-time without requiring micro-batch intervals."},
      {"id": 1, "text": "Keep the same data processing logic but configure a micro-batch interval to process incoming transactions every second."},
      {"id": 2, "text": "Use a processing-time window to detect fraud based on the transaction timestamp rather than the users historical activity."},
      {"id": 3, "text": "Use Structured Streaming, introduce stateful processing to maintain historical transaction counts and values, and apply the anomaly detection model periodically on the updated state."}
    ],
    "ans_id":3,
    "explanation":"Use Structured Streaming, introduce stateful processing to maintain historical transaction counts and values, and apply the anomaly detection model periodically on the updated state.\n This is the most appropriate approach for converting a batch fraud detection pipeline to a real-time streaming pipeline in Databricks, especially given that the model relies on features like daily transaction count, total transaction value, and user activity history, which inherently require maintaining state over time.\n-Structured Streaming: Databricks Structured Streaming is designed for scalable and fault-tolerant processing of real-time data streams.\n-Stateful Processing: To calculate features like daily transaction count and total transaction value in a streaming context, you need to maintain state across events belonging to the same user within a defined window (e.g., a sliding window for the last 24 hours). Structured Streaming provides mechanisms for stateful operations (e.g., groupBy with windowing and aggregation).\n-Periodic Model Application: While the goal is real-time detection, applying a potentially complex anomaly detection model on every single transaction might be computationally expensive. A common strategy is to update the aggregated state (counts, sums, history) in real-time and then apply the anomaly detection model periodically (e.g., every few seconds or minutes) on this updated state to generate a near real-time fraud score. This balances latency and computational cost."
  },
  {
    "question_id":"skillcertpro_e1_q53",
    "question": "As a machine learning engineer for a large retail company, you have multiple Databricks Jobs that are triggered by external events, such as data ingestion or model evaluation completion. To manage and audit your automation pipeline, you need to regularly check the webhooks that are registered within the Databricks workspace. Your goal is to retrieve the list of all registered webhooks and check their configurations to ensure they are functioning correctly. Which of the following methods is the correct way to list all webhooks in a Databricks workspace?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Write a Python script in a Databricks notebook that queries the jobs table in the Databricks metastore to retrieve all webhooks related to jobs."},
      {"id": 1, "text": "Use the Databricks REST API to call the 'webhooks/list' endpoint, which will return a JSON object containing all registered webhooks."},
      {"id": 2, "text": "Access the Databricks Job settings UI and manually navigate to the Webhooks section, where all webhooks are displayed in a dashboard view."},
      {"id": 3, "text": "Open the Databricks model registry, where webhooks associated with model versions are automatically listed for each registered model."}
    ],
    "ans_id":1,
    "explanation":"Use the Databricks REST API to call the 'webhooks/list' endpoint, which will return a JSON object containing all registered webhooks.\n\n The Databricks REST API provides programmatic access to various Databricks functionalities, including managing webhooks. The existence of a webhooks/list endpoint (or a similar endpoint under a broader webhooks API) is the standard way to retrieve a list of all configured webhooks within a Databricks workspace. This allows for automated retrieval and inspection of webhook configurations. The response would typically be in JSON format, which can be easily parsed and processed by scripts or other applications."
  },
  {
    "question_id":"skillcertpro_e1_q54",
    "question": "You are building a machine learning pipeline, and you need to create a new feature table in the Databricks Feature Store that will be used by multiple downstream models. The table contains customer demographic data. Later, you realize that you need to update this feature table with new columns and fresh data. What is the best practice for updating the feature table without losing the lineage of the previous versions used by the models?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Create a new version of the feature table while keeping the previous version intact."},
      {"id": 1, "text": "Append the new data to the existing feature table and manually adjust downstream models."},
      {"id": 2, "text": "Overwrite the existing feature table with the new data using the same name."},
      {"id": 3, "text": "Drop the existing feature table and create a new one with the updated columns and data."}
    ],
    "ans_id":0,
    "explanation":"Create a new version of the feature table while keeping the previous version intact.\n\n Creating a new version of the feature table within the Databricks Feature Store is the best practice for updating it without losing lineage. \n-Lineage Tracking: The Feature Store is designed to track the lineage of features used by different models. By creating a new version, you maintain a history of how the features looked at different points in time. Downstream models that were trained or are running on the older version of the feature table will continue to use that specific version, preserving reproducibility and preventing unexpected changes in model behavior.\n-Controlled Updates: You can then selectively update downstream models to use the new version of the feature table after evaluating its impact and ensuring compatibility. \n-Rollback Capability: If issues arise with the new version, you can easily revert to the previous version for your models."
  },
  {
    "question_id":"skillcertpro_e1_q55",
    "question": "You are tasked with deploying a pre-trained single-node machine learning model for batch scoring. The model is designed to predict customer churn, and you plan to use spark_udf to apply the model in parallel over a large dataset of customer features stored in a Delta table. What is the most efficient way to deploy this model using spark_udf in a batch processing job, while ensuring parallel execution and avoiding resource bottlenecks?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Convert the model to a Pandas UDF and apply it on a DataFrame loaded from the Delta table in chunks."},
      {"id": 1, "text": "Use a Python UDF with a for-loop to sequentially apply the model over rows in the DataFrame."},
      {"id": 2, "text": "Load the model in each Spark worker and apply it in parallel using spark_udf in the DataFrame's withColumn method."},
      {"id": 3, "text": "Apply the model in local mode using a single machine and then distribute the results using Spark's repartition method."}
    ],
    "ans_id":2,
    "explanation":"Load the model in each Spark worker and apply it in parallel using spark_udf in the DataFrame's withColumn method.\n\n This is the most efficient way to deploy a pre-trained single-node model for parallel batch scoring using spark_udf.\n-Parallel Execution: Spark distributes the data across multiple partitions and executors (workers). By loading the model on each worker and using spark_udf within withColumn, the model's prediction logic is applied in parallel to the data within each partition on the respective worker. This fully leverages Spark's distributed computing capabilities.\n-Avoiding Resource Bottlenecks: Loading the model on each worker avoids the bottleneck of sending all the data to a single node where the model is loaded. Each worker has its own copy of the model (or can load it once per worker process), allowing for distributed computation without overwhelming the driver node or a single executor.\n-spark_udf Efficiency: While Python UDFs have some serialization overhead, they are often the most straightforward way to integrate arbitrary Python code (like the prediction logic of a single-node model) into Spark's parallel processing framework."
  },
  {
    "question_id":"skillcertpro_e1_q56",
    "question": "A large e-commerce company has a recommendation system for personalizing product suggestions, which currently operates in batch mode, generating recommendations overnight based on user activity from the previous day. The company wants to move to a real-time recommendation system that adjusts suggestions as users browse products on the website. To achieve this, the company plans to convert its batch deployment pipeline to a streaming deployment using Databricks Structured Streaming. What is the most significant challenge you are likely to encounter when converting the batch pipeline to a streaming pipeline for real-time inference?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Transitioning from a stateless to a stateful model during inference"},
      {"id": 1, "text": "Maintaining the same data ingestion rates as in the batch pipeline"},
      {"id": 2, "text": "Handling large-scale data in real-time streams with consistent latency"},
      {"id": 3, "text": "Training the model on the streaming data instead of batch data"}
    ],
    "ans_id":2,
    "explanation":"Handling large-scale data in real-time streams with consistent latency \n\nThis is the most significant challenge when converting a batch recommendation system to a real-time one using Structured Streaming.\n-Scalability and Throughput: Real-time systems need to process a continuous flow of data as it arrives, which can be high volume in a large e-commerce setting. Ensuring the streaming pipeline can handle this throughput without backing up is crucial. \n-Latency Requirements: Real-time recommendations need to be generated and displayed to users with minimal delay (low latency) to provide a seamless browsing experience. Maintaining consistent low latency under varying data loads can be technically demanding.\n-Resource Management: Efficiently managing compute resources to handle the continuous processing and low latency requirements of a large-scale real-time system can be complex and costly."
  },
  {
    "question_id":"skillcertpro_e1_q57",
    "question": "You are managing the deployment of several machine learning models in Databricks for a large healthcare provider. One of the models, 'Patient Risk Predictor,' is versioned in Databricks' Model Registry and is used in both batch and real-time scoring applications. You need to load the model in a nightly batch job that runs on a Spark cluster to score patient risk levels based on pre-aggregated features. The batch job must always use the 'production' version of the model to ensure stable and reliable predictions. Which of the following is the best practice for loading the 'Patient Risk Predictor' model in the batch job to ensure stability and consistency?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Directly query the model's underlying artifact location and load the model from there."},
      {"id": 1, "text": "Trigger an MLflow experiment run from the batch job to train and register a new version of the model every night."},
      {"id": 2, "text": "Use mlflow.pyfunc.load_model('models:/Patient Risk Predictor/production') to load the model in the 'production' stage."},
      {"id": 3, "text": "Use mlflow.pyfunc.load_model('models:/Patient Risk Predictor/latest') to load the latest version of the model."}
    ],
    "ans_id":2,
    "explanation":"Use mlflow.pyfunc.load_model('models:/Patient Risk Predictor/production') to load the model in the 'production' stage.\n\n This is the best practice for ensuring stability and consistency when loading a production model for a batch job.\n-Explicitly Specifying the Stage: By using 'production', you are explicitly telling MLflow to load the model version that has been designated as the production-ready version in the Model Registry. This ensures that the batch job always uses the stable and approved model for scoring.\n-Abstraction from Version Numbers: You don't need to hardcode a specific version number (e.g., /12), which would require updating the job every time a new production version is rolled out. MLflow handles resolving the 'production' alias to the current production version.\n-Integration with Model Registry Governance: This approach leverages the governance and stage management features of the MLflow Model Registry, ensuring that the batch job adheres to the defined model lifecycle."
  },
  {
    "question_id":"skillcertpro_e1_q58",
    "question": "You have a model in the 'Production' stage, but a newer version of the model has been promoted to 'Production,' making the older version obsolete. What are the best practices for managing the old model version in the Databricks Model Registry, and which command should you use to archive or delete it?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use mlflow.delete_model_version() to completely remove the old version from the registry."},
      {"id": 1, "text": "Archive the old version using mlflow.archive_model_version(), as this will preserve the version history but mark it as deprecated."},
      {"id": 2, "text": "Use mlflow.transition_model_version_stage() to move the old version to the 'Archived' stage, ensuring that its history is preserved but it is no longer active."},
      {"id": 3, "text": "Move the old version to 'Staging' using mlflow.transition_model_version_stage() to ensure it is not in active use but can still be re-promoted if necessary."}
    ],
    "ans_id":2,
    "explanation":"Use mlflow.transition_model_version_stage() to move the old version to the 'Archived' stage, ensuring that its history is preserved but it is no longer active. \n\nThis is the recommended best practice for managing obsolete production model versions in the MLflow Model Registry.\n-Preserves History: Transitioning to the 'Archived' stage keeps the model version and its associated metadata (run ID, metrics, parameters, artifacts) in the registry for historical tracking, auditability, and reproducibility if needed in the future.\n-Marks as Inactive: The 'Archived' stage clearly indicates that this version is no longer intended for active use in production or other deployment environments.\n-Clear Lifecycle Management: It follows a standard model lifecycle management practice, allowing teams to understand the status of different model versions."
  },
  {
    "question_id":"skillcertpro_e1_q59",
    "question": "In a Databricks environment, you are tasked with automating the retraining of machine learning models. You need to decide between using job clusters and all-purpose clusters for this process. Which of the following describes the primary advantage of using job clusters over all-purpose clusters for model lifecycle automation?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Job clusters automatically scale to handle unexpected workloads, reducing manual intervention during long-running processes."},
      {"id": 1, "text": "Job clusters are created on demand for specific jobs and automatically terminated after the job completes, reducing operational costs."},
      {"id": 2, "text": "Job clusters provide dedicated GPUs for every task, enhancing model training speed without requiring manual hardware configurations."},
      {"id": 3, "text": "Job clusters are optimized for real-time data processing, ensuring minimal latency during training and inference processes."}
    ],
    "ans_id":1,
    "explanation":" Job clusters are created on demand for specific jobs and automatically terminated after the job completes, reducing operational costs.\n\nThe primary advantage of using job clusters for model lifecycle automation, such as automated retraining, is their cost-effectiveness. Job clusters are ephemeral; they are spun up only when a scheduled or triggered job needs to run and are automatically terminated once the job is finished. This prevents incurring costs for idle compute resources, which can be a significant factor when automating recurring tasks like model retraining."
  },
  {
    "question_id":"skillcertpro_e1_q60",
    "question": "You are conducting a machine learning experiment that involves hyperparameter tuning, where each tuning iteration is considered a nested run within a parent run. You want to properly track both the parent experiment and the individual hyperparameter trials as nested runs in MLflow. Which of the following statements correctly describe how to manage and track nested runs in MLflow? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Use mlflow.start_run() without specifying the nested argument to track both parent and nested runs."},
      {"id": 1, "text": "Use mlflow.create_experiment() to create a new experiment for each nested run."},
      {"id": 2, "text": "Use mlflow.start_run() with the nested=True argument to create a nested run under an active parent run."},
      {"id": 3, "text": "Use mlflow.set_tag() in each nested run to associate metadata with the parent run."},
      {"id": 4, "text": "Use mlflow.log_artifact() to automatically create nested runs for each artifact generated during the run."}
    ],
    "ans_ids":[2,3],
    "explanation":"Use mlflow.start_run() with the nested=True argument to create a nested run under an active parent run.This is the fundamental way to establish the parent-child relationship between MLflow runs. When nested=True is used within a with mlflow.start_run(): block, it creates a child run that is associated with the currently active (parent) run. This allows for a hierarchical organization of experiments, where hyperparameter tuning trials (nested runs) are clearly linked to the overall experiment (parent run).\n\n Use mlflow.set_tag() in each nested run to associate metadata with the parent run. While mlflow.set_tag() primarily adds metadata to the specific run where it's called (the nested run in this case), these tags are visible when viewing the parent run and its associated nested runs in the MLflow UI or when querying the MLflow tracking server. Using tags in nested runs to identify or categorize them in relation to the parent experiment is a good practice for organization and analysis. For instance, you might tag each nested run with the hyperparameter values being tested."
  }
]