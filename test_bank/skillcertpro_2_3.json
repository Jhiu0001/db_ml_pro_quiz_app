[
  {
    "question_id":"skillcertpro_e2_q21",
    "question": "A logistics company is deploying a machine learning model to predict delivery delays in real time based on incoming sensor data from trucks. The data stream includes information on vehicle location, weather conditions, and traffic patterns. The model's predictions must be combined with business rules that include customer SLAs (Service Level Agreements) and geographical restrictions. These rules are complex and involve multi-level conditions, including handling exceptions for high-priority deliveries. Why is it critical to handle such complex business logic within the streaming pipeline instead of deferring to a downstream system for post-processing?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "To increase the overall throughput of the streaming system by removing downstream complexity"},
      {"id": 1, "text": "To reduce computational costs by handling all logic in the batch processing stage"},
      {"id": 2, "text": "To ensure that business rules are applied only to completed batches of data"},
      {"id": 3, "text": "To avoid high-latency processing, ensuring that decisions are made in real-time"}
    ],
    "ans_id":3,
    "explanation":"To avoid high-latency processing, ensuring that decisions are made in real-time \n\n In a real-time delivery delay prediction system, immediate action based on predictions and business rules is crucial. Deferring the complex business logic (including SLAs and geographical restrictions) to a downstream system would introduce significant latency. The predictions would be made in real-time, but the crucial decisions based on those predictions and the business context would be delayed, potentially leading to missed SLAs, inefficient routing, and customer dissatisfaction. Handling the business logic within the streaming pipeline ensures that decisions are made with minimal delay, enabling immediate responses and adjustments."
  },
  {
    "question_id":"skillcertpro_e2_q22",
    "question": "In your Databricks environment, you are running nested experiments with MLflow to track various sub-tasks in a complex pipeline. You encounter an issue where nested runs are not properly linked to their parent runs. Which of the following could be the reason for this problem?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The parent run must be active when starting a nested run, and the nested=True parameter must be explicitly passed."},
      {"id": 1, "text": "You need to set the run_id of the parent run manually in each nested run to establish the hierarchy."},
      {"id": 2, "text": "Nested runs are only supported in Databricks environments and not in local MLflow setups."},
      {"id": 3, "text": "Nested runs must be started with mlflow.set_experiment() to properly link to the parent run."}
    ],
    "ans_id":0,
    "explanation":"The parent run must be active when starting a nested run, and the nested=True parameter must be explicitly passed.\n To correctly link nested MLflow runs to their parent, two conditions must be met:\n-Active Parent Run: A parent run must be active (i.e., started but not yet ended) when you initiate a nested run. MLflow uses the currently active run as the parent for any new runs started within its context. \n-nested=True Parameter: When starting a nested run using mlflow.start_run(), you need to explicitly set the nested parameter to True. This tells MLflow to treat the new run as a child of the currently active run, establishing the hierarchical relationship in the tracking UI."
  },
  {
    "question_id":"skillcertpro_e2_q23",
    "question": "You are working on a project that requires processing continuous streams of sensor data from IoT devices deployed in a smart city environment. Each device sends data such as temperature, humidity, and GPS coordinates every second. You need to deploy a real-time ETL pipeline on Databricks to aggregate this data and monitor environmental conditions in various city zones. The business requires that if a sensor does not send data for more than 5 minutes, it should be flagged as inactive and included in daily reports for maintenance purposes. What is the best approach for implementing this use case using Structured Streaming?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Set a micro-batch interval of 5 minutes to ensure that data is processed as close to real-time as possible."},
      {"id": 1, "text": "Use the mapGroupsWithState function to maintain the state of each sensor and detect inactivity over time."},
      {"id": 2, "text": "Configure the pipeline with the append output mode to continuously output the sensor states to a database."},
      {"id": 3, "text": "Use static partitioning of the data based on the sensor ID to enable easier query and aggregation of sensor states."}
    ],
    "ans_id":1,
    "explanation":"Use the mapGroupsWithState function to maintain the state of each sensor and detect inactivity over time.\n\n The mapGroupsWithState function in Structured Streaming is specifically designed for maintaining stateful computations across micro-batches for each group of data (in this case, grouped by sensor ID). This allows you to track the last activity time for each sensor. You can define a timeout period (5 minutes in this scenario) within the stateful function. If a new event doesn't arrive for a sensor within this timeout, you can flag it as inactive and update its state accordingly. This state can then be used for generating daily reports."
  },
  {
    "question_id":"skillcertpro_e2_q24",
    "question": "You are managing multiple machine learning models in Databricks using MLflow. To better organize and track the purpose and context of each registered model, you decide to add metadata to the models. What is the most appropriate method for adding metadata to a registered model in MLflow?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Attach a separate text file containing metadata to the registered model in the MLflow UI."},
      {"id": 1, "text": "Use the MLflow Model Registry API to log metadata as key-value tags for each registered model."},
      {"id": 2, "text": "Add metadata using the Databricks workspace CLI, and then sync the changes with MLflow."}
    ],
    "ans_id":1,
    "explanation":" Use the MLflow Model Registry API to log metadata as key-value tags for each registered model. \n\nAPI support: The MLflow Model Registry API indeed provides functionalities to interact with registered models programmatically.\nKey-value tagging: The API allows you to add metadata as key-value tags to registered models and specific model versions. This approach ensures that all model-related metadata is stored alongside the model in the registry, making it easily accessible for tracking and management. Tags are directly associated with the model within the MLflow tracking system"
  },
  {
    "question_id":"skillcertpro_e2_q25",
    "question": "Your data science team is using the MLflow Model Registry to deploy machine learning models. After registering a model and assigning it to the Staging stage, what typical actions can users perform within the Model Registry to ensure that the model is ready for production deployment?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Users can add model descriptions, perform stage transitions, and apply version tags to track deployment history."},
      {"id": 1, "text": "Users can configure the Model Registry to automatically deploy any new model version that is added, bypassing the need for human validation."},
      {"id": 2, "text": "Users can track changes to the dataset used during model training, ensuring that any modifications to data are automatically reflected in the registered model."},
      {"id": 3, "text": "Users can automatically update the model's parameters based on real-time performance and directly modify the model artifacts in the registry."}
    ],
    "ans_id":0,
    "explanation":"Users can add model descriptions, perform stage transitions, and apply version tags to track deployment history.\n\n After a model is in the 'Staging' stage, typical actions to prepare it for production include: \n-Adding Model Descriptions: Providing detailed information about the model's purpose, features, and intended use. \n-Performing Stage Transitions: Moving the model to the 'Production' stage once testing and validation in 'Staging' are successful.\n-Applying Version Tags: Adding tags to specific model versions to mark them as part of a particular deployment or to note important milestones in their lifecycle. These actions help in managing the model's readiness and tracking its journey towards production."
  },
  {
    "question_id":"skillcertpro_e2_q26",
    "question": "You are developing a complex machine learning pipeline where different stages of the pipeline (e.g., data preprocessing, feature extraction, and model training) are treated as nested runs under a single parent experiment run. Each stage requires its own tracked metrics, parameters, and artifacts. Which of the following actions are required to properly track each stage of the pipeline as a nested run? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Use mlflow.log_metrics() and mlflow.log_params() for each nested run to log stage-specific metrics and parameters."},
      {"id": 1, "text": "Use mlflow.register_model() to automatically log models and metrics in each nested run."},
      {"id": 2, "text": "Use mlflow.set_experiment() for each stage to assign them as part of the same parent experiment."},
      {"id": 3, "text": "Call mlflow.end_run() after each stage to ensure correct logging of the pipeline stages."},
      {"id": 4, "text": "Use mlflow.start_run() with nested=True to ensure each stage of the pipeline is tracked under the parent run."}
    ],
    "ans_ids":[0,4],
    "explanation":"Use mlflow.log_metrics() and mlflow.log_params() for each nested run to log stage-specific metrics and parameters. \nFor each stage of the pipeline that is tracked as a nested run, it's essential to log the specific metrics and parameters relevant to that stage. For example, the data preprocessing stage might have metrics related to data quality or the number of features created, while the model training stage will have training and validation metrics and hyperparameters. Using mlflow.log_metrics() and mlflow.log_params() within each nested run ensures that this stage-specific information is correctly associated with that particular run. \n\n Use mlflow.start_run() with nested=True to ensure each stage of the pipeline is tracked under the parent run. \nTo establish the hierarchical relationship where each stage is a nested run under a parent experiment run, you need to use mlflow.start_run(nested=True) when initiating each stage. This tells MLflow that the new run is a child of the currently active (parent) run. Without nested=True, each stage would be treated as an independent top-level run."
  },
  {
    "question_id":"skillcertpro_e2_q27",
    "question": "A financial institution has deployed a credit scoring model that predicts the likelihood of loan default based on various customer features (income, credit history, age, etc.). Six months after deployment, the data science team notices that the distribution of several input features has shifted significantly compared to the original training data, but the model's predictions seem to be performing as expected. What type of drift is most likely occurring, and what should the team prioritize to ensure the model continues to perform well?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Feature drift is most likely occurring, and the team should prioritize investigating which features have drifted and evaluate whether retraining is necessary."},
      {"id": 1, "text": "Feature drift is occurring, but it can be ignored as long as the model s performance metrics remain stable and predictions are accurate."},
      {"id": 2, "text": "Neither label drift nor feature drift is occurring, and the team should investigate for data integrity issues."},
      {"id": 3, "text": "Label drift is most likely occurring, and the team should focus on correcting the target variable by retraining the model on new data."}
    ],
    "ans_id":0,
    "explanation":" Feature drift is most likely occurring, and the team should prioritize investigating which features have drifted and evaluate whether retraining is necessary.\n\n The scenario clearly states that the distribution of several input features has shifted significantly compared to the training data. This is the definition of feature drift (also known as covariate drift). Even if the model's performance currently appears stable, feature drift can be a precursor to concept drift, where the relationship between the features and the target variable (loan default) might change in the future due to the altered feature distributions. Therefore, it's crucial to investigate which features are drifting and assess the potential impact on the model's long-term performance and fairness. Retraining with data that reflects the new feature distributions might be necessary to maintain accuracy and robustness."
  },
  {
    "question_id":"skillcertpro_e2_q28",
    "question": "You are tasked with deploying a machine learning model in a streaming environment for fraud detection in real-time transactions. The system is required to handle incoming transaction data at high throughput and apply the model's predictions in real-time. The business logic for identifying fraudulent activity is complex and involves evaluating transaction history, user profiles, geographical data, and external fraud indicators. How should this business logic be handled in the streaming deployment?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Handle the complex business logic within the model itself by training it on all necessary data."},
      {"id": 1, "text": "Incorporate business logic into the streaming deployment by using stateful operations to track patterns over time"},
      {"id": 2, "text": "Directly apply the model's predictions on each event without additional business logic."},
      {"id": 3, "text": "Preprocess the data stream to simplify the input features and handle business logic outside the streaming pipeline"}
    ],
    "ans_id":1,
    "explanation":"Incorporate business logic into the streaming deployment by using stateful operations to track patterns over time.\n\n Handling complex business logic within the streaming pipeline, especially using stateful operations, is crucial for real-time fraud detection.\n-Stateful Operations: Fraud detection often requires analyzing patterns across multiple events or over a period for a single user or entity. Stateful operations in Structured Streaming (like mapGroupsWithState, windowing) allow you to maintain and update information about past transactions, user behavior, etc., which is essential for implementing complex business rules that look for sequences or aggregations of events. \n-Real-time Decision Making: Applying business logic within the streaming pipeline ensures that the fraud evaluation is done in real-time, as each transaction arrives. This allows for immediate action (e.g., blocking a suspicious transaction) based on both the model's prediction and the complex rules."
  },
  {
    "question_id":"skillcertpro_e2_q29",
    "question": "You are deploying a model in Databricks for batch inference that processes thousands of customer transactions to predict fraud. The model needs to be executed daily, with predictions stored in a Delta Lake table. During model inference, you observe high memory usage and performance degradation over time. What is the most likely cause of this issue, and how can it be resolved?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "The model is saving all intermediate prediction results to memory instead of writing them to Delta Lake after each batch. Modify the batch job to persist predictions incrementally."},
      {"id": 1, "text": "The Spark cluster size is too small to handle the batch job, leading to resource contention. Increase the cluster size to resolve memory and performance issues."},
      {"id": 2, "text": "The model is not properly garbage collecting intermediate data during each batch inference. Enable Spark's memory management optimizations to handle large data volumes."},
      {"id": 3, "text": "The model is not optimized for batch inference. Retrain the model using Databricks AutoML to optimize its performance for batch predictions."}
    ],
    "ans_id":0,
    "explanation":" The model is saving all intermediate prediction results to memory instead of writing them to Delta Lake after each batch. Modify the batch job to persist predictions incrementally. \n\n This is a highly likely cause of high memory usage and performance degradation in a batch inference job processing thousands of transactions. If the predictions are generated and stored in memory (e.g., in a large Pandas DataFrame or a Spark DataFrame that isn't being written out) before the entire batch is processed and written to Delta Lake, it can lead to memory exhaustion, especially with large datasets. \n\nResolution: Modifying the batch job to persist predictions incrementally (e.g., writing predictions to the Delta Lake table in smaller chunks or after processing a subset of the transactions) would prevent the accumulation of a large number of results in memory at once. This approach ensures that memory usage remains manageable and the job can process the entire dataset without performance degradation due to memory pressure."
  },
  {
    "question_id":"skillcertpro_e2_q30",
    "question": "You are working as a machine learning engineer for a logistics company. You have developed a model to predict delivery times based on features such as delivery location, traffic conditions, and package size. The model generates predictions daily and stores the results in a Delta table in Databricks. The data is frequently queried by multiple teams for reporting and analytics purposes. To improve query performance, you are asked to optimize the Delta table by partitioning it on a relevant column. Which column is the best candidate for partitioning the Delta table to speed up queries, given the scenario?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Package Size"},
      {"id": 1, "text": "Delivery Location (Region)"},
      {"id": 2, "text": "Prediction Time"},
      {"id": 3, "text": "Model Version"}
    ],
    "ans_id":1,
    "explanation":"Delivery Location (Region)\n\n Partitioning the Delta table by a high-cardinality categorical column that is frequently used in queries can significantly improve performance by allowing Spark to only read the relevant partitions. In a logistics context, queries for reporting and analytics are very likely to be filtered or grouped by geographical regions (e.g., 'Show average delivery time by region,' 'Report on deliveries for the Northeast'). Partitioning by 'Delivery Location (Region)' would allow Spark to quickly locate and process only the data relevant to a specific region, drastically reducing the amount of data scanned for such queries."
  }
]