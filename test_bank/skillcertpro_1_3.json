[
    {
    "question_id":"skillcertpro_e1_q21",
    "question": "You are responsible for maintaining a machine learning model deployed in a Databricks environment, which predicts customer lifetime value based on a variety of numeric and categorical features. You suspect that both feature and label drift are affecting the models performance over time. Which of the following would be the most comprehensive solution for monitoring drift across all feature types (numeric and categorical) and labels?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "Rely on model performance metrics such as precision, recall, and F1-score to detect any type of drift."},
        {"id": 1, "text": "Implement a combination of statistical tests, such as the chi-square test for categorical features and the Kolmogorov-Smirnov test for numeric features, while comparing label distribution changes over time."},
        {"id": 2, "text": "Regularly retrain the model using new data and monitor the models accuracy to detect any drift in features or labels."},
        {"id": 3, "text": "Monitor summary statistics (mean, median, variance) of the numeric features and track the mode for each categorical feature to detect drift"}
        ],
    "ans_id":1,
    "explanation":"Implement a combination of statistical tests, such as the chi-square test for categorical features and the Kolmogorov-Smirnov test for numeric features, while comparing label distribution changes over time.This option provides the most comprehensive approach to monitoring both feature and label drift across different data types.\n\nFeature Drift (Categorical): The Chi-square test is appropriate for detecting significant changes in the distribution of categorical features over time.\n\nFeature Drift (Numeric): The Kolmogorov-Smirnov (KS) test is a non-parametric test suitable for detecting changes in the distribution of numerical features, regardless of their underlying distribution.\n\nLabel Drift: Directly comparing the distribution of the target variable (customer lifetime value, even if it's continuous, can be binned or its distribution compared using KS) over different time periods can reveal label drift. Significant shifts in the churn rate or the distribution of lifetime values indicate that the underlying relationship the model learned might be changing.\nBy combining these methods, you can get a holistic view of drift affecting both the input features and the target variable" 
    },
    {
    "question_id":"skillcertpro_e1_q22",
    "question": "You are working on an ML experiment using MLflow to track your runs. During this process, you want to ensure that all metadata, such as hyperparameters, metrics, and artifacts, are thoroughly tracked to make your experiment fully reproducible. You are also planning to share this experiment with a team of data scientists. Which of the following strategies will best ensure full reproducibility and sharing of the experiment?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "Use MLflows mlflow.sklearn.log_model() to save the model and its artifacts, but avoid logging the dataset due to storage limitations, relying on team members to load the dataset locally."},
        {"id": 1, "text": "Track and log all experiment metadata using mlflow.log_params(), mlflow.log_metrics(), and mlflow.log_artifacts(), and register the final model in the MLflow Model Registry for future version control."},
        {"id": 2, "text": "Automatically log the experiment runs using MLflows integration with a specific machine learning library (like XGBoost or LightGBM) and save the experiment results in a shared folder for your team to access."},
        {"id": 3, "text": "Use mlflow.log_params() and mlflow.log_metrics() to track the hyperparameters and metrics and manually share the datasets and models used in the experiments with your team."}
        ],
    "ans_id":1,
    "explanation":"Track and log all experiment metadata using mlflow.log_params(), mlflow.log_metrics(), and mlflow.log_artifacts(), and register the final model in the MLflow Model Registry for future version control.This strategy provides the most comprehensive approach to ensure full reproducibility and facilitates sharing with the team: \n\nmlflow.log_params(): Logs all hyperparameters used in the experiment, which are essential for recreating the exact training setup. \nmlflow.log_metrics(): Logs all relevant evaluation metrics, allowing for comparison and understanding of the model's performance.\nmlflow.log_artifacts(): Logs any additional files needed for reproducibility, such as the trained model itself, any preprocessing pipelines, or even data samples or configurations (within reasonable storage limits). \nMLflow Model Registry: Registering the final model provides version control and a centralized location for the team to access and deploy the model, along with its associated metadata" 
    },
    {
    "question_id":"skillcertpro_e1_q23",
    "question": "You are building a machine learning model for a financial prediction task, and you want to track various experiment parameters, metrics, and artifacts in a Databricks environment. Which of the following strategies are essential for efficient experiment tracking in Databricks? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Log both the training and validation metrics separately to understand model performance in both datasets."},
      {"id": 1, "text": "Always use unique names for each experiment to avoid accidental overwriting."},
      {"id": 2, "text": "Include the model's runtime environment (e.g., library versions) in the experiment logs for future reproducibility."},
      {"id": 3, "text": "Artifacts such as training data snapshots should not be logged to avoid storage issues."},
      {"id": 4, "text": "Always use unique names for each experiment to avoid accidental overwriting."}
        ],
    "ans_ids":[0,2],
    "explanation":"Log both the training and validation metrics separately to understand model performance in both datasets. Tracking training and validation metrics independently is crucial for understanding how well the model is learning (training metrics) and how well it generalizes to unseen data (validation metrics). Comparing these metrics helps in identifying issues like overfitting or underfitting, which are essential for effective experiment tracking and model development.\n\n Include the model's runtime environment (e.g., library versions) in the experiment logs for future reproducibility. Reproducibility is a cornerstone of good ML practice. Logging the runtime environment, including the versions of key libraries (e.g., scikit-learn, TensorFlow, PyTorch, MLflow itself, and even Python), ensures that you can recreate the exact conditions under which the experiment was run. This is vital for debugging, sharing results, and redeploying models in the future with consistent behavior."
    },
    {
    "question_id":"skillcertpro_e1_q24",
    "question": "You are building a machine learning pipeline for a healthcare dataset where the preprocessing involves handling missing values, normalizing features, and encoding categorical variables. You want to ensure that the preprocessing logic is tightly coupled with the model, so it can be used in both training and inference stages. To achieve this, you plan to include the preprocessing logic within a custom model class. What is the best approach to include preprocessing logic in your custom model class, ensuring that both training and inference stages apply the same data transformations?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "Implement the preprocessing logic in both the fit() and predict() methods of the custom model class to ensure that data is transformed consistently during training and inference."},
        {"id": 1, "text": "Implement the preprocessing logic in the fit() method of the custom model class to ensure that the data is transformed during training."},
        {"id": 2, "text": "Incorporate the preprocessing steps into the models __init__() method, so the logic is applied when the model is instantiated."},
        {"id": 3, "text": "Include the preprocessing logic only in the predict() method, as preprocessing is most important during inference for making predictions on new data."}
        ],
    "ans_id":0,
    "explanation":" Implement the preprocessing logic in both the fit() and predict() methods of the custom model class to ensure that data is transformed consistently during training and inference.\nThis is the best practice for tightly coupling preprocessing with a custom model class.\nfit() method: During training, the fit() method learns any parameters needed for the preprocessing steps (e.g., mean and standard deviation for normalization, mapping for categorical encoding) from the training data. It then applies these transformations to the training data before fitting the core model.\npredict() method: During inference, the predict() method applies the same preprocessing transformations (using the parameters learned during fit()) to the new, unseen data before feeding it to the trained model for prediction. This ensures consistency in data transformation between training and inference, which is crucial for the model to perform as expected in deployment."
    },
    {
    "question_id":"skillcertpro_e1_q25",
    "question": "Your team is deploying a machine learning model to serve product recommendations. After initial experimentation, you are considering switching from real-time prediction serving to batch prediction serving in Databricks. What are the valid advantages of this approach for the live system? (Select two)",
    "type":"MS",
    "choices": [
      {"id": 0, "text": "Minimizes the impact of model drift during serving"},
      {"id": 1, "text": "Improves system performance by offloading computation from serving layer to batch layer"},
      {"id": 2, "text": "Reduces operational complexity by centralizing model inference"},
      {"id": 3, "text": "Enables continuous model training and deployment without downtime"},
      {"id": 4, "text": "Allows for real-time updates based on user behavior"}
      ],
    "ans_ids":[1,2],
    "explanation":"Improves system performance by offloading computation from serving layer to batch layer Batch prediction involves precomputing predictions for a set of users or items at regular intervals (e.g., daily). When a user requests recommendations, the system simply retrieves the precomputed results. This offloads the computationally intensive model inference from the real-time serving layer, which needs to respond quickly to user requests, to a background batch processing system that can handle the load more efficiently. \n\nReduces operational complexity by centralizing model inference With batch prediction, the model inference logic is centralized within the batch processing pipeline. This simplifies the serving layer, which no longer needs to execute the model in real-time. Managing a batch pipeline for inference can be less complex than maintaining a highly available and low-latency real-time serving infrastructure"
    },
    {
    "question_id":"skillcertpro_e1_q26",
    "question": "Your team has deployed a machine learning model for real-time predictions in an e-commerce application that recommends products based on user behavior. Over the last few weeks, you've noticed a decline in model performance. Upon investigating, you observe significant differences in the feature distribution between the current data and the training data. These differences seem to have appeared gradually over time. You are tasked with determining the cause and deciding on an appropriate solution. What is the most likely cause of the decline in performance, and what should your next steps be?",
    "type":"MC",
    "choices": [
        {"id": 0, "text": "The decline in performance is likely caused by concept drift, but no action should be taken since the model will adapt over time."},
        {"id": 1, "text": "Overfitting to the training data is the main cause, so regularization techniques should be applied to reduce the model complexity."},
        {"id": 2, "text": "Model decay due to data drift, and retraining the model with recent data is the best solution."},
        {"id": 3, "text": "The problem is likely feature leakage, where recent data contains information that was unavailable during model training. The model should be retrained without the leaked features."}
        ],
    "ans_id":2,
    "explanation":"Model decay due to data drift, and retraining the model with recent data is the best solution.\n\nData Drift: The observation of significant differences in feature distribution between the current (production) data and the training data over time is a classic sign of data drift (also known as feature drift). The statistical properties of the input features that the model relies on have changed.\nModel Decay: When the data distribution shifts, a model trained on the old distribution becomes less accurate on the new data, leading to a decline in performance. This is often referred to as model decay.\nRetraining with Recent Data: The most effective way to address data drift is to retrain the model using more recent data that reflects the current feature distributions. This allows the model to learn the new patterns and relationships in the data, thus improving its performance."
    },
    {
    "question_id":"skillcertpro_e1_q27",
    "question": "You are working for a financial company that uses machine learning models to detect fraud in real-time transactions. The transactions need to be processed with minimal latency because a delay in prediction could result in fraudulent transactions being approved. The volume of transactions is relatively low, with only a few hundred transactions per minute. Which of the following strategies would be the most appropriate for deploying your fraud detection model for real-time inference?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Using a distributed computing cluster to ensure that the model can handle spikes in transaction volume."},
      {"id": 1, "text": "Batch inference using a scheduled job that processes transactions in bulk every 10 minutes."},
      {"id": 2, "text": "Asynchronous batch processing where transactions are logged and processed later with lower priority."},
      {"id": 3, "text": "Real-time inference using a serverless architecture that scales based on incoming transaction volume."}
        ],
    "ans_id":3,
    "explanation":"Real-time inference using a serverless architecture that scales based on incoming transaction volume. This is the most appropriate strategy for real-time fraud detection with low latency requirements and fluctuating transaction volume.\n\nReal-time Inference: The need for minimal latency to prevent fraudulent approvals necessitates processing each transaction as it occurs, which is the essence of real-time inference.\nServerless Architecture: Serverless platforms (like AWS Lambda, Azure Functions, or similar on Databricks) are ideal for this scenario because they can automatically scale up or down based on the incoming workload. This ensures that the system can handle fluctuations in transaction volume without manual intervention and without incurring costs for idle resources. \nLow Latency: Serverless architectures are designed for event-driven processing and can typically provide the low latency required for immediate fraud detection."
    },
    {
    "question_id":"skillcertpro_e1_q28",
    "question": "You are working with a machine learning pipeline in Databricks where you want to append new data to an existing Delta table. The existing Delta table has a well-defined schema, and you want to enforce strict schema matching to avoid appending data with incorrect or mismatched fields. What is the best way to enforce schema while appending data to the Delta table?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Use the MERGE INTO command and specify the WHEN MATCHED clause to enforce schema."},
      {"id": 1, "text": "Use the append mode with the option('mergeSchema', 'false') during the write operation."},
      {"id": 2, "text": "Set the Delta table option overwriteSchema = true while performing the write operation."},
      {"id": 3, "text": "Use the overwrite mode with the option('mergeSchema', 'true') during the write operation."}
    ],
    "ans_id":1,
    "explanation":"Use the append mode with the option('mergeSchema', 'false') during the write operation. When writing to a Delta table in append mode (mode('append')), setting the option 'mergeSchema' to 'false' enforces strict schema matching. If the schema of the data you are trying to append does not exactly match the schema of the existing Delta table, the write operation will fail, preventing data with incorrect or mismatched fields from being added."
  },
  {
    "question_id":"skillcertpro_e1_q29",
    "question": "You are managing a machine learning pipeline for a recommendation system. Your models are registered in the Databricks Model Registry, and you need to create a job that triggers automatically when a model moves from the staging stage to production. This job will initiate an automated process to deploy the model in a live environment. Which of the following are key steps required to create a job that triggers upon model stage transitions in the Databricks Model Registry? (Select two)",
    "type":"MS",
    "choices": [
        {"id": 0, "text": "Enable Auto-Retesting on Every Stage Transition"},
        {"id": 1, "text": "Use the Databricks REST API to Monitor Model Transitions"},
        {"id": 2, "text": "Utilize a Databricks Notebook for Continuous Manual Deployment"},
        {"id": 3, "text": "Set up a Webhook for Stage Transition Events"},
        {"id": 4, "text": "Manually Monitor Stage Changes and Trigger Jobs"}
        ],
    "ans_ids":[1,3],
    "explanation":"Use the Databricks REST API to Monitor Model Transitions The Databricks REST API provides programmatic access to the Model Registry. You can write a script (e.g., using Python and the Databricks SDK which wraps the REST API) that periodically polls or listens for events related to model stage transitions. When a model transitions to the 'Production' stage, the script can then trigger the deployment job using the Databricks Jobs API. This allows for automated action based on model registry events. \n\n D. Set up a Webhook for Stage Transition Events: Webhooks allow the Databricks Model Registry to send notifications to an external service (or an internal Databricks service configured to listen) whenever a specific event occurs, such as a model stage transition. You can configure a webhook to trigger your deployment job when a model transitions to 'Production.' This is a more event-driven and potentially more efficient approach than continuous polling."
    },
    {
    "question_id":"skillcertpro_e1_q30",
    "question": "You are monitoring a machine learning model that relies on categorical features, such as product categories and customer segments. The model is in production, and you need a more robust drift detection method than simply monitoring summary statistics for each category. What is the most appropriate testing method for detecting categorical feature drift in this scenario",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Apply the Jensen-Shannon Divergence (JSD) to quantify how different the probability distributions of the categorical features are between the training and production data."},
      {"id": 1, "text": "Use the t-test to compare the means of encoded categorical features between the training and production data."},
      {"id": 2, "text": "Monitor the standard deviation of the one-hot encoded representation of categorical features to detect drift."},
      {"id": 3, "text": "Track the percentage of missing categories and compare it to the training set distribution to detect drift."}
        ],
    "ans_id":0,
    "explanation":"Jensen-Shannon Divergence (JSD) \n -Measures similarity between two probability distributions \n -For categorical features, compute the frequency distribution in training vs production, then apply JSD.\n - JSD is symmetric and bounded, making it interpretable.\n - Most robust method for detecting categorical feature drift."
    }  
]