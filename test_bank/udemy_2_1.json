[
  {
    "question_id":"udemy_e2_q1",
    "question": "In the context of monitoring categorical input variables for a production machine learning application, a machine learning engineer observes an increase in missing values for a specific category in one of the variables. Which tool can the engineer utilize to evaluate this observation effectively?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Kolmogorov-Smirnov (KS) test"},
      {"id": 1, "text": "One-way Chi-squared Test"},
      {"id": 2, "text": "Two-way Chi-squared Test"},
      {"id": 3, "text": "Jenson-Shannon distance"},
      {"id": 4, "text": "None of the options provided"}
    ],
    "ans_id":1,
    "explanation":"One-way Chi-squared Test\n\nWhen you want to detect changes in the distribution of a categorical variable—such as a sudden uptick in the 'missing” category—the standard statistical tool is a one-way Chi-square (goodness-of-fit) test. Databricks' drift-metrics tables even include a chi_squared_test field to track p-values and statistics for categorical drift comparisons (against a baseline or a previous window)"
  },
  {
    "question_id":"udemy_e2_q2",
    "question": "Which deployment paradigm is capable of centrally computing predictions for a single record with extremely fast results?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Streaming"},
      {"id": 1, "text": "Batch"},
      {"id": 2, "text": "Edge on-device"},
      {"id": 3, "text": "None of these strategies will accomplish the task."},
      {"id": 4, "text": "Real-time"}
    ],
    "ans_id":4,
    "explanation":"Real-time\n\n To compute a single record prediction with extremely fast, central (server-side) response, the only suitable deployment paradigm is: Real-time (Online) Inference:\n*Low-latency model serving (often milliseconds)\n*Request/response API pattern\n*Centralized computation\n*Ability to handle single-record inference efficiently"
  },
  {
    "question_id":"udemy_e2_q3",
    "question": "Which code snippet effectively removes the star_rating column from a Delta table located at the specified path by loading in the data and then dropping the column?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "spark.read.format('delta').load(path).drop('star_rating')"},
      {"id": 1, "text": "spark.read.format('delta').table(path).drop('star_rating')"},
      {"id": 2, "text": "Delta tables are immutable and cannot be modified"},
      {"id": 3, "text": "spark.read.table(path).drop('star_rating')"},
      {"id": 4, "text": "spark.sql('SELECT * EXCEPT star_rating FROM path')"}
    ],
    "ans_id":0,
    "explanation":"spark.read.format('delta').load(path).drop('star_rating')\n\nThe question asks specifically for a snippet that:\n*Loads a Delta table from a given path\n*Drops the star_rating column\n*Produces a DataFrame with the column removed\n*It does not ask to write the result back, only to load and drop."
  },
  {
    "question_id":"udemy_e2_q4",
    "question": "Which operation within the Feature Store Client, fs, enables the retrieval of a Spark DataFrame for a dataset linked to a Feature Store table?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "fs.create_table"},
      {"id": 1, "text": "fs.write_table"},
      {"id": 2, "text": "fs.get_table"},
      {"id": 3, "text": "There is no method available in fs for this task"},
      {"id": 4, "text": "fs.read_table"}
    ],
    "ans_id":4,
    "explanation":"The FeatureStoreClient.read_table(name) method returns a Spark DataFrame containing all the feature data from the specified feature table. Under the hood, it reads the underlying Delta table and presents it as a DataFrame that you can further process or join against other data"
  },
  {
    "question_id":"udemy_e2_q5",
    "question": "What is the most common deployment paradigm for machine learning models in machine learning projects?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "On-device"},
      {"id": 1, "text": "Streaming"},
      {"id": 2, "text": "Real-time"},
      {"id": 3, "text": "Batch"},
      {"id": 4, "text": "None of these deployments"}
    ],
    "ans_id":3,
    "explanation":"Batch scoring remains by far the predominant paradigm for ML model deployment—Databricks' own training materials note that 80-90 percent of production deployments use batch inference, leveraging existing ETL and data-warehouse workflows to generate predictions on large datasets at scheduled intervals"
  },
  {
    "question_id":"udemy_e2_q6",
    "question": "How can a data scientist enable MLflow Autologging for all machine learning libraries used in a notebook, ensuring compatibility across various versions of Databricks Runtime for Machine Learning and regardless of workspace-wide configurations selected in the Admin Console?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "mlflow.sklearn.autolog()"},
      {"id": 1, "text": "mlflow.spark.autolog()"},
      {"id": 2, "text": "spark.conf.set('autologging', True)"},
      {"id": 3, "text": "It is not possible to automatically log MLflow runs."},
      {"id": 4, "text": "mlflow.autolog()"}
    ],
    "ans_id":4,
    "explanation":"mlflow.autolog() \n\n This single call activates autologging for every supported library in the environment, including (scikit-learn, Spark ML, XGBoost, LightGBM, TensorFlow, Keras, PyTorch, Transformers) and others supported in the Databricks Runtime\nThis ensures full compatibility across notebook code without requiring Admin Console configuration."
  },
  {
    "question_id":"udemy_e2_q7",
    "question": "Which MLflow operation can automatically compute and log a Shapley feature importance plot?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "mlflow.shap.log_explanation"},
      {"id": 1, "text": "None of these operations can accomplish the task."},
      {"id": 2, "text": "mlflow.shap"},
      {"id": 3, "text": "mlflow.log_figure"},
      {"id": 4, "text": "client.log_artifact"}
    ],
    "ans_id":0,
    "explanation":"mlflow.shap.log_explanation\n\n MLflow provides a dedicated SHAP logging utility, this function can automatically:\n*Compute SHAP values\n*Generate the corresponding SHAP plots (e.g., summary plot, force plot)\n*Log both the explanation object and the visualizations to the MLflow run\nIt is explicitly designed for automated SHAP-based feature importance logging, making it the correct choice."
  },
  {
    "question_id":"udemy_e2_q8",
    "question": "A data scientist has developed a scikit-learn random forest model but has not yet logged it with MLflow. They aim to retrieve the input schema and the output schema of the model to document the expected input data type. Which MLflow operation can achieve this task?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "mlflow.models.schema.infer_schema"},
      {"id": 1, "text": "mlflow.models.signature.infer_signature"},
      {"id": 2, "text": "mlflow.models.Model.get_input_schema"},
      {"id": 3, "text": "mlflow.models.Model.signature"},
      {"id": 4, "text": "There is no way to obtain the input schema and the output schema of an unlogged model."}
    ],
    "ans_id":1,
    "explanation":"mlflow.models.signature.infer_signature \n\nThe helper function infer_signature in the mlflow.models.signature module is explicitly designed to examine a model's inputs and outputs and return a ModelSignature object that captures both the input schema and the output schema. You simply provide:\n*A representative batch of input data (e.g. a Pandas or Spark DataFrame)\n*The model's outputs for that same batch (e.g. model.predict(inputs)) \n\nThis call returns a ModelSignature containing both the input and output Schema—even for a model that you haven't yet logged—so you can document the expected feature names and types as well as the prediction structure"
  },
  {
    "question_id":"udemy_e2_q9",
    "question": "Which tool is suitable for creating a continuous data preparation pipeline in a machine learning application, where the data needs to be processed in equal-sized batches for inference?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Spark UDFs"},
      {"id": 1, "text": "Structured Streaming"},
      {"id": 2, "text": "MLflow"},
      {"id": 3, "text": "Delta Lake"},
      {"id": 4, "text": "AutoML"}
    ],
    "ans_id":1,
    "explanation":"Structured Streaming \n\nApache Spark's Structured Streaming engine is designed for continuous data processing in micro-batches. You can control the size of each batch—in terms of files, bytes, or records—so that data preparation happens in equal-sized chunks. \n\nThis admission control guarantees that each micro-batch processes a roughly consistent volume of data, making it ideal for an always-on data preparation pipeline that feeds downstream inference jobs"
  },
  {
    "question_id":"udemy_e2_q10",
    "question": "In MLflow Model Serving, when enabling real-time serving for a model with one version in each stage of the Model Registry, which stages' model versions are automatically available for querying?",
    "type":"MC",
    "choices": [
      {"id": 0, "text": "Staging, Production, Archived"},
      {"id": 1, "text": "Production"},
      {"id": 2, "text": "None, Staging, Production, Archived"},
      {"id": 3, "text": "Staging, Production"},
      {"id": 4, "text": "None, Staging, Production"}
    ],
    "ans_id":3,
    "explanation":"Staging, Production \n\nWhy Staging and Production are served\nWhen you enable real-time serving for a registered model in Databricks, the serving cluster automatically hosts all active versions—those in the Staging and Production stages—as REST endpoints. You can invoke each by using either the numeric version or the stage name in the URI (e.g. /model/<model-name>/Staging/invocations and /model/<model-name>/Production/invocations)"
  }
]